リカレントニューラルネットワークをを理解するキーワードは2つです。前の出力を次の計算に利用するセル（Cell）。単語が一直線に並んだ文章や、順次変化する株価変動などの、一列に並んだ時系列データという概念。通常のニューラルネットワークはこの時系列データを取り扱うことが難しい、あるいはできないのですが、リカレントニューラルネットワーク（RNN）は、セル（Cell）というものを利用する事で時系列データを取り扱う事ができます。
リカレントニューラルネットワーク（RNN）とは、時系列データを取り扱えるようにニューラルネットワークにセル（Cell）というものを付けたもの。と言っても、リカレントニューラルネットワーク（RNN）の中心概念、”時系列データ”が普通のデータとどのように違うのか、一見するとわかりにくいですよね。「セル」が普通の伝搬と何が違うのかを理解する為に、リカレントニューラルネットワーク（RNN）による文章生成を見てみましょう。
文章生成とはその字面の通り、人工知能（AI）に文章を生成させるものです。例えば、夏目漱石の”吾輩は猫である”の文体を学習した人工知能（AI）に、適当な冒頭の1単語を与えると文章を生成しましょう。例えば「吾輩」という単語を「吾輩は猫である」を学習した人工知能（AI）入力すると、「は」と次の単語を予測して出力します。そして、「は」という出力を次の入力として再度、次の単語を推測して「猫」や「ここで」、「藁」といった”それらしい単語”を出力します。これが文章生成の基本です。
まず、文章生成を用いて時系列データを考えてみましょう。仮に「吾輩は猫である」という文章を見てみると、”吾輩”、”は”、”猫”、”で”、”ある”という5つの単語が連なったデータであると考えることができます。そして、この4つの単語には順列性があります。”吾輩”の後に続くのはいわゆる”てにをは”である格助詞でなければいけません。そして”吾輩”、”は”と続いた後にまた”吾輩”が出てきては”吾輩は吾輩”となってしまって、文法的には間違っていなくてもヘンテコな文章になってしまいます。
この種の普通のデータとは異なり、過去のデータに大きく影響されますよね。このように過去のデータが将来のデータに影響を及ぼすデータを、時系列データと呼びます。リカレントニューラルネットワーク（RNN）とは、時系列データを取り扱えるようにニューラルネットワークにセル（Cell）というものを付けたもの。と言っても、リカレントニューラルネットワーク（RNN）の中心概念、”時系列データ”が普通のデータとどのように違うのか、一見するとわかりにくいですよね。「セル」が普通の伝搬と何が違うのかを理解する為に、リカレントニューラルネットワーク（RNN）による文章生成を見てみましょう。
文章生成とはその字面の通り、人工知能（AI）に文章を生成させるものです。例えば、夏目漱石の”吾輩は猫である”の文体を学習した人工知能（AI）に、適当な冒頭の1単語を与えると文章を生成しましょう。例えば「吾輩」という単語を「吾輩は猫である」を学習した人工知能（AI）入力すると、「は」と次の単語を予測して出力します。そして、「は」という出力を次の入力として再度、次の単語を推測して「猫」や「ここで」、「藁」といった”それらしい単語”を出力します。これが文章生成の基本です。
まず、文章生成を用いて時系列データを考えてみましょう。仮に「吾輩は猫である」という文章を見てみると、”吾輩”、”は”、”猫”、”で”、”ある”という5つの単語が連なったデータであると考えることができます。そして、この4つの単語には順列性があります。”吾輩”の後に続くのはいわゆる”てにをは”である格助詞でなければいけません。そして”吾輩”、”は”と続いた後にまた”吾輩”が出てきては”吾輩は吾輩”となってしまって、文法的には間違っていなくてもヘンテコな文章になってしまいます。
この種の普通のデータとは異なり、過去のデータに大きく影響されますよね。このように過去のデータが将来のデータに影響を及ぼすデータを、時系列データと呼びます。
そして、リカレントニューラルネットワーク（RNN）は時系列データを扱うために”セル（Cell）”というものを利用します。リカレントニューラルネットワーク（RNN）の中間層は、通常の入出力の他に、ループ状の入出力を持ちます。単語予測の時に、中間層の出力を二つ用意し、片方は通常の単語予測の為の出力、もう片方は次の単語予測の時の為の出力します。
普通のニューラルネットワークとリカレントニューラルネットワーク（RNN）の唯一にして最大の違いとは、セル（Cell）があることです。セル（Cell）とは、今回の出力の計算過程を、次回の計算に利用するループ状のニューラルネットの構造の事です。文章生成においては、次の単語予測の為の出力が、次の単語予測の時に通常の入力と併せて中間層に入力される事により、過去の中間層の状態を考慮した予測を実現するのに使われます。このループ状の中間層の構造を、”セル（Cell）”と呼びます。
リカレントニューラルネットワーク（RNN）は文章生成の他にも、株価変動や天気予報などの、時系列を考慮しなくてはいけないタスクに活用されています。
しかし、リカレントニューラルネットワーク（RNN）の主な活用分野は文章生成です。今回は夏目漱石を例としましたが、シェイクスピアっぽい詩や、ハリーポッターの次回作に近い小説を自動生成させる事などが試みられています。どちらも同じ手法を用いていて、例えばシェイクスピアに関してはシェイクスピアの作品を収録したデータセットを利用して人工知能（AI）を学習させた後、その人工知能（AI）に適当な単語や文章を与えて次の単語を予測させるというものです。
これらもリカレントニューラルネットワーク（RNN）を活用しています基本概念として使っています。また、Googleが開発しているGPT-3も文章生成の一分野ですが、これは文章によって説明された機能を実装するHTMLコードとCSSコードを生成するという事も成し遂げています。例えば、「黄色いボタン」という物を入力すると、人工知能（AI）がHTMLとCSSを用いて実際に黄色いボタンを実装するコードを生成できるとのこと。
リカレントニューラルネットワーク（RNN）は時系列を考慮する関係上、長い文章を処理しようとすると、後半になるにつれて最初の方の文章を忘却する事があります。例えば「吾輩は猫である」を使って、以下の文章を生成するとしましょう。
「（前略）吾輩は藁の上から急に笹原の中へ棄てられた。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである」」
この時、「吾輩は藁の上から急に笹原の中へ棄てられた」まで生成したものの、過去の文章がどういったものかを忘却してしまい、「られた」につながる単語として「ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである」を参考に「時」を出力し、「吾輩は藁の上から急に笹原の中へ棄てられた時何だかフワフワした感じがあったばかりである」というようなヘンな文章を生成してしまう場合があります。
これを解決する為に、リカレントニューラルネットワーク（RNN）の拡張であるLong shot-term memory（LSTM）では”セル（Cell）”に改善を加え、メモリを持たせて記憶を実現する事で、長い文章も問題なく処理できるようにしました。現在は研究、開発、どのような分野であれLong shot-term memory（LSTM）のようなリカレントニューラルネットワーク（RNN）の進化系を用いる事が普通です。特に理由がなければ、リカレントニューラルネットワーク（RNN）よりもLong shot-term memory（LSTM）を用いましょう。
リカレントニューラルネットワーク（RNN）は”時系列データ”を扱うために”セル（Cell）を用いるニューラルネットワークの一種。畳み込みニューラルネットワーク（CNN）は”畳み込み”という”画像の特徴的な部分だけを抽出する計算”によって画像データを圧縮し、推論しやすいようにするものであり、この二つは根本的に用途が異なるという事。そして、リカレントニューラルネットワーク（RNN）の拡張・改善としてLong shot-term memory（LSTM）のような新技術です。
文章生成や文章翻訳は”自然言語処理”というジャンルで、突き詰めればとても深いものですが、リカレントニューラルネットワーク（RNN）の事が分かった皆さんは、既にその入口を突破しているといえるでしょう。
RNN(Recurrent Neural Network)は、ある層の出力が遡って入力される再帰結合を持つリカレント（再帰型）ニューラルネットワークです。自分の出力をもう一度自分に食わせる、そんなややこしい構造を持つネットワークなのですが、なぜ、このような処理をする必要があるのでしょうか。 
その鍵は時系列と可変長です。前回学んだ画像の文字がマルかバツか判断するCNNは、固定長の静止画を畳み込み処理するだけでした。しかし、例えば自然言語処理(NLP)などの領域では、時間の概念が必要となり、入出力のデータサイズも可変となります。例をあげて説明しましょう。 
あなたはTV番組でどちらが「麻里ちゃん通」なのかAIと勝負することになりました。相手は麻里ちゃんを学習済のAIなので、早押ししないと勝てないかも知れません。さて、図3のような問題が流れてきました。「麻里ちゃんの」「好きな」。ここではまだ解答がわかりませんが、すでに頭の中は激しく回転していて続く言葉として「人は誰か」とか「食べ物はなに」「場所はどこ？」などが浮かんでいます。 
続く音声の「ケーキは」まで聞いたとき、AI野郎が勢いよくボタンを押しました（しまった、わかっていたのに…）。そして、自信たっぷりに「チーズケーキ」と答えたのです。 
この流れを時系列でみてみましょう。時点(t-1)で「麻里ちゃんの」、時点(t)で「好きな」と聞き、時点(t+1)で「ケーキは」ときたから問題文の意図が理解できたわけです。この人間なら当たり前のことが、順伝搬NNでは少し難しいのです。なぜなら、「ケーキは」だけだと文の意図が読み取れないので、その前の「麻里ちゃんの」や「好きな」を記憶しておく必要があり、それを可能にする構造が再帰型なのです。 
リカレントニューラルネットワークの展開図とは，時間軸に展開して説明しましょう。再帰型構造を時系列に展開したモデルです。問題文は4つの文節からなるので、4ステップのニューラルネットワークに展開できます。時点(t)の時の隠れ層には、入力層からの「好きな」と前ステップからの「麻里ちゃんの」という2つの入力があり、これを組み合わせ計算します。続く時点(t+1)で、入力層からの「ケーキは」と前ステップ隠れ層からの「麻里ちゃんの×好きな」を組み合わせ、「あ、わかったぞ」とAIはピンポーンしたわけです。 
lights.png通時的誤差逆伝搬(Back propagation Through time)
組み合わせるって簡単に言っていますが、実際は2つの入力の重みを考慮した数学的な行列計算です。前回マルという記号を判別するのにも苦労したように、人間が頭の中で自然とできている処理がAI君は結構大変なのです。AIがかしこくなる仕組みは、誤差逆伝搬(Back propagation)で重みを調整するからでしたね。RNNの学習法も同じく逆伝搬による重み調節なのですが、前ステップからの情報も加わるために特別に通時的誤差逆伝搬(BPTT)と呼ばれています。
お、そう来ましたか。確かに時点(t)で出力層って何が出力されているのでしょうか。わかっていたつもりで分かっていなかったかもです。う~ん、いつもながらシンプルな疑問ほど難しい…。 
実は、この答えはAIが何を目的に学習したかによって異なります。今回のモデルは、”問題文の意図”を出力するテキスト解析(Text Analytics)としてトレーニングしている想定です。時点(t)では「麻里ちゃんの好きな人を尋ねる」のか「麻里ちゃんの好きな場所を問う」のか文意がまだわからず、どの候補も信頼度が低い状態です。そして時点(t+1)で出力層に「麻里ちゃんの好きなケーキを問う問題」という候補ができ、その信頼度が閾値を超えたからAI野郎はボタンを押したのです。 
普通に、問題文の意図ではなく、文章全体を予想するモデルでもいいでしょう。時点(t)では、「麻里ちゃんの好きな〇〇」という候補が多すぎて判断つかなかったものが、時点(t+1)までくれば、後ろに「なんですか？」をくっつけた「まりちゃんの好きなケーキはなんですか？」という候補が浮上します。つまり、①文章全体を想定出力する→②文章の意図を読み取る→③意図と正解を学習(Classification)した分類器で解答を導き出す、というような3ステップの処理になります。ただし、この場合、疑問文になると分かっていないので、「チーズケーキ」という候補を選んでしまう可能性も大きいので、クイズ専用に学習したモデルには負けます。     
別の目的ならどうでしょうか。例えば、感情分析(Emotion Recognition)なら、時点(t+1)「麻里ちゃんの好きなケーキは」までで「ジョイ」という感情が0.8出力され、時点(t+2)まで来ると0.3くらいに落ちる感じです(Google Cloud Natural Languageで試したらこんな感じでした）。
機械翻訳(Machine Translation)はどうでしょうか。時点(t+1)では"Mari's favorite cake is" と出力され、時点(t+2)で"What is Mari's favorite cake？"と疑問文に変化して出力されます(こちらはGoogle Cloud Translationで試してみました）。  
ちなみに、ネットなどでは、次に来る単語を予測出力するという例で説明していることが多いようです。まあ、将棋の「次の一手名人」のような設定ならそういう出力もアリだと思いますが、正直、文章生成（deep Writing）のような実用イメージしか湧かなかったので麻里ちゃんに登場してもらいました。 」 
lights.png人生は、リカレントでない方がいい場合もある
実は、将棋の「次の一手」は、これまでの手順にとらわれずに、その局面だけを見て最前手を見つけ出す方がいいです。とかく、人間は”前の手の顔を立てて”とか”勢いで”とかを重視して手を選びがちなのですが、AI君がそんな要素を全く無視して最強手段を出すようになってから、人間もそれを見習うように変わりつつあります。
小さな嘘をついてしまったから大きな嘘をつく、ああ言ったからそれを正当化する発言をしてしまう、そんなふうに過去に引きずられて悪い方向に行ってしまうこともあるので、リカレントも時には考えものです。
リカレントニューラルネットワークが使われる技術分野は，自然言語処理，自然言語理解，機械翻訳，テキスト分析，音声認識，画像分析，感情分析，文章生成，音声合成，動画分析，パーソナルアシスタント，チャットボットなどが挙げられます。
代表的な適用分野は自然言語処理(NLP)です。自然言語理解(NLU：Natural Language Understanding)という言葉もよく使われますので合わせて覚えておきましょう。NLPはいかにも人間らしい分野で、図5に示すさまざまな用途の総称でもあります。 
・テキスト分析(Text Analysis) 
テキスト（文章）を読んで、〇〇を出力する処理です。〇〇のところは目的に応じていくつかあります。図6は、それらの例です。機械翻訳なら英文、感情分析なら感情と度合い、タグ付けなら「麻里ちゃん」と「ケーキ」が出力されていますね。入力が音声の場合は、普通は音声認識（Speech to Text）でいったんテキストにした後で処理しますが、感情分析のようにそれに加えて声のトーンなど音声情報も使って処理する場合もあります。  
テキスト分析は、社内ドキュメントに自動タグ付けしてドキュメント検索を楽にするとか、ネット上の技術情報を自動的にスクレイピングするなどに役立ちます。また、感情分析も、コールセンターのお客が怒っているかとか、従業員で鬱予備軍がいないか、などさまざまな用途で使われつつあります。 
・機械翻訳(Machine Translation) 
2016年にGoogle翻訳の精度が飛躍的に向上したのは、エンジンをディープラーニングに切り替えたからです。翻訳は、入力と出力で言語の長さが異なりますが、元の言語の理解と変換言語の文章生成という2つにリカレント技術が大活躍しています。  
・音声認識(Speech to Text) 
日本でもAmazon EchoやGoogle Homeなどのスマートスピーカーが発売されましたね。私も両方使っていますが、その音声認識精度の高さにはいつも関心しています（といいつつ、以前からブログをGoogleの音声入力で書いていましたが…）。音声認識は入力が可変長の音声で、出力も可変長のテキストです。単なる音の文字化ではなく、文脈をきちんと理解しているからこそ高い精度で変換できるのです。  
・画像分析(Image Analysis) 
前回説明した画像認識(Image Recognition)はCNNで処理しました。でも、画像を見て何をしているのか説明するような場合は、CNNにRNNを組み合わせて使います。例えば、麻里ちゃんがおいしそうにモンブランを食べている写真をインプットすると、最初にCNNで画像認識した後、認識した「1人の女性」「ケーキ」「食べている」などのタグを使ってRNNが「1人の女性がケーキを食べている」というように説明してくれます。こうした技術はネット上にアップされた写真の自動説明付けなどに応用されつつあります。 
・動画分析 
静止画でなく動画の分析はどうでしょうか。前回のマルを認識するAIで説明しましょう。今回はマルという文字の全体がわからず、図7のように上からスキャンして1/3ずつ画像が渡されて、それをCNNで認識するとします。時点(t+1)のとき、真ん中の画像しか渡されないより、時点(t)の画像も渡してもらう方が的確に判断できますね。こんな感じで動画の場合は、前の映像との連続した情報で判断する必要がありますのでリカレントNN向きです。この技術を使って、ネット上の写真（静止画）だけでなく、動画もタグ付けされている例を見かけます。 
・チャットボット(Chat bot) 
チャットボットは、話しかけられた言葉から意図を理解して、テキストや音声で適切な応答を行います。いろいろな自然言語理解の技術を組み合わせた1階層上位の技術ともいえます。「ああ言えば、こう言う」というのは親子喧嘩の決まり文句ですが、botもそんなフロー作成を基本としています。でも、言い方を変えても意図をきちんと理解できる柔軟性があり、そのあたりが第2次AIブームのエキスパートシステムから進化しているところです。botの利用範囲はかなり広く、ECサイトの接客ロボだったり、社内システムに対する指示(会話型UI)だったり、いろいろな面で使われつつあります。  
・パーソナルアシスタント(Personal Assistant) 
パーソナルアシスタントも上位階層の技術です。チャットボットが単なる会話のやり取りなのに対し、こちらは会話以外にメンバー全員のスケジュールを見て会議室を予約してくれたり、タクシー（米国だとUBER）の手配をしてくれたり、よりインテリジェントなイメージでしょうか。こうした上位層での応用には、さらにいろいろな技術が組み合わさって使われていますが、人間とのインターフェースに関わる自然言語理解が活躍しています。 
リカレントニューラルネットワークは、過去を記憶して利用する技術です。これがずらずらと長い文章だったと想像してください。記憶する範囲が大きくなると、勾配（過去のどの情報がどれくらい影響を及ぼすか）が複雑になり過ぎて伝えるべき誤差が消滅したり（勾配消失問題）、記憶したものをどう生かすかという計算量も爆発的に増えます。そのため、リカレントNNは記憶する範囲をちょっと前までに限定していて、それ以前のものは切り捨てています。このことを打ち切り型通時的逆伝搬(Truncated Back propagation Through time)とも言います。 
でも、現実社会では、もう少し前までの情報を使わないとうまくいかないものもあります。そのためRNNを改良する形で登場したのがLSTM(Long Short-Time Memory)という技術で、日本語では長・短期記憶ユニットと呼ばれています。次回は、このLSTMについて解説しますのでお楽しみに。 
ニューラルネットワークは入力を線形変換する処理単位からなるネットワークである。このネットワーク内に循環が存在する、すなわちユニットの出力が何らかの経路で自身へ再び入力する場合、これを回帰型ニューラルネットワークという。回帰のないネットワーク（順伝播型ニューラルネットワーク（英語版）; Feed-Forward Network; FFN）と対比される。
RNNは任意のひと続きの入力を処理するために内部状態（記憶）を使うことができる。これによって、時系列のための時間的な動的振る舞いを示すことが可能となる[2]。これによって、分割化されていない、つながりのある手書き文字認識や音声認識といった課題に応用が可能になっている。
「回帰型ニューラルネットワーク」という用語は、類似した一般構造を持つ2つの広いネットワークのクラスを指し示すために見境なく使われる。1つは有限インパルス、もう1つは無限インパルスである。どちらのネットワークのクラスも時間的な動的振る舞いを示す。有限インパルス回帰型ネットワークは厳密な順伝播型ニューラルネットワークに展開でき、置き換えることができる有向非巡回グラフであるのに対して、無限インパルス回帰型ネットワークは展開できない有向巡回グラフである。
有限インパルスと無限インパルス回帰型ネットワークはどちらも追加の保管状態を持つことができ、この保管場所はニューラルネットワークによる直接的な制御下とすることができる。保管場所は他のネットワークやグラフが時間遅延を取り込むか、フィードバックループを持つのであれば、それらで置き換えることもできる。こういった制御された状態はゲート状態またはゲート記憶と呼ばれ、長・短期記憶ネットワーク（LSTMs）およびゲート付き回帰型ユニット（GRUs）の一部である。


 
