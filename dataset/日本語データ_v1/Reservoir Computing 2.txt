Reservoir Computing
RNN(Recurrent Neural Network: 過去の出力を現在の入力として扱えるようにすることで時系列などの順番を扱えるようにしたNN)の一種として認識されている。
Echo State Network(以下ESN)とLiquid State MACHINE(以下LSM)というほぼ同時期に考えられた2つのモデルが始まりと言われています。
現状ESNのほうが知られているかもしれません。少なくともGoogleの検索に引っかかりやすい。
ここで述べる内容も主にEcho State Networkについて考える。
Reservoir Computingは入力層、中間(Reservoir)層、出力層の3層からなっています。
Reservoir特有の特徴としては入力層とReservoir層において重みの更新が行われない点です。
最初にランダムな値で決め打ちしてしまったら、あとのことはReservoir層のノードの内部状態と出力層の重みに仕事を丸投げして出力層の重みを調整することで学習していきます。
Reservoir層でのみ同層間の結合を許しReservoir層のノードが内部状態を持つことでモデル内に短期記憶を用意し、時系列データの持つ因果関係を扱えるようにしているみたいです。


RNNの場合、過去の情報についても遡って逆伝搬で学習するため回帰なしのNeural Networkに比べて学習コストがとんでもないことになるという問題がありました。
Reservoir Computingの場合、学習の部分を出力層のみに任せることでこの大量の学習コストの問題を解消できる。

また学習方法として出力層の重みは最小二乗誤差をベースに学習するため従来のRNNだと時間を遡りすぎたことにより起こる伝搬の消失や局所解に嵌ったりといったことが起こりにくくなる。


手順としては以下のようになります。

1. 各種重みを初期化
2. 訓練データを流し込む
3. 流し込まれたデータに対し全時刻での内部状態を記憶
4. 記憶した内部状態を基にここで初めて出力層の重みを更新
5. 更新データと内部状態からデータを予測


Echo State Property(以下ESP)とは与えられたタスクをReservoir Computingが達成するために満足すべき条件のことです。ざっくり言ったらfading Memoryという現在の入力は現在の内部状態に過去の入力や状態よりも大きな影響を及ぼすという機能を満たす必要があります。
つまりESPを持つということはESN は最終的に安定状態になった時に初期状態の影響を全て洗い流してしまえるということみたいです。
ここでESPを満たす場合、W_resの固有値の絶対値が最大のもの(spectral radius)を1ではないが1に限りなく近い値として持たなければいけないようです。
このあたりは正直自分も全然理解できていない場所で又別の論文を読む必要があるようです。
プログラム的にはspectral radiusでW_resを割ったあとに0.99をかけておけばなんとかなります。

ここでspectral radiusとx(t)の式との関係を考えた人は鋭いです。W_resが変化するということはu(t)の大きさがモデルに与える影響とほとんど同じ影響をspectral radiusはモデルに与えます。

違う点としては、u(t)の大きさはx(t+1)におけるu(t)項に影響を与え、直近の入力u(t)のx(t+1)に対する寄与率に影響する。
spectral radiusはx(t+1)におけるx(t)に影響を与え、過去のReservoir層の状態すべてのx(t+1)に対する寄与率に影響するという点です。

