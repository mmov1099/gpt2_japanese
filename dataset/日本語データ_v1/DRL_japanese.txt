深層強化学習（DRL）は、強化学習（RL）と深層学習を組み合わせた機械学習のサブフィールドです。RLは、試行錯誤によって意思決定を行うことを学習する計算エージェントの問題を考慮します。Deep RLは、ソリューションにディープラーニングを組み込んでいるため、エージェントは、状態空間を手動で設計しなくても、非構造化入力データから意思決定を行うことができます。。Deep RLアルゴリズムは、非常に大きな入力（たとえば、ビデオゲームで画面にレンダリングされるすべてのピクセル）を取り込み、目的を最適化するために実行するアクション（たとえば、ゲームスコアの最大化）を決定できます。深層強化学習は、ロボット工学、ビデオゲーム、自然言語処理、コンピュータービジョン、教育、輸送、金融、ヘルスケアなど、さまざまなアプリケーションに使用されてきました。深層強化学習とは強化学習と深層学習の手法を組み合わせた物で、代表的な手法に、Deep-Q-Network(DQN)があります。DQNは、Q学習における行動価値関数(Q関数)を、畳み込みニューラルネットワークに置き換えて近似したものです。
ただ単純にQ関数を畳み込みニューラルネットワークを置き換えても、学習がうまくいくわけではないので、学習を収束させるための工夫がなされています。Q学習では、状態数 s × 行動数 a のテーブルを更新することによってQ関数を更新していましたが、状態数が大きくなってくると、テーブルによってQ関数を表すことが現実的ではなくなってきます。これを解決するために、Q関数を畳み込みニューラルネットワークで表現するアプローチをとり、学習が収束するための工夫をしたものがDeep Q Networkです。Deep-Q-Networkの略称で、前述の強化学習における行動価値関数の部分を、畳み込みニューラルネットワーク(CNN)で近似した手法。学習を収束させるための工夫がなされています。DQNでは行動価値関数を畳み込みニューラルネットワーク(Convolutional Neural Network, CNN)で近似しています。CNNは入力値に画像などの行列をとり、畳み込み層のフィルターとプーリング層で入力の特徴量を処理していき、出力に画像のクラス分類予測などの確率を出力します。DQNでは、リサイズされた84×84グレースケールの4時刻分のゲーム画面を入力にとり、ゲーム環境での行動を出力します。DQNの学習を収束させるための工夫として、Experience Replayがあります。Experience Replayは、エージェントが経験した過去の体験をreplay memolyに一定期間保存して置き、過去の経験をランダムにサンプリングして学習を行う手法です。データ間に強い相関があると学習が収束しなくなってしまうことを防ぐために、学習に使用するデータの偏りを無くすのが目的です。深層強化学習では、Q関数の更新が1時刻前(t−1)のニューラルネットワークのパラメータ(重み・バイアス)に依存しています。パラメータは学習ごとに更新されるので、目標とする評価関数が安定しなくなり、学習が収束しにくくなってしまいます。そこで学習を安定させるために、Q関数の更新の際に参照するネットワークを固定します(過去のネットワークで固定して、更新の際に参照するQ関数のパラメータを固定します)。この固定された過去のQ関数は、一定周期ごとに新しいQ関数に置き換えられます。AlphaGoは深層強化学習の技術を用いた囲碁のAIです。2015年10月にヨーロッパ王者のプロ2段の棋士に19路盤でのハンデ無し戦で勝利し、2016年3月にはLee Sedol(9段)に4-1で勝利しました。
囲碁の盤面を19 × 19の画像として扱うことによって、方策・価値ネットワークへの入力としています。AlphaGOはモンテカルロ木探索と複数のネットワークが使用されており、その一部にDQNの技術が使用されています。Qテーブルの学習にディープラーニングを活用することで、たとえば、ビデオゲーム画面などの画像を状態sとしてディープニューラルネットワークに入力して学習し、AIが次のアクションを決定することも可能になります。
深層強化学習は、ゲームだけでなく、エネルギー需給バランス、物流の輸送経路やスケジューリング、生産や在庫の管理、製造装置の制御など、様々な最適化や制御の高度化への適用が期待されます。東芝デジタルソリューションズでは、深層強化学習の応用研究を進め、産業分野を中心に、AIによるシステムの最適化・自律化の実現を目指します。深層学習による特徴抽出と強化学習による予測制御を組合せることで、ゲームAIやロボット制御などの複雑なシステムの制御ができるようになりました。本節では、深層学習が強化学習において果たす役割について考察します。
強化学習においては、環境についての知識は未知であるため（モデルフリー）、探索によって環境について情報収集する必要があります。例えば、囲碁やビデオゲームを習得する場合、囲碁の盤面の石の配置とか、ビデオゲーム画面のキャラクターの配置などから、ゲームの状況を把握して次の一手を決定しなければなりません。このような場合、盤面やゲーム画面などの2次元の画像情報から、ゲームの局面という高次の特徴量を抽出することが要求されます。車の自動運転においても、センサーが取得した画像情報から歩行者や障害物の特徴を把握して適切な操作をしなければなりません。さらに、囲碁の場合で考えると、盤面の特徴を把握して局面を理解できたとして、その後の戦略を何手先までも先読みして最適な一手を選択する必要があります。その場合、一連の行動とそのもたらす結果として、状態と報酬の系列を事前にシミュレーションする必要があります。つまり、状態・行動・報酬の系列データを逐次的に生成する仕組みが必要です。このように、強化学習の適用には、観測データからの特徴量抽出と、予測シミュレーションを可能とする系列データ生成とが必要となります。これら2つの重要な仕組みを提供するのが深層学習です。深層学習（Deep learning）は、ニューラルネットワークの層を多数積んだ深層ニューラルネットワーク（DeepNeural Network, DNN）とそれらを学習するための一連の技術体系をまとめたものです。観測データからの特徴量抽出器としては、畳み込みニューラルネットワーク（Convolutional Neural Network, CNN）が有効であることが知られています。これは、畳み込み演算と言われる演算操作により、一定の拡がりを持つ空間情報を集約して次の層に渡す処理を繰り返しながら、空間情報の特徴量を抽出する方法です。CNNでは、こうして得られた特徴量を利用して様々なタスクに適用することができます。例えば、画像分類問題に適用する場合、画像に対して被写体の分類ラベル（ネコ、イヌ、ヒト）が一意に紐づく画像セットを訓練データとして、画像から分類ラベルを予測するモデルを学習します（図1.9）。その際、CNNによる特徴量出力を全結合ニューラルネットワークからなるラベル予測モデルに渡して学習することで、人間を超える分類精度を達成することができました。系列データ生成器としては、再帰型ニューラルネットワーク（Recurrent Neural Network, RNN）が有効なモデルとして知られています。これは、ニューラルネットワークの層の情報を時間方向にも伝播することで系列データを学習できるようにしたものです。具体的には、ある層の情報を次の層に渡すだけでなく、次時点の同じ層にも再帰的に渡すことで系列データの特徴を把握します。また、こうして学習されたRNNを用いて、逐次的に系列要素を1個ずつ予測しながら系列データを生成することができます。この技術は、言語の自動翻訳に応用され、高い精度の翻訳を実現できるようになりました。こうした深層学習による特徴量抽出と系列データ生成を強化学習に適用して、これまで制御が難しいと考えられていたタスクを制御できるようにする試みが深層強化学習です。機械学習の一分野である深層強化学習により、ロボットや自律システムなどの複雑なシステムのコントローラーや意思決定システムを実装できます。深層強化学習では、シミュレーションや物理システムから動的に生成されたデータを使用して学習を行うことで、複雑な動作を学習できるディープ ニューラル ネットワークを実装することができます。他の機械学習技術とは異なり、ラベルがあるかないかにかかわらず、事前定義された学習データセットは必要ありません。一般的に必要なものは、対象の環境を表現するシミュレーション モデルだけです。深層強化学習エージェントは、入力としての状態から出力としてのアクションへのマッピングを実行するディープ ニューラル ネットワーク方策と、該当する方策を更新するアルゴリズムで構成されます。広く利用されているアルゴリズムは、Deep Q-Network (DQN)、Deep Deterministic Policy Gradient (DDPG)、Soft Actor Critic (SAC)、Proximal Policy Optimization (PPO) などです。アルゴリズムは、環境から収集した観測値と報酬に基づいて方策を更新し、期待される長期的な報酬を最大化します。深層強化学習アルゴリズムを用いた学習は、エージェントが周囲の環境とやりとりする動的なプロセスです。ロボティクスや自律システムのなどのアプリケーションの場合、実際のハードウェアを使用してこの学習を行うことは費用がかかるうえに危険を伴います。そのため、深層強化学習では、シミュレーションを通じてデータを生成する環境の仮想モデルが推奨されています。倒立振子のバランス制御、迷路の探索、カートポールでのバランス制御などの簡単な問題について方策の学習を行うことで、深層強化学習を開始します。自律走行車向けに、アダプティブ クルーズ コントロール (ACC) および車線維持支援用のシステムを設計することもできます。また、深層強化学習は、軌道計画などのロボティクス アプリケーションや、歩行などの動作指導にも利用できます。強化学習とディープラーニングの融合は、旧来の強化学習に大きな技術的進展をもたらし、社会・ビジネスへの活用が大幅に進む契機となりました。深層強化学習アルゴリズムを使用してタスクを解決するためのポリシーをトレーニングするためのさまざまな手法が存在し、それぞれに独自の利点があります。最高レベルでは、モデルベースの強化学習とモデルフリーの強化学習が区別されます。これは、アルゴリズムが環境ダイナミクスのフォワードモデルを学習しようとするかどうかを示します。モデルベースの深層強化学習アルゴリズムでは、通常、ニューラルネットワークを使用した教師あり学習によって、環境ダイナミクスのフォワードモデルが推定されます。次に、学習したモデルを使用したモデル予測制御を使用してアクションを取得します。真の環境ダイナミクスは通常、学習したダイナミクスとは異なるため、エージェントは環境内でアクションを実行するときに頻繁に再計画します。選択されたアクションは、クロスエントロピー法などのモンテカルロ法、またはモデル学習とモデルフリー法の組み合わせを使用して最適化できます。強化学習とは、機械学習のアルゴリズムのひとつであり、「システム自身が試行錯誤を繰り返して最適なシステム制御を実現していく仕組み」のことを指します。機械学習には、教師あり学習や教師なし学習のように、明確なデータをもとにした学習方法も存在しますが、強化学習の場合は明確なデータをもとにするわけではありません。プログラム自体が与えられた環境の観測を行い、一連の行動結果を踏まえた上で、より価値のある行動を学習していくという仕組みです。そして、その行動についての評価も自ら更新していきます。さまざまな行動を試しながら、より価値のある行動を探していくという点を踏まえると、人間の動作に近いものといえるかもしれません。
そんな強化学習ですが、この概念自体は近年のAIブームよりも前から存在していました。強化学習の原型といえるものは、機械の自律的制御を実現する「最適制御」の研究が行われていた1950年代から存在していたのです。なお、1990年代には強化学習の生みの親とされるリチャード・サットン教授（カナダ・アルバータ大学）を中心としたチームにより研究が進められていたといいます。このように、強化学習の原型といえるものは古くから存在していたわけですが、そこに飛躍的な進歩をもたらしたのが「深層強化学習」というものです。これは、従来の強化学習に深層学習（ディープラーニング）を応用したものであり、強化学習を軸として稼働するAIが世間に広まるきっかけとなりました。強化学習には、Q-Learning、SARSA、モンテカルロ法という3つのアルゴリズムが存在します。3つの手法の中で一番多く用いられているのが、Q-Learning（Q学習）です。強化学習について勉強していく際は、まずQ-Learningから学ぶことになるでしょう。Q-Learningは、Q関数という行動価値関数を学習し、制御を行っていく仕組みです。行動価値関数Q(a|s)は、状態s(t)において行動aを行った場合、その先の報酬はどれくらいもらえるかの予想を出力していきます。Q関数に行動「右に押す」と「左に押す」を入力した場合の出力を比較し、より報酬が多いほうを選択すると、CartPoleが立ち続けることになるわけです。SARSAは、Q関数を学習するという点ではQ-Learningと同じですが、学習の仕方に違いがあります。「実際に行動した結果」を用いて、期待値の見積もりを置き換えていくのが特徴です。そのため、現在の価値を更新するには、再度エージェントが行動を行わなくてはなりません。モンテカルロ法は、Q値の更新において「次の時点のQ値」を使用しないという点が特徴です。何かしらの報酬が得られるまで行動を行い、報酬値を把握します。そして、辿ってきた状態・行動に対して、報酬を分配するという仕組みです。ディープラーニングの発達に伴い、現在は強化学習においてもディープラーニングが積極的に活用され始めています。ディープラーニングの発達によって登場したのが深層強化学習と呼ばれる技術です。これまでは、Q関数を表すために表を使用するのが一般的となっていました。表のサイズは「状態sを離散化した数」×「行動の種類」という計算によってに決まるため、限りがあります。
しかし、ディープラーニングを用いたDQN(Deep Q-Network 、Deep Q-Learning Network)が実用化されたことで、これまで以上に複雑なゲームや制御問題を解決できるようになったのです。このディープラーニングを活用した強化学習のことを「深層強化学習」と呼びます。自動車における自動運転も、強化学習が活用されている分野のひとつです。Prefferd Networks社という日本の企業が行っている研究では、自動車の幅に対して道の幅が狭く、車が密集している交差点のような難しい状況下において、強化学習でどれだけ運転の精度を高められるかという実験が行われています。この技術を用いることにより、前後左右のすべての方向を集中してみることが可能になるため、前方向と同じように後方向にも躊躇なく移動することができるそうです。近年は高層ビルが増加していることもあり、エレベーターの制御は極めて重要な役割を担っています。ただし、エレベーターの安全性を高めるのはもちろんのこと、エレベーターの利便性を高めることも、良いエレベーター制御システムの条件のひとつです。そのため、客の待ち時間が長くなってしまうエレベーターの制御システムは高く評価できません。特に、デパートやオフィスビル、タワーマンションといった、毎日大勢の人々が乗り降りする場所には欠かせない条件といえるでしょう。エレベーターは簡単に増設することもできませんから、台数と定員を変えずに待ち時間を短くする必要があるのです。
そこで活用されるのが、強化学習です。数理的な手法で割り当てを行うにしても限界があるため、強化学習によって過去の経験にもとづいた最適な選択肢を選ぶことで、より柔軟にエレベーターを稼働させることが可能になります。もちろん、日々の運行履歴も学習していくため、新たな学習データを追加した上で、より最適な判断方法にアップデートしていくことも可能なのです。
そのため、エレベーターの待ち時間が長くなってしまっている建物などは、特に強化学習を導入するメリットが大きいといえるでしょう。強化学習の代表的な活用事例として挙げられるのが、2016年に登場した囲碁AIの「AlphaGo」です。プロ棋士に勝利をしたことで大きな話題を呼びました。そんなAlphaGoには、Googleが開発したディープラーニングと強化学習を組み合わせた深層強化学習が活用されています。囲碁だけでなくさまざまなゲームにおいて圧倒的な強さを発揮しており、もはや深層強化学習の知名度を高めるきっかけとなった存在と言っても過言ではありません。AlphaGoには、「モンテカルロ木探索」と呼ばれる探索型AIが搭載されています。このAIは、「統計的に勝つ確率の高い一手」を算出することを目的としたもので、囲碁のように「明らかな正解が存在するケース」に対応可能です。ただし、モンテカルロ木探索は、ある程度盤面の選択肢が絞り込まれた状況でなければ使用することができません。そのため、「盤面評価」と「戦術予測」を実行する「深層強化学習を搭載したAI」を活用し、分析を進めていくわけです。そんな優れたAIを搭載するAlphaGoですが、現在は進化版として「AlphaZero」というものも登場しています。この「AlphaZero」は、AlphaGOを破ることにも成功しており、チェスや将棋といった別の種目にも対応していることから大きな注目を集めているのです。今後どのレベルまで成長を続けるのか、期待を寄せられています。レコメンドサービスやレビュー管理サービスなどを提供している「ナビプラス」では、自動最適化機能に強化学習を活用しています。ナビプラスが提供するサービスの一つに「NaviPlusレコメンド」というものがあります。このサービスは、Webサイトのパーソナライゼーション強化を支援することが目的です。
そのWebサイトにとって最適なレコメンドを実現するためには、一連の行動結果を踏まえた上で、より価値のある行動を学習していく仕組みが欠かせません。その仕組みを実現することができるのが、強化学習なのです。
たとえば広告を表示させるとき、「AとBどちらが高いクリック率を実現できるか」という点においては、初めから正解が存在するわけではありません。実際に試しながらデータを収集していく必要があります。
「実際に試して得られる報酬を最大化するための戦略」を練る上で、強化学習は非常に重要な役割を担っているのです。コンテンツのレコメンドにも、強化学習は活用されています。例えばNetflixでは、「流行」「視聴率」「ストーリー性」「離脱率」といったさまざまなデータをAIに学習させ、ユーザーごとに最適なコンテンツをレコメンドする仕組みとなっているのです。そのため、「ユーザーが満足しそうなコンテンツ」が優先的に表示されるようになり、より満足度を高めやすい環境が実現されています。Netflixではオリジナル作品の制作にも力を注いでいるため、今後データが蓄積されることによって、よりユーザーに適した作品が増加していくことも期待できるでしょう。