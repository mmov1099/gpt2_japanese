ディープラーニングの仕組みの１つに、ニューラルネットワークと呼ばれるものがあります。これは、人間の脳に存在する神経細胞を真似たものです。
オートエンコーダは、ニューラルネットワークの仕組みの１つです。入力されたデータを、後で復元できる状態に圧縮する処理を指します。通常、データを圧縮すると情報の一部が欠落します。しかし、欠損部がなくても再現できるよう、重要度の高い情報を残しておかなければなりません。そこで、オートエンコーダでは、重要度の高い情報を洗い出し、それ以外の部分を削ぎ落します。たとえば、体格を表すのに、身長と体重の関係を二次元グラフ上のプロットで示したり、プロット結果を近似した一本の直線（一次元）で表したりすることがあります。オートエンコーダでは、その意味を維持したままより少ない次元に落とし込むことが行われます。この作業を「次元削減」または「特徴抽出」と呼びます。ニューラルネットワークは、入力値と出力値の間に「隠れ層」と呼ばれる圧縮状態を挟むのが特徴です。このようにすることで、従来では不可能だった複雑な処理も可能になりました。そこで、さらに多くの隠れ層を挟むことで、より高度な処理能力が実現するのではないかと考えられるようになりました。ところが、実際に試みたところ、かえって性能が下がりました。この方法では予測と正解の誤差のフィードバックが必要ですが、階層をさかのぼるごとに誤差が減少し、学習速度が低下したのです。この問題を「勾配消失」と言います。オートエンコーダは勾配消失を解決する方法として期待されました。ニューラルネットワークの初期値に、オートエンコーダで学習させたものを用います。事前学習を行うことで、勾配消失による学習速度低下を防止します。過学習とは、特定のデータへの対応にのみ長けてしまうことです。訓練に使ったデータを完全に記憶してしまうと、処理の練習になりません。結果的に、未知のデータにはまったく対処できない状態になります。
そこで活用されたのがオートエンコーダです。オートエンコーダによりデータを粗な状態にすることで、過学習を防止します。
オートエンコーダを用いた事前学習による、勾配消失の防止手順は以下のとおりです。
１．層を分割する
ネットワークを構築する多層構造を、複数の単層に分割します。入力層、隠れ層１、隠れ層２……出力層と分割され、それぞれに事前学習を行います。
２．入力層から学習させる
分割する前の構造を想定し、入力層から順に教師なし学習を行います。隠れ層１と同じサイズの訓練用隠れ層を１つだけ用意し、圧縮・出力させます。
３．すべての隠れ層で訓練を繰り返す
ステップ２で得た訓練用隠れ層の値を入力値として、隠れ層１の訓練を行います。この際の訓練用隠れ層は、隠れ層２を想定し、それと同じサイズにします。この作業をすべての層で繰り返しましょう。
４．元のネットワークに戻す
学習させた層で元のネットワークを構築します。この状態で新しい層を追加して学習させると、勾配消失を防止できます。
オートエンコーダとは、ニューラルネットワークの1つです。入力されたデータを一度圧縮し、重要な特徴量だけを残した後、再度もとの次元に復元処理をするアルゴリズムを意味します。このように、小さい次元に落とし込む作業を次元削減や特徴抽出と呼びますが、オートエンコーダはそれだけでなく、生成モデルとしても用いられます。オートエンコーダは、2006年にトロント大学のコンピュータ科学および認知心理学の研究者であるジェフリー・ヒントン氏らによって提唱されました。ディープラーニングは、このオートエンコーダを何層にも重ね合わせてできた構造を持っています。オートエンコーダの仕組みはそのままディープラーニングの仕組みだと言えるでしょう。オートエンコーダは入力層のノードでデータを受け取り、隠れ層に圧縮します。この時に重みづけと呼ばれ、データはその重要度にあわせて点数がつけられます。この点数が低いデータは除外されます。これをエンコードと言います。データが出力層に移る時も重み付けされ、ノードが複数のエッジから受け取ったデータの合計が最終的な値になります。これをデコードと言います。ニューラルネットワークにおいて、層を重ねることで、より複雑な処理ができます。主に採用されている計算方法である誤差逆伝播法においては、層を重ねすぎると逆に精度が落ちてしまいます。誤差逆伝播法では、予測と正解の誤差を利用して学習していくのですが、その誤差は層を重ねるとともに消えていってしまい、最終的に精度が落ちてしまいます。これを勾配消失と言います。事前学習を採用しているオートエンコーダは勾配消失を避けられます。過学習とは、学習の訓練データに適合しすぎて、評価データに対応できなくなることです。過学習になると、学習データだけに最適してしまい、汎用性が失われ、実際に使うことができません。この解決策として、ちょうど良いタイミングで学習をストップさせる方法や、学習データ数を増やして解決する方法もありますが、オートエンコーダを用いることでも解決できます。次元を圧縮するとデータが荒くなるので、過学習を防ぐことができます。積層オートエンコーダは、先述のオートエンコーダを何層にも重ねたシンプルな構造をしています。1層ずつ学習していくのがポイントで、一気にエンコードして、一気にデコードするのではなく、エンコードとデコードを交互に実施しています。1層ずつ学習していくことで、初期値を最適解に近づけることができ、ファインチューニングすればすぐに使えるようになります。現在、技術の発展により、積層オートエンコーダのメリットをカバーできるようになったため、あまり使われていません。積層オートエンコーダは、始めに示したシンプルなオートエンコーダのエンコーダおよびデコーダ部分を多層化した構造となります。入力データはエンコーダにおいて段階的に次元を減らし、デコーダで復元されます。エンコーダおよびデコーダを多層化することで、より複雑で高度な特徴量抽出を狙っています。
当初このような深いニューラルネットは、勾配消失という問題によってうまく学習することができませんでした。これは、層を深くしていくと前半の層で勾配がほとんどゼロになってしまい学習が進まなくなる、という問題です。これを解決するために、積層オートエンコーダは、ニューラルネットの各層における学習を一層ずつ行って、最後にすべて積み重ねる、という手順で構築します。入力データセットを復元するように1層目の学習を行った後、得られた重みを固定値として使って次の層である2層目の学習を行います。これを繰り返して、3層目までの重みを得ることができます (事前学習）。最後に1層目から3層目をすべて繋げて元々のネットワークを構築し、得られた重みを初期値として設定したのち、全体の学習を通して重みを微調整（ファインチューニング）します。1層ずつの学習とすることで初期値が最適解に近き、適切な学習が可能となります。他にも、ノードを減らしていくというオートエンコーダのネットワーク構造自体が、ディープニューラルネットワークのもう一つの課題であった過学習の抑止に繋がるというメリットがあります。
しかし、その後の技術の発達により、ディープニューラルネットワークは事前学習無しでも適切な学習が可能となり、積層オートエンコーダをこのような事前学習に用いることはほぼなくなっています。
オートエンコーダは、ただデータの圧縮と復元をするだけでしたが、変分エンコーダはデコードする際に変数を混ぜることで、入力とは少し違う出力をします。そのため、変分エンコーダは生成モデルとして有名です。変分オートエンコーダは、生成モデルとして有名です。通常のオートエンコーダとの大きな違いとして、入力データを圧縮して得られる特徴ベクトル（潜在変数）を確率変数として表します。一般的にはＮ次元の潜在変数が、Ｎ次元正規分布に従うように学習します。これまでに紹介したオートエンコーダでは、次元削減後の特徴ベクトルには特に制約はありませんでした。そのため、特徴空間上でデータがどのように表現されているかはわかりません。しかし、VAEでは、ここに正規分布という制約を設けることでデータの潜在空間上での分布に連続性が生じ、似た潜在変数からは似たデータが生成されるようになります。エンコーダの出力として平均と標準偏差を推定し、それらで表される正規分布からランダムサンプリングによりデコーダに入力する潜在変数を決めます。しかし、このランダムサンプリングという操作は微分不能という問題があります。誤差逆伝搬（バックプロパゲーション）し、ネットワークを学習させるためには、各ノードをつなぐエッジが微分可能な演算でなければいけません。そこでReparameterization trickという方法を用います。標準正規分布からランダムにサンプリングして得る確率変数εを導入し、これを用いて  として潜在変数 を決定します。このようにすることで と は微分可能なエッジで繋がり、バックプロパゲーションが可能となります。条件付き変分オートエンコーダは、変分オートエンコーダを拡張したもので、指定したクラスのデータを生成できるようになっています。変分オートエンコーダでは、デコーダによるデータ生成の際、潜在変数を指定することはできますが、出力データのクラスを直接指定できません。言い換えますと、潜在変数を変えると出力クラスも変わってしまう可能性があります。条件付き変分オートエンコーダは、出力クラスを指定したうえで、潜在変数も自由にコントロールすることができます。変分オートエンコーダのネットワークと異なる点は、エンコーダ、デコーダともにラベル情報を入力していることです。ラベル情報の入力にはいくつかやり方があります。例えば、デコーダの場合1次元の潜在変数ベクトルに結合する方法や、足し合わせる方法などです。このようにラベル情報を学習に含めることで、学習後、指定したラベルのデータを生成することができるようになります。

畳み込みオートエンコーダ(CAE)は、畳み込みニューラルネットワーク（CNN）を用いたオートエンコーダです。CNNとは、入力層と出力層の間に、入力データの特徴量を捉える「畳み込み層」と、その特徴への依存性を減らす「プーリング層」を加えたニューラルネットワークのモデルです。
畳み込み層により特徴を抽出できるため、画像の処理に利用することが多いです。
先述の通り、学習データを入力すると、まだ存在しないデータを出力できます。たとえば、数字が書かれた画像を入力すると、あたかも他の人が書いたかのような新しい画像を生成できます。異常検知は、入力されたデータが正常か異常か判断し、異常を検知することです。オートエンコーダは教師なしで異常検知できることが特徴です。教師ありで異常検知する場合は正常データと異常データの2つをデータセットとして学習させなければいけませんが、教師なしでは、正常データのみで学習できます。また、正常データを集めるのは容易なので、異常検知はオートエンコーダにうってつけの作業です。
正常データだけで学習すると、復元後は正常データに近いデータが出力されます。つまり、異常データを入力すると、正しく復元できず、もとの入力データとの誤差が大きくなります。その誤差を用いることで、正常か異常か判断します。また、誤差が大きい場所を見れば、異常が発生している場所を特定できます。
ノイズ除去は、入力データ中の不必要な部分を除去することです。オートエンコーダにノイズのあるデータを入力し、ノイズのないデータを正解データとして、教師あり学習をさせます。学習後にデータを入力すると、ノイズのない綺麗なデータを出力するように、エンコードされます。
クラスタリングは、データの特徴ごとに分類することです。オートエンコーダでは、エンコード時に特徴が抽出されるので、その特徴ごとにクラスタリングできます。こちらも正常データと異常データの2つを学習させれば、異常検知に用いることができます。勾配消失や過学習を避けるために開発されたオートエンコーダですが、現在はその目的で利用されてはおらず、データ生成や異常検知のために使用されています。オートエンコーダ とは、NN (Neural Network) を用いて実現される次元圧縮・特徴抽出に使われる機械学習アルゴリズムです。2006 年に Geoffrey Hinton 教授が提案しました。
基本のオートエンコーダは入力層と出力層が同じになるように学習をすすめます。
オートエンコーダの学習は、入力データと一致するデータを出力することを目的とする教師なし学習です（後述のように教師あり学習とすることもできます）。オートエンコーダのネットワークは、入力したデータの次元数をいったん下げ、再び戻して出力するという構造になっています。このため、入力から出力への単なるコピーは不可能です。オートエンコーダの学習過程では、入出力が一致するように各エッジの重みを調整していきます。この学習を通して、データの中から復元のために必要となる重要な情報だけを抽出し、それらから効率的に元のデータを生成するネットワークが形成されます。こうしてオートエンコーダの前半部分は次元削減、特徴抽出の機能を獲得し、後半部分は低次元の情報をソースとするデータ生成機能を獲得します。前半部分をエンコーダ、後半部分をデコーダと呼びます。
学習後、この2つのネットワークは別々に使うことができます。すなわち、エンコーダは特徴抽出器、デコーダは生成器として独立に用いることができます。

