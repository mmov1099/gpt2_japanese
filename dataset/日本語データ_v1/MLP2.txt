MLPは一般には3層のニューラルネットワークのことで、深層学習の深層でないバージョンと考えることのできるモデルです。
パーセプトロンは単純パーセプトロンとも呼ばれ、人間の脳の神経細胞を模して考えられたアルゴリズムです。パーセプトロンは2層のニューラルネットワークと考えることができ、複数の入力からスカラーの出力を得るモデルです。しかし一度の線形変換のみから構成されるこのモデルでは線形分離不可能なデータをうまく識別することができず、うまく識別できるデータは限られていました。
ニューラルネットワークは脳の神経細胞を模して作られた機械学習モデルの総称であり、単純パーセプトロン、MLP、深層学習のいずれもニューラルネットワークと呼ばれます。
深層学習は多層（4層以上）から成るニューラルネットワークのことで、各層での線形変換に加え別途活性化関数と呼ばれる非線形な変換を行うことで、単純パーセプトロンで表現しきれない線形分離できないデータに対してもよくフィットするモデルです。現在の人工知能ブームを牽引しているのがこの深層学習であり、画像認識の分野で広く成功を収めている畳み込みニューラルネットワーク、自然言語処理の分野で成功を収めていた（現在はAttentionベースの手法が多いが）再帰ニューラルネットワークなどの手法が様々提案されています。
MLPとは一般には3層から成るニューラルネットワークのことであり、2回の線形変換とそれぞれに対応する活性化関数で構成されます。MLPはパーセプトロンの欠点であった線形分離不可能な問題に対応するために、一度線形変換を行った後にsigmoid関数と呼ばれる非線形関数を適用しています。
しかしMLPには層を何層も重ねると勾配消失問題によって学習がうまく進まなくなるという問題がありました。勾配消失問題とは誤差逆伝播で学習を行う際に、途中で勾配の値が小さくなりすぎることで手前の層の学習がうまく行えなくなる現象のことです。深層学習ではReLUと呼ばれる活性化関数やその他学習のテクニックを用いることでこの問題を解決し、さらなる進化を遂げました。
機械学習のアルゴリズムの１つにニューラルネットワークがあります。このニューラルネットワークの基本になっているのが、人間の脳を模した構造を数理モデル化した「パーセプトロン」です。人間の神経細胞のように細胞と細胞を多層に組み合わせたモデルを多層パーセプトロン（MLP）と呼んでいます。
AI（人工知能）は、Artificial　Intelligenceの略で、一般的には、「コンピュータ上に人間のような知能を再現する技術」のことです。もともと1956年夏にアメリカのダートマス大学で開催されたダートマス会議において、初めて初めて人間のように考える機械のことを「AI（人工知能）」と呼ぶようになりました。
AI（人工知能）には、3回のブームがあります。第1次AIブームは、1950年代〜1960年代で、コンピュータに「探索・推論」させることによって、問題を解かせる研究が中心でした。第2次AIブームは、1980年代。コンピュータに「知識」を入れるアプローチでの研究が盛んに行われました。そして、現在、ブームの真っただ中である、第３次AIブームではデータからコンピュータが特徴量を抽出し、予測や回帰を行うディープラーニングがブームを牽引しています。
ディープラーニングは、ニューラルネットワークの一種です。ニューラルネットワークとは、脳機能に見られるいくつかの特性に類似した数理的モデルのことで、分類や回帰などで非常に高い精度で予測できることが期待されます。
人間の脳は、神経細胞（ニューロン）と神経回路網（シナプス）で構成されておいます。神経細胞（ニューロン）は電気信号として情報を伝達し、神経細胞（ニューロン）と神経細胞（ニューロン）をつなぐ神経回路網（シナプス）のつながりの強さによって、情報の伝わりやすさが変わります。この構造を模して、ニューラルネットワークでは、神経細胞（ニューロン）にあたるものを「ノード」、神経回路網（シナプス）にあたるものを「エッジ」として呼び、神経回路網（シナプス）のつながりの強さを「重み」として表現をしています。
ニューラルネットワークは、複数のノードとエッジが2層以上につながってネットワーク上の構造のことで、入力データと出力データの誤差が最も小さくなるように重みを調整して、学習を行います。
また複数のノードが多層につながっており、ネットワーク上になっているものをディープニューラルネットワークと呼びます。ディープニューラルネットワークによって、複雑な情報に対応できるようになりました。
そのため、現在のディープラーニングは、画像認識、音声認識、自然言語処理や異常検知など様々な分野に応用されています。
多層パーセプトロン（MLP）は、ニューラルネットワークの1つで少なくとも３つのノードの層から構成されます。多層パーセプトロンの特徴は、複雑な問題を解ける点です。
多層パーセプトロン（MLP）の学習では、バックプロパゲージョン（誤差逆伝播法）と呼ばれる学習方法を用いて行います。これは、入力データに対して、最初に重みを設定して、出力データを作ります。その出力データに対して、入力データを与え、入力データとの誤差が最小になるように各重みの値を少しだけ増減させ調整をします。それを繰り返すことによって、データをうまく分類できるようにします。これにより、あらゆるデータに対して機械学習を行えるようになりました。
多層パーセプトロン（MLP）では、層を重ねると複雑な処理ができる一方で、誤差伝播法においては、層を重ねすぎると誤差が入力層に伝えられるときには非常に小さくなってしまって、精度が逆に下がってしまうという勾配消失という欠点があります。誤差伝播法とは、出力と正解データとの誤差を入力とは逆方向にフィードバクをすることで、学習の訓練データに適合過ぎて汎用性が失われてしまう過学習という状態に陥りやすい傾向があります。
多層パーセプトロン(MLP)と単純パーセプトロンの違いとは、パーセプトロンが多層かどうかの差です。単純パーセプトロンは、複数の入力に対して出力は単一になるパーセプトロンのこと。つまり、多層パーセプトロン(MLP)にあった中間層がありません。そのため、単純パーセプトロンは０か１のどちらかの出力しか表現できません。
例えば分類問題を解くときは、線形の分類しか対応できません。一方多層パーセプトロン(MLP)は、多層な構造で出力も様々な形で表現ができるため非線形の分類問題にも対応できます。
そのため、例えば線形の分類のような単純な分類などは単純パーセプトロン、それ以外の非線形の分類のような複雑な事項の処理は多層パーセプトロンを使います。
ディープラーニングと多層パーセプトロン（MLP）の関係とは、多層パーセプトロン（MLP）の欠点を解決したものがディープラーニングです。
多層パーセプトロン（MLP）は、層が多層になればなるほど、誤差が入力層に伝わりにくく、精度が落ちてしまう欠点や学習データに適合しすぎて汎用性が失われてしまいます。しかし、ディープラーニングを行うためには、ニューラルネットワークを多層にする必要があります。そうすると、誤差が消失したり、過学習を起こしてしまいます。
そこで多層にしても学習がうまくいくようにブレイクスルーを起こしたのが、オートエンコーダ。オートエンコーダとは、入力データを中間層で次元を圧縮することです。オートエンコーダの登場により、多層パーセプトロン（MLP）の層を多層にした際も、誤差をうまく調整でき、過学習を防ぐということを実現できました。
オートエンコーダが登場して、多層パーセプトロンからさらに進化したディープラーニングが登場しました。
AI（人工知能）は、一般的には、コンピュータ上に人間のような知能を再現する技術である
ディープラーニングとは、人間が行うタスクをコンピュータに学習させる機械学習の手法のひとつである
多層パーセプトロン（MLP）は、機械学習の一種であるニューラルネットワークの1つである
多層パーセプトロン（MLP）と単純パーセプトロンの違いは、複雑な出力が表現できるかどうかである
多層パーセプトロン（MLP）とディープラーニングの関係性としては、多層パーセプトロンの欠点を解決したものがディープラーニングである
多層パーセプトロン（MLP）が使えると、ニューラルネットワークの基礎を習得できます。この記事で多層パーセプトロン（MLP）について理解して、ディープラーニングの実装に役立てましょう。
多層パーセプトロンは複数の形式ニューロンが多層に接続されたネットワークを指します。
単純パーセプトロンは入力層と出力層のみであったのに対し、多層パーセプトロンは中間層(隠れ層)と呼ばれる、層が複数追加されたネットワーク構造を持ちます。
単純パーセプトロンと違い複数のクラス分類を可能とし、線形分離不可能な問題も解くことができます。
現在は、この多層パーセプトロンの形式を拡張したものがよく使われています。
多層パーセプトロン（たそうパーセプトロン、英: Multilayer perceptron、略称: MLP）は、順伝播型（英語版）ニューラルネットワークの一分類である。MLPは少なくとも3つのノードの層からなる。入力ノードを除けば、個々のノードは非線形活性化関数を使用するニューロンである。MLPは学習のために誤差逆伝播法（バックプロパゲーション）と呼ばれる教師あり学習手法を利用する。その多層構造と非線形活性化関数が、MLPと線形パーセプトロンを区別している。MLPは線形分離可能ではないデータを識別できる。
多層パーセプトロンは時折、特に単一の隠れ層を持つ時、「バニラ」ニューラルネットワークと口語的に呼ばれることがある。
多層パーセプトロンが全てのニューロンにおいて線形活性化関数、すなわち、個々のニューロンの出力に重み付けされた入力（英語版）をマップする線形関数を持つとすると、線形代数から、いかなる数の層も2層からなる入力-出力モデルに削減することができることが示される。MLPでは、一部のニューロンは、生物学的ニューロンの活動電位の頻度および発火をモデル化するために開発された「非線形」活性化関数を用いる。
MLPは、非線形的に活性化されるノードの3つ以上の層（入力層と出力層と1つ以上の「隠れ層」）からなり、ディープニューラルネットワークを作り出す。MLPは全結合（fully connected）のため、1つの層中のそれぞれのノードは次の層中の全てのノードと任意の重みで結合している。
学習は、個々のデータが処理された後に、期待される結果と比較した出力中の誤差の大きさに基づいて、結合加重を変化させることによってMLPにおいて起こる。これは教師あり学習の一例であり、誤差逆伝播法（バックプロパゲーション）を用いて実行される。誤差逆伝播法は、線形パーセプトロンにおける最小二乗法アルゴリズムの一般化である。
「多層パーセプトロン」という用語は、複数の層を持つ単一のパーセプトロンを意味しない。むしろ、層へと組織化された多くのパーセプトロンを含む。代替用語は「多層パーセプトロンネットワーク」である。さらに、MLP「パーセプトロン」は、最も厳密に言えばパーセプトロンではない。真のパーセプトロンは正式には、ヘヴィサイドの階段関数といった閾値活性化関数を用いる人工ニューロンの特殊な場合である。MLPパーセプトロンは任意の活性化関数を用いることができる。真のパーセプトロンは二項分類を実行する。対して、MLPニューロンは、その活性化関数に依存して分類あるいは回帰のどちらを実行するかは自由である。
「多層パーセプトロン」という用語は後に、ノード/層の特性に関係なく適用されるようになった。ノード/層は、パーセプトロンに限定されず、任意に定義されたニューロンから構成することができる。この解釈は、一般に人工ニューロンを意味するところの「パーセプトロン」の定義の緩和を避けている。
MLPは、確率的に問題を解くことができるため研究において有用である。MLPは適応度近似（英語版）のような極めて複雑な問題に対する近似解をしばしば与える。
MLPはCybenkoの定理（英語版）によって示されているように普遍的な関数近似器であるため、回帰分析によって数理モデルを作成するために使うことができる。分類は、応答変数がカテゴリ一変数である時の回帰の特殊な例であり、MLPはよい分類アルゴリズムを作る。
MLPは1980年代に人気のある機械学習法であり、音声認識や画像認識、機械翻訳ソフトウェアといった多様な分野に応用されたが、その後より単純なサポートベクターマシンとの激しい競争に直面した。ディープラーニングの成功によってバックプロパゲーションネットワークへの関心が戻った。
多層パーセプトロン（MLP）とは人間の脳の神経細胞であるニューロンの働きをモデル化したパーセプトロンの構造を多層化したものです。ただし、これだけではわかりにくいのでそれぞれについて順に説明しましょう。
まず、人間の脳にあるニューロンとは電気信号を伝達するための細胞です。具体的にはコアとなる細胞体と軸索、樹状突起、シナプスなどによって1つのニューロンを構成しており、これらが複雑に結びつくことで脳という回路を構成します。
仕組みとしては上位のニューロンから流れてきた電気信号を樹状突起で受け取り、シナプスと細胞体で処理を行って化学反応させます。そして、この反応によって電気信号を発生させるのですが、この際に出力するかどうかは閾値を超えるのかで決めます。なお、閾値を超えない場合には電気信号は出力されません。
あとは発生した信号を軸索を経由して次のニューロンに伝えます。以上がニューロンの働きと仕組みになり、これを以下のようにモデル化したのがパーセプトロンです。
パーセプトロンは入力層と出力層の2層で構成され、入力に対して重みを付加して総和し、ニューロンで行った出力の判断は活性化関数（ステップ関数など）を利用して処理される仕組みです。このパーセプトロンの考え方は画期的なものであり、実際に人間の小脳も同じような仕組みで働いていることが後に実証されました。
しかし、パーセプトロンは単純なものであれば問題ないですが、複雑な事象を扱うと線形分離できない課題があるとわかったのです。この線形分離可能とは、平面上にあるAという分布とBという分布が存在していた場合に、両者の間に直線を引くことが可能であるということで、可能であれば機械にも数式として理解できます
一方で直線が引けない場合では、Aという分布とBという分布の区切りがぐちゃぐちゃな線になってしまうので、数式化することができず機械には理解できません。そこで考え出されたのがパーセプトロンを多層化する多層パーセプトロンでした。
パーセプトロンに中間層が加えられることで出力層への重みvが加わり、それぞれの層で総和されます。そして、最終的な出力の判断は非線形分離可能な非線形活性化関数（ReLU）で行われる仕組みです。
こちらは(−∞, +∞)の入力に対して(−1, +1)を出力するハイパボリックタンジェント（tanh）を利用する方法で、xが0以上であればそれに対応した値を出力し、0以下であれば出力を行いません。これによってパーセプトロンの活性化関数よりも多層パーセプトロンでは複雑な表現が可能になるため、線形分離できないものでも理解できるようになります。
多層パーセプトロンについての解説は以上です。これらを踏まえて多層パーセプトロンの長所と課題点をお伝えしましょう。
多層パーセプトロンの長所は、パーセプトロンの短所である線形分離不可能なものでも対応できることでした。ここではこの長所について論理演算を利用して説明しましょう。
なお、論理演算とはコンピューターや回路などで利用されている演算で、真と偽の状態を1、0で表すことでさまざまな演算を行えるものです。こちらについては論理和，論理積，否定論理積，排他的論理和の４種類の論理演算などがあります。
1つの直線で両者を分けることができないため、線形分離不可能です。つまり、排他的論理和をパーセプトロンでやろうとすると機械は数式化できないので理解できません。
しかし、これを多層パーセプトロンで行った場合には線形分離が可能になります。具体的には複数の論理演算のパーセプトロンを組み合わせ、以下のような多層パーセプトロンにすることで排他的論理和を表現できます。
これがどういうことかといえば排他的論理和は入力1、入力2のどちらかが真であり、両方が真でない場合に出力が真になる演算です。そのため、右の論理積の入力1を論理和、入力2を否定論理積のパーセプトロンにして多層パーセプトロンを構成すれば実現できます。
したがって、パーセプトロンでは線形分離不可能だった排他的論理和も多層パーセプトロンでは線形分離が可能になります。以上のように多層パーセプトロンであれば対象が複雑な場合でも線形分離できます。
多層パーセプトロンであれば線形分離不可能な問題もクリアできますが、課題点がないわけではありません。なぜなら、多層化することで学習が進まなくなる勾配消失問題が生まれるからです。
パーセプトロンや多層パーセプトロンなどでは何をもって学習を進めていくのかといえば、重みWを更新していくことがポイントになります。こちらについては出力結果のデータと教師データを比較した誤差関数、または損失関数と呼ばれるものを使い、その関数の傾き（勾配）がどうなっているのかで重みを更新します。
イメージとしては高校数学で習ったy=x2 +1などの関数の極小値を求めていく作業に似ているでしょう。具体的にはある地点の重みの傾きを微分して求め、そこからさらに傾きの小さい地点に移動し、誤差関数の値が最小になる重みWをみつけ出していく作業を行います。
この際に学習を進める対象がパーセプトロンであれば、決まった誤差関数を使い、ランダムの重みを設定して教師データと比較する作業をしていけば問題はありません。しかし、中間層を持つ多層パーセプトロンでは連鎖律による問題「勾配消失問題」が発生するのです。
これの何が問題かというと利用する活性化関数によっては微分した値が1未満であることがあり、この場合は層が増えるほど重みが更新されなくなります。
例えば、よく利用されるシグモイド曲線「A(x)=1/1+ e-x」の場合は微分した関数の極大値は0.25です。そのため、上のn層の多層パーセプトロンでシグモイド曲線を使うと、結果としてw1の勾配は限りなく0に近づきます。
そうなると重みの更新が行われなくなるため、学習が進まなくなります。これが勾配消失問題であり、層の数が少ない場合でも学習に影響があり、層を増やすほど大きく顕在化します。
ちなみに多層パーセプトロンよりも層の大きいディープラーニングでもこの問題は付きまとうため、現在では微分した値が1を超えるような活性化関数を利用します。具体的には「多層パーセプトロン（MLP）とは」で取り上げている、ハイパボリックタンジェント（tanh）を利用するReLUなどが当てはまるでしょう。
したがって、多層パーセプトロンなどを扱う際には勾配消失問題が起こらないような活性化関数を利用しなければなりません。
これまで解説してきた多層パーセプトロンと今話題のディープラーニングの関係性とはどのようなものなのでしょうか。こちらについては、多層パーセプトロンを発展させたものがディープラーニングだといえます。
具体的には中間層を含む3層で構成されるのが多層パーセプトロンであり、中間層をよりたくさん設けたものがディープラーニングです。ちなみにディープラーニングのディープは中間層（隠れ層ともいう）が深層化（ディープ）したものであるという意味から名付けられています。
背景としては1980年代に多層パーセプトロンが考えられ、線形分離不可のものでも扱えることがわかったことから研究が盛んになりました。ところが中間層をより多層化しようとすると勾配消失問題や当時の技術的な問題などにより上手くいきませんでした。
しかし、そのそれらの問題を解決するアプローチや技術が追い付いてきたことにより、中間層をより多層化することが可能になりました。そのため、多層パーセプトロンからディープラーニングという名称が名付けられ、両者は明確な違いを持ったのです。
ちなみに現在では教師あり学習だけでなく、教師なし学習や強化学習といった学習方法でラーニングさせたり、ニューラルネットワークの構造を変えたディープラーニングも生まれています。
まず、CNN「Convolutional Neural Network」は畳み込み処理とプーリング処理を行って入力する手法です。こちらは画像に対して利用されることが多いニューラルネットワークであり、画像データを圧縮して特徴量を検出して学習を進めます。
この時には畳み込み処理で元データの特徴を持った圧縮データを作成し、プーリング処理でさらに特徴を強調しながらより小さくします。これによって最終的には画像データが特徴量を記載したデータになるため、これを基に画像認識などに活用します。
次に、RNN「Recurrent Neural Network」は再帰的な構造を持ったニューラルネットワークです。学習においては時系列データなど連続したデータを扱うことがありますが、その場合には通常のニューラルネットワークでは対応が難しいです。
これはある時点でのデータがその後のデータに影響を与えるからです。したがって、RNNでは最初のデータx1の入力による出力を次の層y1だけでなく、次の入力データx2の中間層に出力することを繰り返すことで学習します。
RNNの構図としては以下のように再帰的なやり取りをする中間層（セルとも呼ばれる）を持ったニューラルネットワークを構成します。
多くの場合では店舗の売上データや気象データなどを扱う際にRNNは利用されることが多いです。ただし、時系列データの中には長期的なデータを扱うことがありますが、その場合ではRNNでは難しいことがあるので、3のLSTM(Long short-term memory)を利用します。
こちらは上のセルをLSTM blockと呼ばれるものに置き換えて構成されたニューラルネットワークです。
RNNに似ていますがLSTM blockはより構造が複雑になっており、内部にはセルだけでなく入力ゲートと出力ゲート、忘却ゲートと呼ばれる機構が備わっています。
仕組みとしては実際の入力値と入力データ、1つ前の出力データが三か所の入力ゲートに同時に入力。そして、重み付けなどがなされてセルを通り、入力ゲートや出力ゲートなどに出力されます。
この際に入出力ゲートの値が0に近い場合はデータを伝えず、1に近い場合のみ伝えることで適切にコントロールします。また、忘却ゲートではセル内部のメモリを状況に応じてクリアするもので、これによって時系列データのパターンが変わった場合でも機構が動くようにします。

