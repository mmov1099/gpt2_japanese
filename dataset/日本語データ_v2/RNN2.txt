回帰型ニューラルネットワークは1986年のデビッド・ラメルハートの研究に基づく[9]。ホップフィールド・ネットワークは1982年にジョン・ホップフィールドによって見出された。1993年、ニューラルヒストリー圧縮システムが、時間に展開されたRNN中で1000以上の層を必要とする「非常に深い学習」問題を解決した。
長・短期記憶（LSTM）は2007年頃から音声認識に革命をもたらし始め、特定の音声認識への応用において伝統的なモデルをしのいだ。2009年、コネクショニスト時系列分類（英語版）（CTC）で訓練されたLSTMネットワークは、パターン認識大会で優勝した初のRNNとなった。このネットワークはつながった手書き文字認識の複数の大会で優勝した[12][13]。2014年、中国の大手検索サイト百度は、伝統的な音声処理法を用いることなくSwitchboard Hub5'00音声認識ベンチマークを破るためにCTCで訓練されたRNNを用いた[14]。
LSTMはまた、大規模語彙音声認識およびテキスト音声合成を改良し、Google Androidにおいて使われた.。2015年、GoogleはCTCで訓練されたLSTMによって音声認識の劇的な性能向上が達成された[17]と報告され、この技術はGoogle Voice Search（英語版）で使用された。
LSTMは機械翻訳、言語モデリング、多言語処理[20]の記録を破った。畳み込みニューラルネットワーク（CNN）と組み合わされたLSTMは自動画像キャプション（短い説明文）付けを向上させた。

基本的なRNNは連続する「層」へと編成されたニューロン的ノードのネットワークであり、所定の層中の個々のノードは次の層中の全てのノードと有向（一方向）結合により結合されている。個々のノード（ニューロン）は時間変動する実数値の活性化を有する。個々の結合（シナプス）は変更可能な実数値の重みを有する。ノードは（ネットワーク外からデータを受け取る）入力ノード、（結果を得る）出力ノード、（入力から出力への途上でデータを修正する）隠れノードのいずれかである。
離散時間設定における教師あり学習のため、実数値入力ベクトルの配列は入力ノードに到着する（一度に1つのベクトル）。任意の時間ステップにおいて、個々の非入力ユニットはそれに結合した全てのユニットの活性化の加重和の非線形関数としてその現在の活性化（結果）を計算する。ある時間ステップにおける一部の出力ユニットのために教師が与えられた目標活性化を提供することができる。例えば、入力配列が数字音声に対応した音声シグナルであるならば、配列の最後における最終目標出力は数字を分類するラベルとなるだろう。
強化学習のセッティングでは、教師は目標シグナルを与えない。代わりに、適合度関数または報酬関数がRNNの性能を評価するために使われることがある。これは環境に影響を与えるアクチュエータに結合された出力ユニットを通してその入力ストリームに影響する。これは、進行が勝ち取った点数によって測定されるゲームをプレーするために使うことができるかもしれない。
個々の配列は、全ての目標シグナルのネットワークによって計算された対応する活性化からのずれの和として誤差を生じる。膨大な配列のセットを訓練では、全誤差は全ての個別の配列の誤差の和である。
エルマンネットワークとジョーダンネットワーク
エルマンネットワークは、一連の「文脈ユニット」（右図中のu）を追加した3層ネットワーク（右図中でx、y、zとして垂直に配置されている）である。中央（隠れ）層は1の重みに固定されたこれらの文脈ユニットに結合されている]。個々の時間ステップにおいて、入力は順伝播され、学習規則が適用される。固定された逆結合は文脈ユニット中の隠れユニットの以前の値のコピーを保存する（これは、それらが学習規則が適用する前に結合を通じて伝播されるためである）。したがって、ネットワークは一種の状態を維持することができ、これによって標準的な多層パーセプトロンの能力を超える時系列予測といった課題を実行することが可能となる。
ジョーダンネットワークはエルマンネットワークと似ている。文脈ユニットは隠れ層の代わりに出力層から入力を得る。ジョーダンネットワーク中の文脈ユニットは状態層とも呼ばれる。それらはそれら自身への回帰的結合を持つ。
エルマンネットワークとジョーダンネットワークは「単純回帰型ネットワーク（SRN）」としても知られている。

ホップフィールド
ホップフィールドネットワークは全ての結合が対称的なRNNである。定常入力を必要とし、複数パターンの配列を処理しないため、汎用RNNではない。ホップフィールドネットワークは収束することを保証している。もし結合がヘッブの学習を用いて訓練されるならば、ホップフィールドネットワークは結合変化に抵抗性のある頑強な連想メモリとして機能することができる。
Bart Koskoによって発表された双方向連想メモリネットワークは、ベクトルとして連想データを貯蔵するホップフィールドネットワークの一変型である。双方向性は行列とその転置行列を通って情報が流れることから来ている。典型的には、双極符号化が連想対の二値符号化よりも選好される。最近、マルコフ飛び（ステッピング）を用いた確率的BAMモデルが増強したネットワーク安定化ために最適化され、現実世界の応用と関わりを持った。
BAMネットワークは2つの層を持ち、そのうちのどちらかを、連想を思い出し、もう一方の層上へ出力を生成するための入力として動作させることができる。
エコー状態
エコー状態ネットワーク（Echo state network、ESN）は、疎らに結合されたランダム隠れ層を持つ。出力ニューロンの重みは変更可能な（訓練可能な）ネットワークの一部でしかない。ESNは特定の時系列の再現に秀でている。スパイキングニューロンのための派生形式は液体状態マシンとして知られる。
独立RNN (IndRNN) 
独立回帰型ニューラルネットワーク（Independently recurrent neural network、IndRNN）は、従来の完全結合型RNNにおける勾配消失および爆発問題に対処する。1つの層中の個々のニューロンは（この層中の他の全てのニューロンへの完全な結合の代わりに）文脈情報としてそれ自身の過去状態のみを受け取り、ゆえにニューロンは互いの履歴に独立である。勾配バックプロパゲーションは、長期または短期記憶を保持するため、勾配消失および爆発を避けるために制御することができる。ニューロン間情報は次の層において探索される。IndRNNはReLUといった非飽和非線形関数を使って確実に訓練することができる。スキップコネクションを使うことで、深いネットワークを訓練することができる。
再帰型
再帰型ニューラルネットワーク（recursive neural network）は、トポロジカル順序で可微分なグラフ様構造を横断することによって、同じ一連の重みを構造に再帰的に適用することによって作られる。このようなネットワークは典型的に自動微分の反転モードによって訓練することもできる。再帰型ニューラルネットワークは、論理項といった構造の分散表現を処理することできる。再帰型ニューラルネットワークの特殊な場合が、構造が直鎖に対応するRecurrent（回帰型）NNである。再帰型ニューラルネットワークは自然言語処理に応用されてきた。再帰型ニューラルテンソルネットワークは、木中の全てのノードに対してテンソルベースの合成関数を使用する。
ニューラルヒストリーコンプレッサ
ニューラルヒストリーコンプレッサ（neural history compressor）はRNNの教師なしスタックである[36]。入力レベルにおいて、前の入力から次の入力を予測することを学習する。この階層型構造において一部のRNNの予測不可能な入力のみが次のより高いレベルのRNNへの入力となる。したがって、極めてまれにしかその内部状態は再計算されない。ゆえに、個々のより高位のRNNは下位RNN中の情報の圧縮表現を学ぶ。これは、入力配列がより高レベルにおける表現から正確に再構成できるような方法で行われる。
このシステムは、記述長またはデータの確率の負の対数を効果的に最小化する。入ってくるデータ配列中の多量の学習可能な予測可能性を考えると、最高レベルのRNNは、重要な事象間に長い間隔がある深い配列でさえも容易に分類するために教師あり学習を用いることができる。
このRNN階層を2つのRNN、「意識的」チャンカー（高位）と「無意識的」オートマタイザー（下位）に抜き出すことが可能である。チャンカーがオートマタイザーによって予測不可能な入力の予測と圧縮を学習すると、次にオートマタイザーは次の学習フェーズにおいて追加ユニットを通して、よりゆっくりと変化するチャンカーの隠れ層を予測または模倣することになる。これによってオートマタイザーが、長い間隔を超えて適切な、めったに変化しない記憶を学習することが容易になる。次に、チャンカーが残った予測不可能な事象に注視できるように、これはオートマタイザーが以前は予測不可能だった入力の多くを予測できるものとするのを助ける。
生成モデルは、1992年に自動微分またはバックプロパゲーションの勾配消失問題を部分的に克服した。1993年、こういったシステムは時間方向に展開されたRNN中に1000を超える後続層を必要とする「非常に深い学習」課題を解決した。
二次RNN
二次（second order）RNNは、標準的な重み の代わりにより高次の重み を用い、状態は積となる。これによって、訓練、安定性、表現において有限状態機械への直接的マッピングが可能となる。長・短期記憶（LSTM）はこの一例であるが、こういった形式的マッピングまたは安定性の証明は持たない。
長・短期記憶
長・短期記憶（LSTM）は勾配消失問題を回避するディープラーニング（深層学習）システムである。LSTMは通常、「忘却」ゲートと呼ばれる回帰型ゲートによって拡張されている。LSTMは勾配の消失または爆発からの逆伝播誤差を防ぐ。代わりに、誤差は空間方向に展開された無制限の数のバーチャル層を通して逆向きに流れる。すなわち、LSTMは、数千または数百万離れた時間段階前に起こった事象の記憶を必要とする課題を学習できる。問題特化型のLSTM的トポロジーを発展させることができる。LSTMは重要な事象間に長い遅延が与えられても機能し、低周波数と高周波数成分を混合した信号を扱うことができる。
多くの応用がLSTM RNNのスタックを用いており、訓練セット中のラベル配列の確率を最大化するRNN重み行列を見付けるためにそれらをコネクショニスト時系列分類（CTC）によって訓練している。CTCはアラインメントと認識の両方を達成する。
LSTMは隠れマルコフモデル（HMM）や類似の概念に基づく以前のモデルとは異なり、文脈依存言語を認識することを学習することができる。
ゲート付き回帰型ユニット
ゲート付き回帰型ユニット（GRUs）は2014年に発表された回帰型ニューラルネットワークにおけるゲート機構である。完全な形式やいくつかの単純化された方式で使われている。多声音楽モデリングおよび音声信号モデリングにおけるそれらの性能は長・短期記憶の性能と似ていることが明らかにされた。これらは出力ゲートを持っていないため、LSTMよりもパラメータが少ない。
双方向性
双方向性（bi-directional）RNNsは要素の過去および未来の文脈に基づいて配列の個々の要素を予測あるいはラベル付けするために有限配列を用いる。これは、2つのRNNの出力を統合することによってなされる。一方のRNNは配列を左から右へ、もう一方は右から左へと処理する。統合された出力は教師が与えられた対象シグナルの予測である。この技法はLSTM RNNsを組み合わせた時に特に有用であることが証明されている。
連続時間
連続時間（continuous time）回帰型ニューラルネットワーク（CTRNN）は、入ってくるスパイクの一連の流れのニューロンへの影響をモデル化するために常微分方程式の系を用いる。



