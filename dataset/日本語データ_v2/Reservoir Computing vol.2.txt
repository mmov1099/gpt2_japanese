Reservoir Computingの概念
下の画像をご覧ください。
水を貯めるための容器や湖を想像してください。
その中に石を投げ入れるとします。
このとき、3つの石を連続して投げ入れると
3つの石が作る水面の波紋を観察することができます。
石の大きさや形によって波紋は変化します。
また、投げる順番によっても波紋は変化します。
石によって水面がどのように変化するか、想像できますか？
貯水池コンピューティングは、あるアイデアをきっかけに生まれました。
波紋の形状を観察することで、時系列データの特徴を抽出する。
石を投げた順番や時間が、湖面の波紋として現れる。
つまり、記憶が表現されているのです。
時系列処理に適している。
貯水池に簡単な識別器をつけることで
パターンを解析することができる。
読み出しはリザーバーに取り付けたリーニングマシンである。
複雑なパターンを表現するリザーバーと簡単なリーニングマシンにより
時系列データをリーン化することができる。
読み出しには、線形学習機がよく使われる。
線形関数の係数は学習により決定される。
パーセプトロンを学習した。
時系列データも簡単な学習機で学習できる。
これがReservoir Computingの利点である。


linear learning machine、Deeplearning、Reservoir Computingを比較
線形学習機1台では、非線形の入出力関係を学習することはできない。
Deeplearningは、学習コストは高いが、計算性能は高い。
非線形の入出力関係を学習することができる。
Reservoir Computingは、linear learning machineとReservoirの組み合わせである。
低い学習コストで非線形の入出力関係を学習することができる。
しかし、計算性能はタスクによって異なる。
下図は教科書から引用したものである。
この図は、性能と学習コストの関係を示している。
Reservoir Computingの学習コストは、linear learning machineとほぼ同じであり、性能は高い。
同規模のDeeplearning modelと比較すると、調整すべきパラメータ数が少ない。
学習コストが低いとはいえ計算性能にばらつきがある。


Reservoirを利用することで軽量なリカレントニューラルネットワークを構築することができる。
Reservoir Computingは、ディープニューラルネットワークと比較して、高いポテンシャルを有している、最近注目されている計算モデルです。

Reservoir Computingの代表的なモデルであるESN（Echo State Network）について
ESNは時系列パターン認識などに利用されている。
入力履歴がエコーして残るエコー状態がリザーバーに作られる。
リザーバーに現れた特徴は、学習された読み出しによって処理される。
学習されるのは読み出しのみである。
単純な学習機械が読み出しに使われることが多い。
リザーバーでは、リカレントニューロンが複雑に接続されている。
この接続は固定されており、学習されない。
W^outのみが学習されるため、学習コストは低い。
これは下図のベクトルの定義である。
入力はN_u次元のベクトルである。
リザーバには x_1 から x_N_x までの多数のニューロンがある。
これは読み出しに接続されている。
出力層の次元は N_y である。
W_in は入力層とリザーバを接続する接続重みである。
リザーバ内の接続の重みは W である。
W^out はリザーバと出力層を接続する接続重みである。
W_inとWは固定で、W^outのみを学習させる。
リザーバ内の時系列パターンの表現を見てみよう。
入力ベクトルと W^in は乗算され、リザーバに与えられる。
リザーバ内では、ニューロンはリカレント接続される。
追加される前の1回分のステップでのリカレント接続。
リザーバの状態ベクトルの時間発展を表現することができる。
各ニューロンには、非線形活性化関数が適用される。
活性化関数は特に断りのない限り双曲線タンジェントである。
出力ベクトルは、リザーバの状態ベクトルと W^out の掛け算である。

ESNの派生モデルの例で
基本モデルと一般モデルには、いくつかの違いがあります。
まず、出力層とリザーバをつなぐフィードバック接続があること。
次に、入力層と出力層のディレクトリをつなぐ接続がある。
接続を追加しても、W^outだけが学習される。
緑で示されるフィードバック接続の重みは固定である。
ニューロンにはいくつかの種類があります。
ここでは、Leaky Integrator(LI)モデルについて説明する。
LIモデルを用いることで
LIモデルを用いることで、リザーバの状態ベクトルの時間発展の速度を制御することができる。
したがって、過去の情報をどの程度保存するかを制御することができる。
Î±はリーク率であり、このモデルのハイパーパラメータである。
Î± = 1のとき、右項のみの効果である。
これは、基本モデルに相当する。
Î±が小さくなると、リザーバの状態の変化は入力であるuに影響を与えない。
リザーバーの時間発展はより遅くなる。
この効果は、時系列入力データの高周波成分を除去するローパスフィルタのようなものである。
このモデルを用いることで、性能の向上が期待できる場合もある。
Reservoir Computingのシンプルで効果的な微分モデルである。


ESNの重要な考え方であるESP(Echo State Property)について
ESPとは、時系列入出力変換器としての再現性を保証する性質である。
これは、リザーバーが満たすべき性質の一つである。
リザーバの状態ベクトルは、初期状態と時系列入力によって決定される。
初期状態が変化すると、同じ入力を与えてもリザーバの応答が異なることがある。
このような初期状態の影響を防ぐためには、十分な時間が経過した後に、時系列入力のみによってリザーバの状態の時間発展を決定する必要がある。
これが前文の数学的表現である。
異なる状態から出発したリザーバー状態ベクトルは、例えばゼロに変換された値など、同じ値に変換される。
この図に示すように、異なる状態から始まるリザーバーの状態ベクトルは同じ軌道に変換されます。
リザーバはこの性質を満たす必要がある。
これがESPを満たす条件である。
ここで、活性化関数は双曲線タンジェントである。
指標としてスペクトル半径を用いることが多い。
スペクトル半径とは、リザーバの接続重みの最大固有値のことである。
すべての固有値は1以下であることが望ましい。
すべての固有値が1より小さい行列に掛け続けると、ベクトルは短くなる。
この操作を繰り返すことで、ベクトルは0に変換されます。
これがESPを満たす有名な条件です。
貯水池を設計する際には、この条件を考慮する必要があります。
もう一つの条件は、特異値の最大値である。
詳細は省くが、この条件はあまり重要ではないかもしれない。
基本的には、スペクトル半径を考慮してリザーバを設計する。


学習方法について
学習は readout のみである。
他の教師あり学習と同様に、入力と目標信号を与える。
回帰問題を解く場合は、連続的な目標値が与えられる。
分類問題を解く場合は、1ホットのベクトルが与えられる。
読み出しの学習には、いくつかの方法があります。
これは典型的な読み出しの学習方法です。
これは線形回帰の例である。
ここでは、ターゲットとモデルの出力の2乗誤差を最小にするW^outを求めます。
DとXの2乗誤差を最小にすることにする。
この式は次のような式に変形することができる。
これが基本的な方法である。
次に、リッジ回帰を見てみよう。
この方法では、式に正則化項を追加する。
W^outが大きくなるのを防ぐことができる。
これが変形された式です。
この講座で見てきた正則化と似ていますね。
この式を解くことでW^outを決定することができます。
学習コストは低くなります。

タスクの例
これは線形回帰の例である。
このタスクでは、次の時間ステップでの正弦波の値を予測する。
リザーバーに入力時系列データを与え、読み出しのトレーニングを行います。
リッジ回帰を用いることで、正弦波を予測することができる。
出力は連続した値である。


分類の例
これは音声認識の例である。
出力値が0の場合、「0」の出力ニューロンだけが1になります。
他のニューロンは反応しません。
ターゲット信号は、クラスに対応するワンホットベクトルである。
このリザーバーコンピューティングは、音声認識にも応用できる。
しかし、原信号の学習が困難であるため、入力音声の前処理が必要である。
そこで、音声信号を周波数帯に応じて分解する。
前処理を導入することで、Reservoir Computingにより音声信号の分類が可能になる。

Reservoir Computingは軽量なモデルで、ディープリカレントニューラルネットワークと同等の能力を持ち最近注目されています。


Reservoir Computingは一般的に入力層、リザバー、出力層から構成されます。まずはリザバーコンピューティングの概念を理解するために、簡単な例を考えてみます。リザバー（Reservoir）という単語には、貯水池という意味があります。いま、水面に石を投げ込むとします。すると、水面に波紋が生じます。その波紋は石の大きさや形、スピードによって変化します。つまり、この波紋は投げた石の情報を反映していると考えることができます。では、複数の石を次々と投げるとどうなるでしょうか。今度は石が1つの場合よりもはるかに複雑な波紋が生まれます。このような波紋の動的パターンは、投げ込まれた複数の石の大きさや形、さらにはそれらの投げ込まれた順番にも依存します。
つまり、リザバーは系列入力を波紋の時空間パターンに変換する装置とみなすことができます。Reservoir Computingは、このリザバーが生成する動的パターンから、簡便なアルゴリズムを用いて系列入力の識別を行います。


Reservoir Computingの代表的なモデルの1つにエコーステートネットワークがあります。エコーステートネットワークでは、リザバーとして結合重みを固定したリカレントニューラルネットワークを用いて、時系列入力の過去の情報が反響して残る状態（エコーステート）を作り出し、そこから入力の特徴の読み出し（リードアウト）を行います。入力層とリザバーの間の結合重みとリザバー内のフィードバック重みはあらかじめ固定しておき、リザバーと出力層の間の結合重みだけを計算量の小さい線形学習器で最適化します。この工夫によって、全ての結合重みを学習する一般のリカレントニューラルネットワークよりも高速な学習が可能となります。ただし、高い性能を実現するには、あらかじめ固定する結合重みの値を適切に設定しておく必要があります。


Reservoir Computingは、従来のリカレントニューラルネットワークに比べて学習時に変更する要素が少ないため、ハードウェア実装が比較的容易であると考えられます。また近年、リザバーの非線形変換機能を物理現象によって実現する物理Reservoir Computingも注目されています。電子、光、スピン、流体、機械、ナノ粒子、培養細胞などさまざまな媒質・基質を利用する物理リザバーが提案されています。
このようにReservoir Computingは物理系のダイナミクスによっても実現できるため、ソフトウェア実装に比べて高速性や低消費電力性を持つ実装が可能であり、高効率機械学習デバイスを実現するための基盤技術として注目されています。



Reservoir Computingは，近年様々な実データの時系列パターン認識に応用されてきている．その最大のメリットは，他の再帰的ニューラルネットワークモデルに比べて，学習が極めて高速であるという点である．また，「作り込まない」物理的実装が可能であることから，新たな機械学習デバイスの開発につながると期待されている．そのため，Reservoir Computingは，機械学習やニューラルネットワークのコミュニティだけではなく，エレクトロニクスを含む他の分野からも高い関心を集めている．
