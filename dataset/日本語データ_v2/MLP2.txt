コンピュータビジョンの分野では、Convolution Neural Network (CNN ) が長らく最もポピュラーな規格であり、最近では Vision Transformer (ViT ) のような Attention-based Network も注目されている。しかし、本論文では、Convolutional Networks に注目する。しかし、本論文では、Convolution と Attention の両方が不要であることを示す。MLP-Mixerは2つの層からなり，1つ目は画像パッチごとに適用されるMLP（位置情報を混合するため），2つ目は画像パッチ間にわたって適用されるMLP（空間情報を混合するため）である．MLP-Mixerは、十分な量のデータセットと正規化手法を用いることで、SoTAに匹敵する画像分類性能を達成している。
入力として、画像を16x16のパッチに分割します。次に、各パッチに対して線形埋め込みを適用する。そして、それらにMixer Layerを繰り返し適用する。最後に、一般的なCNNと同様にGlobal Average Poolingを適用し、画像を分類する。



次に、上図に示すMixer層の内容を説明します。ミキサーは、チャンネルミキシングMLPとトークンミキシングMLPの2種類のMLPを使用します。トークンミキシングMLPは、異なる空間位置（トークン）間の特徴を混合する役割を担っています。トークン混合MLPは，異なる空間位置（トークン）間の特徴を混合する役割を担っており，テーブルの各列を入力として扱い，各チャネルに独立して適用されます．一方，チャンネル混合MLPは，異なるチャンネル間の特徴を混合することを担い，各トークンに独立して適用され，テーブルの各列を入力として扱います．
チャンネルミキシングMLPは1×1畳み込みのCNNとみなすことができ，トークンミキシングMLPは1チャンネルの深さ方向の畳み込みとみなすことができる．しかし，このMLP-Mixerは，そのようなCNNに比べ，はるかにシンプルなアーキテクチャを有しています．

また，トークンミキシングMLPとチャンネルミキシングMLPを数式に変換してみましょう．MLP-Blockの構造を以下に示します．全結合層＋GELU＋全結合層で構成されており、CNNやTransformerよりもはるかにシンプルな構成になっています。また、各MLPはResNetのようなスキップ結合で接続されています。また、各MLPの前には、Layer Normalizationが行われる。
MLPの実験では、従来手法と同様に大規模データセットで事前学習を行い、小規模データセット（下流タスク）で微調整を行う。目的は以下の3つである。

1.下流タスクでの精度 
2.事前学習にかかる計算コスト 
3.推論時のスループット

なお、この論文の目的はSoTAの実現ではなく、MLPベースのモデルが現在のCNNやTransformerに匹敵し、それを超える可能性を持っていることを示すことです。

パーセプトロンとは、動物の脳は多数(人間の大脳の場合140億個位)の「神経細胞」という細胞がシナプス結合されて出来ています。  そこでまず研究者達はこの「神経細胞」を数学的にモデル化した「パーセプトロン」を考案しました。

次に研究者達は「パーセプトロン」を多数結合して「多層パーセプトロン(MLP: Multi-Layer Perceptron)」と呼ばれるネットワークを考えました。
MLP は「入力層(Input Layer)」、「隠れ層(Hidden Layer, 中間層とも言う)」、「出力層(Output Layer)」ごとに層(Layer)分けされた多層構造になっています。  各層は更に多数の層に分かれている場合もあります。
そして MLP の各層間のパーセプトロンは全て互いに結合しているような構造になっています。  この様な構造を持つニューラルネットワークの事を「全結合型(Fully Connected)ニューラルネットワーク」と呼びます。
ところでネットワークの構造として回帰(または再帰)構造を考えることも出来ますが、一般に MLP は回帰構造を持たない「フィードフォワードニューラルネットワーク(FFNN)」とし、回帰構造を持つ「回帰型ニューラルネットワーク(RNN)」とは区別しています。

単純パーセプトロンは入力層と出力層のみであったのに対し、多層パーセプトロンは中間層(隠れ層)と呼ばれる、層が複数追加されたネットワーク構造を持ちます。 単純パーセプトロンと違い複数のクラス分類を可能とし、線形分離不可能な問題も解くことができます。

多層パーセプトロン（たそうパーセプトロン、英: Multilayer perceptron、略称: MLP）は、順伝播型ニューラルネットワークの一分類である。 MLPは少なくとも3つのノードの層からなる。 入力ノードを除けば、個々のノードは非線形活性化関数を使用するニューロンである。

多層パーセプトロンとは、1980年代に登場した機械学習手法の一種です。多層パーセプトロンはもともと1960年代に登場した単純パーセプトロンの欠点を補う目的で開発されました。
単純パーセプトロンとは複数の入力値（x1, x2, x3とする）に対し、それぞれ調整した重み（w1, w2, w3とする）をかけて出力値（y = w1x1+w2x2+w3x3）を計算し、その結果を使って入力データを分類する手法です。二次元平面に直線を引き、入力データがその直線の右側か左側かによって分類をします。例えば、測定結果がある閾値に対して大きいか小さいかを判定するような場合がこれに当てはまります。

単純パーセプトロンは二次元平面を直線で左右に分けるまでが限界でした。そうすると、例えばOKデータが平面上の右上（第一象限）と左下（第三象限）、NGデータが右下（第四象限）のそれぞれ全域に分布しているような場合、OKデータとNGデータを分けられる直線は存在しません。すると当然、直線を曲線にしたいというニーズが発生しますよね。

そしてこれら中間層の値にさらに重み(W1, W2, W3とする)をかけ、すべて足した値を出力値(z = W1y1 + W2y2 + W3y3)として算出するのです。
つまり、出力値z = W1(w11x1 + w12x2 + w13x3) + W2(w21x1 + w22x2 + w23x3)+W3(w31x1 + w32x2 + w33x3)と表すことができ、単純パーセプトロンより調整できるパラメータがずっと増えています。

多層パーセプトロンの良いところは、データをうまく分類できるよう教師データを使って重みの値を自動調整できることです。この自動調整の手法を「逆誤差伝播法」と言います。
逆誤差伝播法を非常に簡単に説明すると、各重みの値を少しだけ増減させ、多層パーセプトロンによる予測値が教師データの正解に近づくように重みを変化させます。これにより、人がパラメータを調整することなく教師データを使って分類することができる、いわゆる機械学習が実現できました。
また、もう一つの良いところは、あらゆるデータに対して機械学習が可能な点です。画像データは機械学習が最も得意とする対象ですが、画像データも元々は0と１で表現されたデータです。同じように多層パーセプトロンでは0と１で表現された音声データのような時系列データも扱うことができます。パソコンで扱うデータはすべてこのような0と1で表されたデータなので、工夫によって機械学習ができると言えますよね。
多層パーセプトロンの欠点は分類の根拠がわからないことです。仮に100％思い通りに分類できていたとしても、どういう根拠で分類されているのかが判断できないため、作成した多層パーセプトロンに対して完全に信頼することができません。

例えば、製品の画像検査を多層パーセプトロンで学習させた場合を考えます。開発者としては製品の欠陥があればNGと判断して欲しいのですが、NGデータとして用いた画像すべてに埃が写っていたとすると、実は多層パーセプトロンは欠陥の有無でなく埃の有無でOKかNGかを判断している可能性があります。そして、学習結果の重みなどを確認しても欠陥の有無で判断しているのか埃の有無で判断しているのかはわかりません。

多層パーセプトロンを実装して動作を検証したいとき、最もオススメの方法はPythonを利用し、Tensorflowライブラリを利用する方法です。Pythonとはプログラミング言語の一種で、近年機械学習分野で最も広く使われている言語です。機械学習に必要な機能がすべて揃えられる上、ネットや書籍など情報量も多いため、機械学習をするなら迷わずPythonを選びましょう。
ライブラリとは、Pythonの標準機能にはない機能を追加する拡張キットのようなもので、Tensorflowライブラリを用いると、高度な計算に必要なあらゆる機能が使えるようになります。
まずはPythonをインストールしましょう。インストール方法は、Python公式サイトからインストーラーをダウンロードし、実行するだけ。なお、TensorflowライブラリはPythonバージョン3.7までしか対応していないので、注意が必要です。
Python本体のインストールが終わったらTensorflowライブラリのインストールをしましょう。コマンドプロンプトにてpip install Tensorflowと入力するだけでインストールができます。以上で多層パーセプトロンを実装する下準備は完了です。
