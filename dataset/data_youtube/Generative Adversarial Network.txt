The topic of this video is a generative adversarial model.
Let's consider what is the generative model.
The networks that we have seen throughout the lectures are used for classification.
These models determine the class of the input data.
From the above example, it takes “0” as the input, and its output “0.9” and “0.2”.
Then, the model classifies the input as “0” by converting the output to the one-hot vector.
The below example also classifies the output to “1” in the same manner.
This is the mechanism of MLP, CNN, and RNN.
On the other hand, a generative model creates data that is given to the feedforward networks as the inputs.
For example, if we input a feature value of “0” such as 0.9 and 0.2, a “0”-like the image is generated.
We can get a “1”-like the image from the input features such as 0.1 and 0.8.
There are several generative models and for the example,
Boltzmann machine, variational autoencoder, and generative adversarial network (GAN).
Boltzmann machine is a network that has been studied for a long time.
Below, these two models are invented recently.
GAN is applied to many fields so you may have seen it.
We will look at the detail of GAN later.
Generative Adversarial network or GAN.
This model has been applied in many fields and gained attention in recent years.
This is also a generative model and it can be learned by unsupervised training.
The discriminator, which outputs the probability of image class,
and the generative model, which outputs the data from features, both are used in GAN.
A neural network evaluates another neural network.
Only the information of real or fake is given as the supervisor signal to the discriminator.
The generation model is trained by the discriminator iteratively.
No explicit teacher is required for traversing the generative model and the discriminator.
This figure shows an outline of the GAN. The structure itself was a big invention.
GAN has a Generator “G” as the generation model, and Discriminator “D” as the classifier.
The key is how to make "G" and "D" to learn well.
Let’s look at the rough structure.
The image-like data “Y” is generated by the generator using random numbers.
The discriminator is executed using the real data “X” and fake data “Y”.
The information whether the input of “D” is real or fake is given by discriminator.
“D” and “G” are trained with the backpropagation using the supervisory data, real or fake.
It requires two neural networks, two data:
fake data generated by the network and the real data,
and the supervisory data.
The discriminator and generative model have important roles for each other.
The role of the discriminator is to distinguish supervisory data and the generated data.
Discriminator requires to answer “Real” to the supervisory data.
It needs to answer “Fake” to the generated data by another network.
The role of the generative model is to generate data similar to the supervisory data...
that cannot be discriminated by the discriminator.
Therefore, this network is named adversarial network, since these models are needed to out complete each other.
The classifier wants to correctly distinguish the data.
The generator wants to fool the discriminator with the generated data.
Each other purpose is adversative.
By learning this, the data generated by the generative model becomes closer to the real supervisory data.
In the training phase of the discriminator, the true information is given to “D”,
in the training phase of the generator, the false information is given to “D”.
Let’s look at the training method of the discriminator. It has two types of data.
The real data “X” is given with the supervisory data, “real”, to make “D” answer “real”.
The generated data “Y” is given with the supervisory data, “fake”.
The generated image “Y” is produced by giving the “Z” seed to generate the random number.
In the path below, input “Z”, “G” generates “Y”, and put “Y” to “D”.
The "D" is trained using "Y" with the supervisory information “Fake”.
In this phase, the value of “G” is not updated.
On the other hand, in the training phase of the discriminator,
“Y” is generated using a random number, then pass it to the discriminator.
G wants the discriminator to classify that the generated data is real.
Since “G” wants to generate data that cannot be seen as fake,
The false supervisory information “real” is given to “D” and perform backpropagation
Only the value of the generator is updated in this phase, however, the discriminator is not be updated.
The discriminator and the generator are trained repeatedly.
In the training phase of the discriminator, “D” is trained to discriminate the real image.
In the training phase of the generator, “G” is trained to generate an image...
that cannot be seen through by the discriminator.
By repeating this, both “D” and “G” become stronger and the output “Y” becomes real to the human eye.
The process of each part is not so difficult and many extensions can be concerned.
For example, it is possible to replace the random number generator with an encoder.
Data other than images can be also generated.
For "D" and "G", CNN can be used for image processing and RNN can be used for the time series data.
There are various applications for GAN.
For example, deep convolutional GAN can compute the addition or subtraction of concepts.
A man with glasses minus a man without glasses plus woman is a woman with glasses.
Another example is the style transfer.
For example, GAN learns the relationship between the horse and the zebra, then generate zebra from a iamge of a horse.
In the recent misused example, the fake videos of celebrities are created and spread on social media.
The application fields of GAN are diverse and it has a great possibility.
For example, improving the accuracy of images, colorize line drawings, and generating images from text.
However, sometimes it is difficult to train the network.
There are many topics for research to solve these problems.
Please work on the exercises of GAN.