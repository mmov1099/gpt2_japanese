まず、ニューロンモデルを見てみましょう。
これはニューラルネットワークの基礎となるものです。
ニューロンモデルには大きく分けていくつかあります。
まず、スパイクニューロン・モデルです。
これは、脳の神経細胞を模したものです。
このように、スパイクの回数やタイミングなどで情報をコーディングします。
このニューロンモデルは、どちらかというと生物の脳に近いと思います。
次に、この人工ニューロンモデルです。
これは脳の機能を抽象化したようなもので
で、ニューロンの発火頻度を実数で表現したものです。
ニューラルネットワークには、大きく分けてスパイク型ニューラルネットワークと人工ニューラルネットワークの2種類があります。
スパイクニューラルネットワークは、生物の脳を目指したアプローチのニューラルネットワークです。
人工ニューラルネットワークモデルは、工学的に有用なものを目指したアプローチです。
今回は、人工ニューロンモデルの基本を紹介する。
この図は、人工神経細胞モデルを表したものです。
図中の円は神経細胞を表しています。
矢印はシナプスを表す。
シナプスは結合の重み付けであるWを持つ。
シナプスは結合重み付け W を持ち、結合重み付け W と入力の乗算にバイアスをかける。
非線形関数を適用し、その値を出力する。
この後、この図の丸と矢印がよく出てくる。
この矢印のところに重みWが設定され、入力との乗算積算演算がとられる。
式で書くとこんな感じになります。
このような図を見たら、掛け算の演算式をモデル化したものと思ってください。
このニューロンモデルの数学的意味を、簡単な例で理解してみましょう。
このような一次式が何を意味するのか考えてみましょう。
入力Xが2次元のベクトルだとすると、この式はこの図のようになります。
2次元ですから、直線式になります。
この直線で何ができるかというと、この空間を上か下かのように区切る直線を引くことができるのです。
ここで、n次元の空間を考えてみよう。
例えば、3次元ではこのようになります。
これを拡張したn次元でも同じことが言える。
超平面が多次元空間を区切っている。
これが、1つのニューロンを持っているときの数値的な意味です。
次に、ニューラルネットワークの話をしたいと思います。
先ほど紹介したように、1つのニューロンでできることは
n次元の空間をn-1次元の超平面で分離することです。
このような分離で解ける問題を線形問題と呼びます。
つまり、ニューロンモデルを使えば、線形問題を解くことができるのです。
左はANDの例、右はORの例です。
AND論理では、両方の入力が1であれば出力は1、一方または両方の入力が0であれば出力は0になります。
このANDを線形問題として解くには、ここに直線を引き、その直線が
が分離面より大きいか小さいかを判断する。
そうすることで、ANDはニューロンモデルで解くことができます。
また、右の例のORも同様です。
ORの場合は、ここに線を引くことで解くことができます。
ここで質問ですが、XORの問題を1本の直線で解くことができるでしょうか？
答えはNOです。
分離するための線を引いてみてください。
ここに線を引くと、上側は分離されますが、下側は1と0が混在しています。
一本の直線で分離されるわけではありません。
また、このように線を引いても分離することはできません。
このXOR問題は、直線的に分離できない典型的な問題です。
非線形問題とは、直線平面上では切り離せない問題のことです。
非線形問題と呼ばれる。
これを何とかして解きたい。
非線形問題を解くために、隠れ層を持つニューラルネットワークを導入してみよう。
線形問題は1つのニューロンで解くことができた。
このようなネットワークを構築することにする。
この場合、隠れ層に2つのニューロン、出力層に1つのニューロンを用意します。
入力を隠れ層で線形に区切られた空間に変換し、隠れ層で区切るというものです。
しかし、いくら線形モデルを積み重ねても、結局は線形モデルになってしまう。
この隠れ層で、活性化関数と呼ばれる非線形関数を適用するのです。
このような構成のニューラルネットワークをMLP（Multi-Layer Perceptron）と呼びます。
MLP は、ニューラルネットワークの最も基本的な構造です。
ここに示すように、いくら線形モデルを積み重ねても
1つの線形行列として終わってしまいます。
これを防ぐために活性化関数を適用しています。
この例では、活性化関数としてReLUという非線形関数を用意しました。
ReLUの構造はこの図の通りである。
入力が負の場合は0を返し、正の場合は入力をそのまま出力に返す。
このような構造を持つ非線形関数である。
このとき、Wを表す矢印をすべて1にし、バイアスを0と-1にする。
そして隠れ層空間を作る。
入力空間では、どう直線を引いても分離することはできませんでした。
しかし、この空間を変形して隠れ層空間に持っていくと
H1 と H2 の空間に直線的な分離を見出すことができます。
ここに直線を引くことで0と1を分離することができるのです。
このようにニューラルネットワークを作ることで、1つの線形ニューロンでは解けなかったXOR問題を把握することができる。
XOR問題を多層パーセプトロンの隠れ層空間における線形分離問題として解くことができるのです。
以上が非線形問題の基本的な考え方である。
そして、これから様々なニューラルネットワークを構築していく。
入力層と出力層の構造は、問題が決まれば決めることができる。
先ほどのXORの例では、入力がX1とX2の2次元なので、入力層の数は2つです。
そして、出力層は0か1が出れば良いので、出力層の数は1つです。
したがって、入力層と出力層は、入出力ベクトルの次元に合わせる必要があります。
隠れ層はプログラマが調整する。
プログラマが調整するパラメータをハイパーパラメータと呼ぶ。
これはこの分野の専門用語です．
単純なMLPであっても，決めるべきパラメータは様々です．
例えば，隠れ層の場合．
例えば隠れ層の場合，この図では1つのニューロンしか示していませんが，2つ，3つと増やしていくことができます．
また，ニューロンの数も考慮する必要があります．
この図では3つのニューロンしかありませんが、10にも100にも増やすことができます。
さらに、活性化関数についてです。
先ほどのReLUのほかにもいろいろな活性化関数があります。
その中から最適な組み合わせを見つけていくことをハイパーパラメータ探索といい、プログラマが定義します。
一般に、層数やニューロン数が多いほど性能が高いと言われています。
しかし、一般に「学習が難しくなる」ことが知られている。
計算コストが純粋に高くなるからです。
トレードオフを調整しながら、良いネットワーク構造を見つけることが、設計者の腕の見せ所である。
活性化関数としては、シグモイド関数やハイパーボリックタンジェントなどが一般的に使われている。
活性化関数が非線形であれば、様々なものが使えるが、微分可能であることが必要である。
演習パートでは、Google Colaboratory（Colab）を使用します。
まず、Colabの設定方法について説明します。
まず、ColabをセットアップするためにGoogleドライブにアクセスしてください。
Googleドライブは、Googleのトップページから開くことができます。メニューからドライブのアイコンを選択してください。
Googleドライブを開き、演習用のフォルダを作成してください。そして、そのフォルダの中に配布されたipynbファイルをアップロードしてください。
初回のみ、Colabサービスを追加する必要があります。
左上の新しいアイコンボタンをクリックし、「Connect more apps」を選択してください。
Google Workspace Marketplaceの検索ボックスに colab と入力し、Colabアイコンをクリックしてください。
インストールボタンをクリックし、インストールしてください。
Googleアカウントを選択してください。
インストールが完了しました。
インストールが完了したら、ipynbファイルをダブルクリックすることでColabを起動することができます。
これがColabの画面です。
セル」を編集していきます。
セルには、「テキストセル」と「コードセル」の2種類があります。
テキストセルにはテキストを追加することができます。
コードセルにはコードを追加したり、コードを実行したりすることができます。
この2つのセルを使っていきます。
画面の左側には、目次（TOC）が表示されています。
ボタンをクリックすると、TOCを隠したり、消したりすることができます。
TOCの中の項目をクリックするとその場所にジャンプできます。
左側の一番下にあるアイコンをクリックすると、ファイルの管理ができます。
このファイルタブでは、ファイルのアップロードやダウンロードが可能です。
また、Google Driveに接続することができます。
Google Driveからファイルを取得したり、Google Driveにファイルをプッシュしたりすることができます。
ランタイム＞ランタイムタイプの変更をクリックすると、ノートブックの設定ウィンドウを開くことができます。このウィンドウでハードウェアアクセラレータを選択することができます。
None」、「GPU」、「TPU」が選択できます。
演習ではGPUを使用します。
それでは、演習の内容を説明したいと思います。
例題について説明します。
この例題では、MNISTデータセットをMLP（Multi-Layer Perceptron）により分類するプログラムを作成する予定です。
PyTorchを使用して、Deep Learning（DL）プログラムを作成します。この例は5つのステップで構成されています。
最初のステップ：ライブラリのインポート。
2番目のステップ。ネットワークを定義する。
3番目のステップ。エラー関数とオプティマイザを設定する。
第四のステップ データセットを設定する。
5番目のステップ ネットワークを学習する。
それでは、最初のステップをご覧ください。
これらのライブラリをインポートします。
ライブラリをインポートするためのコードを以下に示します。
import torch.nn as nn と書くことで、インポートすることができます。
torch.nn を nn と短縮することができます。
同様に「torch.nn.functional」を「FF」、「torch.optim」を「optim」と短縮することができます。
コードセルを実行してみましょう。
左上の再生ボタンをクリックするか、コントロールボタンを押しながらエンターボタンを押すと、コードを実行できます。
ここでは、エラーメッセージが出なかったので、プログラミングを続けることができます。
では、2番目のステップに進みましょう。
ネットワークを定義するためのクラスを作成します。
nn.Moduleというクラスを継承してクラスを作成します。
メソッドを2つ定義します。「init と forward です。
initメソッドでネットワークの構造を定義する。
forward メソッドでネットワークの順方向の伝搬を定義する。
これがネットワーク定義のコード・セルである。
このクラスは、nn.Module.Moduleを継承している。
initとforwardの2つの関数を持っています。
nn.Linearを使用して完全連結層を作成します。
活性化関数としてFF.ReLUを使っています。
ネットワークを定義する方法を説明します。
nn.Linearを使って、完全連結層を作ります。
nn.Linear関数は2つの引数を取ります。
最初の引数は入力サイズ、つまりニューロンの数です。
ここでは、写真の入力サイズである784を設定します。
第2引数は出力サイズです。
ここでは100とした。これは、ネットワークに100個のニューロンを持つ隠れ層があることを意味する。
また、層があります。
第1引数は100で、これは隠れ層の出力サイズに相当します。
第2引数は出力サイズである。
fc2 の出力はネットワークの出力である。
MNISTデータセットの出力は0から9までの数字である。
出力として10個のニューロンが必要なので、出力サイズとして10を設定した。
このようにネットワークの構造を設定することができる。
次に、forward関数を設定しましょう。
forward関数は、ネットワークの入力としてxという引数を持つ。
まず、ネットワークの第1層であるfc1でxを与える。
完全連結層では、インスタンス名の後の括弧内に入力を与えることで、順伝播の計算を行うことができる。
fc1 の戻り値はこの完全連結層の出力である。
その返り値を活性化関数ReLUに与える。
fc1の出力を与えることで、ReLUの出力を得ることができます。
次に、fc2にReLUの出力を与えることで、出力xを得ることができる。
最後に、前進関数はfc2の出力を返す。
このようにしてMPLを定義することができる。
ここでは定義されたクラスからインスタンスを作成します。
mlp.to(device)でGPU上でインスタンスを動作させることができます。
次に、エラー関数とオプティマイザの設定について説明します。
エラー関数としては、NN.MSELossやNN.CrossEntropyLossがよく使われます。
NN.MSELossは回帰問題で使用されます。
NN.CrossEntropyLossは分類問題で使用されます。
今回は画像の分類問題を解くので、NN.CrossEntropyLossを使用します。
optim.SGD (Stochastic Gradient Descent) または optim.Adam は、オプティマイザとしてよく使われます。
この例では、SGDを使用します。
criterionと呼ばれる誤差関数のインスタンスを作成します。
オプティマイザーの引数を設定します。
最初の引数には，学習させたいネットワークのパラメータを与えます．
ここでは、前のステップで定義されたMLPのパラメータを学習させます。
mlp.parameters()によって、学習用のパラメータを取得することができます。
第2引数には，学習率を与えます．
ここでは，0.01という値を設定します．
このようにして、誤差関数やオプティマイザを設定することができます。
これで、セルを実行することができました。
次に、4番目のステップに進む。
ここでは、MNISTデータセットをロードする。
MNISTデータセットは28x28のグレースケール画像である。
MNISTデータセットは、トレーニングデータセットとテストデータセットに分かれています。
トレーニング用には6万枚、テスト用には1万枚の画像を用意する。
また、バッチサイズも設定した。
バッチサイズとは、ネットワークに一度に与えるデータの数である。
データ数が多いほど、並列処理を行うため、学習時間が短くなります。
あまり大きな値を指定すると、メモリの使用量が増えるため、メモリエラーになります。適切な数値を設定してください。
逆に小さい数値を設定すると、バッチサイズが小さくなります。
反復回数を増やすと学習時間は長くなるが、メモリ使用量は少なくなる。
この2つのセルを実行することで、MNISTデータセットをダウンロードすることができます。
通常は、torchvision.datasets.MNIST を実行すると自動的にデータセットがダウンロードされる。
Colabのバグで、この機能からダウンロードしようとするとエラーになることがあります。
今回は、MNISTデータセットのディレクトリをダウンロードします。
このコードセルを実行して、MNISTデータセットをダウンロードしてください。
torchvision.datasets.MNISTが代わりにダウンロードしたデータセットをロードします。
このセルでは主に3つのことを設定します。
ここでは、画像の変換を行います。
データセットを transforms.ToTensor 関数でテンソル配列に変換しています。
PyTorchはテンソル配列を使って計算を行います。
ここではノーマライザーを使っています。
ダウンロードした画像の範囲、[0, 1]を[-1, 1]に変換しています。
次に、torchvision.dataset.MNIST でデータセットを読み込みます。
この関数は4つの引数を取ります。
第一引数はデータセットの場所です。
ここでは、カレントディレクトリを指定しています。
第2引数で学習用のデータセットであるかどうかを指定します。
trueを指定すると学習用データセットとして使用されます。falseを指定すると、テスト用に使用されます。
データセットをダウンロードするかどうかを指定しています。
今回はダウンロードする必要がないので、falseを指定します。
第4引数で、画像変換のプロパティを設定します。
最後の設定は、ミニバッチです。
ミニバッチの設定は torch.utils.data.Downloader で行うことができます。
第1引数にデータセットを指定します。
バッチサイズを第2引数で、データをシャッフルするかどうかを第2引数で指定します。
データの順番は学習性能に影響するため、学習データをシャッフルする必要があります。
学習データの順番は学習性能に影響を与えるので、偏りが生じないようにシャッフルする必要があります。
テストデータについては、データの順番はテストに影響しないので、シャッフルする必要はありません。
このようにデータセットを準備することができます。
では、最後のステップに進みましょう。
これはネットワークを学習させるためのステップである。
まず、入力データと理想的な出力データ（教師ありデータ）をネットワークに与える。
そして、差分値に対してネットワークの変数をリセットする。
実際の出力とネットワークの理想的な出力との差を計算する。
その後、バックプロパゲーションを実行し、ネットワークのパラメータを更新する。
これらのステップを繰り返し、ネットワークを学習させる。
これらは学習用のプログラムである。
まず、エポック数（データセットを何回繰り返すか）を指定します。
ここではエポック数が2なので、for文によりデータセットが2回繰り返されることになります。
このループの中で、バッチサイズごとに学習データセットが繰り返されます。
変数dataには，各繰り返しにおける学習の入力データとラベルが格納されます．
私たちはMLPに入力を与えます。
画像サイズを.view で変換する必要があります。
画像サイズは784でバッチの番号のサイズに変換されます。
GPUで学習を行います。
to(device)」でデバイスを指定する必要があります。
これは、ステップ2で定義したデバイスの設定です。
Cudaが利用できる場合はcuda0、そうでない場合はCPUがデバイスとなります。
GPUが利用可能であれば、入力とラベルはGPUに送られます。
前方伝播のためにMLPに入力を与えます。
ネットワークのインスタンスに入力を与えることで、順伝播の計算を行うことができます。
その戻り値が、ネットワークの出力となります。
次に，ネットワークパラメータの差分値をリセットします．
そして、誤差関数を計算する。
ステップ3で定義した基準の引数を2つ設定する。
第1引数はネットワークの実際の出力である。
第2引数は理想的な出力であるラベルである。
戻り値は誤差の値である。
バックプロパゲーションは loss.backword で計算することができる。
次に、オプティマイザーを使ってネットワークのパラメータを更新する。
optimizer.step によって行うことができる。
学習の進捗を確認するために、学習中の誤差を表示する。
ここでは、100回に1回の割合で表示しています。
誤差の値は loss.item で得ることができる。
損失の総和を計算する。
100回分の損失の平均値を表示する。
このプログラムでは、各エポックごとに学習した後、ネットワークのテストを行っています。
testloader_mnist は for 文で反復処理されます。
変数dataは入力とラベルを持つ。
同様に変換された画像を GPU に送る。
そして、MILから出力を得る。
実際の出力と理想の出力を基準にして、誤差を求めます。
逆算しなくても、criterion.item で損失が得られます。
テストの損失はサムアップされる。
ネットワークの出力からラベルを取得する。
outputs.argmax でネットワークの最大出力を確認する。
実際の出力と理想的な出力を比較し、精度を計算する。
pred.eq にラベルを与えることで、実際の出力と理想的な出力を比較することができる。
正解の数を合計して正解とする。
繰り返し計算の結果、精度を得ることができる。
さて、このコードセルを実行してみましょう。
100回に1回、誤差関数の出力が得られる。
各エポックの終了時に精度を得ることができます。
これで2エポック分の学習が終了しました。
その結果、この例で定義したネットワークの精度は89%になりました。
昨今のディープラーニングブームで、私たちの身の回りにはAIを応用した製品やデバイスがたくさんあります。
例えば、スマートフォンやAIスピーカーなどがあります。
このAIを開発するための環境も非常に充実しています。
例えば、プログラミングフレームワークには、PyTorch,
Chainer、Keras、TensorFlowなどがあります。
その中には、すでに開発が終了しているものもあります。
ChainerはPyTorchを後継として開発を終了しています。
また、大企業が提供するクラウドサービスも充実しています。
よく知られているのは、Amazon、Google、Microsoftが提供しているものです。
これらが提供するクラウドサービスを利用すれば
自社の製品やサービスにAI機能を取り入れることができます。
プログラミングそのものはできなくても
AIが一般化し、プログラミング環境が充実した状況で必要とされるのは、こうした人材ではありません
と、プログラミング環境が充実してきました。
例えば、ディープラーニングの理論を理解している人。
あるいは、ディープラーニングのことはよくわからないけど、プログラミングはできるし、ツールも使えるという人。
必要な人材は、ディープラーニングの仕組みを理解している人。
ライブラリを使って実装できる技術を持っていて、性能向上のためのチューニングができる人です。
近年、この機械翻訳が非常に発展していると言われています。
これらはニューラルネットワークを利用した機械翻訳です。
例えば、DeepLやGoogle翻訳などは、非常に精度が高いと言われています。
これらのツールは、非常に自然に言語を翻訳することができます。
また、これらのツールをうまく活用して、英字新聞や論文を読んでみましょう。
AIは、自律走行やADAS（先進運転支援システム）にも大きく関わってくると言われています。
これはNVIDIAの場合ですが、ディープラーニングとGPUアクセラレーションを利用して、道路標識の検出。
信号、停止線などをこのように
人間の目に代わる機能をコンピュータが実現することが可能になりつつあるのです。
一般的な画像認識の例を紹介します。
入力された画像のカテゴリーを推測する問題です。
この例は、2012年にディープラーニングが注目されるきっかけとなった国際的な画像認識コンテストの結果です。
例えば、このトラの写真を見せると
コンピュータは「虎だ」と答えます。
テレビの写真を見せれば、コンピューターは「テレビだ」と答えるでしょう。
しかし、左上のコアラの画像では、コンピュータはウォンバットと答えます。
間違えても、人間と同じように間違えます。
もう10年も前の話です。
このコンテストがきっかけで、Deep Neural Networkが急速に発展したと言われています。
先ほどの一般的な画像認識を提供しているのが、このGoogle Cloud Vision API
は、先ほどの一般的な画像認識をクラウドサービスとして提供するものです。
クラウドベースの画像認識を実現するAPIです。
ディープラーニングや画像認識の仕組みを詳しく知らなくても、サービスに利用することは可能です。
例えば、人物の顔を検出する機能を追加し
とか、その人が今どんな表情をしているのかを認識する機能を追加することができます。
私たちの周りには、たくさんのAI活用事例があります。
実際にしばらく使ってみることで、Webアプリケーションで簡単に体験することができます。ぜひ試してみてください。
まず1件目は、先ほど紹介したGoogle Cloud Vision APIです。
こちらのURLからアクセスできます。
アップロードした画像から見えるものを回答するAIを試してみてはいかがでしょうか。
例えば、左上の画像をアップロードした場合。
は、その画像のカテゴリーを返します。
例えば、「映っている」「自然が映っている」「水が映っている」とAIが答えます。
カテゴリ認識の精度が高いことが分かります。
左下の例では、画像に写っている文字が認識されています。
このように簡単に試すことができますので、ぜひご自身で試してみてください。
もう一つの例として、線画に自動的に色をつける例です。
NVIDIA Jetson AI Certificationを試してみませんか？
これは、NVIDIAが発行しているCertificationです。
例えば、就職などで履歴書を作成する際に、資格として記載することができます。
本講座では、自由な発想でAIを使ったプロジェクトを実施します。
本講座の最終課題として、このNVIDIA認定資格に挑戦することができます。
また、NVIDIA Japanの日本人スタッフによるサポートも受けられます。
日本語、英語ともに対応可能だと思います。
このAIセミナーの参加者は、NVIDIAのスタッフからサポートを受けることができます。
プロジェクト終了時に英文レポートを提出する必要があります。
その際、英文レポートの校正が必要な場合は、大学側が校正料をサポートします。
また、小型ロボット「TurtleBot」を貸与します。
そして、組み込み型GPUのJetson NANO。
このプロジェクトの最終タスクに必要な
これが今回のプロジェクトの流れです。
トレーニングの段階は、このAIセミナーの講義
またはNVIDIAのホームページのイントロビデオを見ることでトレーニングができます。
次のプロジェクトベースの評価は、この講義の最終プロジェクトになります。
プロジェクトベースの評価のレポートは、NVIDIAで評価されます。
プロジェクト評価に必要な機材は弊社でサポートします。
レポート作成のための英文校正費用
また、このNVIDIA本社への応募の際には、NVIDIAの社員によるサポートも受けられます。
ご興味のある方は、こちらのホームページをご覧ください。
詳細な情報を入手することができます。
これは日本語の記事です。
ロボスタによるこの資格の記事です。
興味のある方は、ご自分で検索して読んでみてください。
このロボスタの方々は、人がマスクをしているかどうかを判断するAIを作りました。
Jetson AI Specialistは、彼らに日本初の認定を受けました。
では、ニューラルネットワークのトレーニングについて説明します。
ニューラルネットワークの訓練は...
で、ニューラルネットワークが望ましい入出力関係を近似できるように、最適な重みとバイアスを計算することです。
重みは、この図の丸を結ぶ矢印で示されています。
式中、重みはWとbに相当する。
多層パーセプトロンの場合。
隠れ層と出力層の間のパラメータを学習します。
XORの例で考えてみましょう。
を入力とし、[0, 0], [0, 1], [1, 0], [1, 1]を入力とします。
XORが正しく出力されるように、Wとbを計算する。
ネットワークの規模が小さい場合は、パラメータ値を手で探して設定すればよい。
ネットワークの規模が大きいと、人間が勝手にパラメータを設定することはできない。
そこで、データを使ってネットワークを学習させる。
ネットワークを学習させるためには、以下のような誤差の変化を計算する。
という誤差の変化を計算し、NNの重みとバイアスを変えて、実際の出力と目的の出力との誤差を計算する。
そして、誤差が小さくなるようにパラメータを調整する。
バックプロパゲーションを用いて、誤差の変化を求める。
誤差が最小になるように重みとバイアスを調整することを最適化といいます。
右の図では、横軸が重みとバイアス、縦軸が誤差を表しています。
データを与えて見てみましょう。
この例は、画像認識によく使われるFashion-MNISTというデータセットです。
データは一般的に、トレーニング、バリデーション、テストの3種類に分けられます。
学習データは、モデルの重みとバイアスを更新するために使われます。
検証データは、モデルの更新には使用されない。
検証データはモデルの更新には使わず、モデルの評価と学習の調整に使われる。
したがって、ニューラルネットワークの学習は、学習データと検証データによって行われる。
ネットワークが学習された後
学習後のモデルの汎化性能をテストデータセットで評価します。
このとき、データがトレーニング、バリデーション、テストに分けられていることを確認してください。
次に、学習データの与え方を決める単語、batch、epochについて説明します。
ミニバッチと呼ばれる単位でデータを与えます。
この例で考えてみましょう。
ここでは、バッチサイズが10なので、10枚の画像が同時に与えられます。
バッチサイズは重要です。ある程度まとめてデータを渡した方が良いと言われています。
しかし、バッチサイズが大きすぎると、GPUのメモリ不足でプログラムが動かなくなることがあります。
バッチサイズを調整する必要があります。
この例では、データセットを8つのmini-Batchに分割しています。
すべての学習データを投入することをエポックと呼びます。
バッチ、バッチサイズ、エポックという重要な用語の意味を、この図を使って理解してください。
次に、学習方法であるバックプロパゲーションについて説明します。
バックプロパゲーションは、重みとバイアスに対する誤差関数の勾配を計算する方法である。
つまり、微分値を計算するのである。
誤差関数は、ニューラルネットワークの実際の出力と所望の出力との誤差を定義するものである。
この関数は、誤差が大きい場合に大きな値を出力する。
例えば、回帰タスクの場合、平均二乗誤差を用いる。
回帰タスクは、連続的に変化する値を予測するタスクである。
例えば、気温の予測は回帰タスクである。
分類タスクの場合は、クロスエントロピーの誤差関数がよく使われる。
分類タスクとは、データがどのクラスに属するかを予測するタスクである。
例えば、「1」という数字の画像をネットワークに見せると、「1」というクラスのニューロンが活性化されます。
では、バックプロパゲーション法を計算してみましょう。
3層MLPを例にとります。
これは，入力層，隠れ層，出力層から構成されています．
ニューロンのインデックスはそれぞれi,j,kです。
ここで，順伝播の計算を確認しておきましょう．
入力層から出力層への計算を順伝播と呼びます。
XORタスクの例と同様である。
まず、入力-隠蔽層間で累積入力xと重みwを掛け合わせ、バイアスをかける。
その後、非線形関数を適用する。
隠れ層の値が入力層の出力となる。
隠れ層から出力層への計算は、隠れ層の出力を用いて行われる。
同様に、乗積演算と非線形関数を適用する。
値が出力されたら、実際の出力と所望の出力との差を求める。
つまり、実際の出力と監視データを比較するのである。
その差を求めるのに、誤差関数を使う。
誤差関数を2種類見てみましょう。
まず、平均二乗誤差関数です。
これは回帰処理に使われる。
目的の出力(t)は監視信号として与えられる。
2つ目はクロスエントロピー誤差関数である。
この場合、正解tはラベルとして与えられる。
ラベルでは、正しいクラスは 1、それ以外は 0 で表現される。
誤差関数を用いて、実際の出力と目標との誤差を測定する。
誤差を測定した後、バックプロパゲーションを適用する。
バックプロパゲーションは、出力層から入力層への計算処理である。
誤差関数の出力の偏導関数を w に関して計算する。
偏導関数は、3つの偏導関数の積に展開される。
展開された偏微分方程式は図のように対応する。
この方程式の2番目と3番目の偏導関数をデルタkに置き換える。
黄色の部分の1つ目について考えてみよう。
この部分には、出力層と隠れ層から複数の誤差が流れ込んでいます。
それを踏まえて式を考える必要があります。
実際の出力と目標との誤差は、後方に伝搬させることができます。
ただし、逆伝播の計算は複雑なので、その点は注意が必要です。
最近の深層学習フレームワークでは、計算グラフを用いた逆モード微分法が採用されています。
逆伝搬は、逆モード微分を用いてフレームワークが自動的に計算する。
これにより、プログラマーは簡単にネットワークを定義することができる。
これはフレームワークを利用する大きなメリットの一つである。
このクラスでは、この操作を手動で計算することはありません。
階層構造を計算することにより、出力であるyを得ることができます。
バックプロパゲーションは非常に複雑です。
L, g, ...の導関数を計算する必要がある．
計算グラフのすべての要素が微分可能であれば
が自動的に差分を計算します。
活性化関数g1,g2は微分可能であると述べました。
したがって、誤差関数も非線形活性化関数も微分可能でなければ、バックプロパゲーションの計算はできない。
逆モード微分の仕組みを簡単に説明する。
簡単な例で考えてみよう。
計算グラフとは、演算とデータをノードとエッジで表現したものである。
ノードはデータと基本的な算術演算を表す。
矢印で示される辺は、計算結果の流れを表している。
例えば、a×b＋c×dを計算するとしよう。
対応するグラフをスライドに示します。
図のgにはa×b＋c×dの計算結果が記入されています。
逆モード微分の仕組みについて考えてみましょう。
式中、a×b＋c×d。
a x bはグラフのeに相当する。
c×dはfに対応する。
e＋fの結果はgに対応する。
入力a,b,c,dに対して、それぞれ3,2,4,5が入力であると仮定する。
次に、計算グラフの各部を微分することを考える。
まず、一番右の部分であるgを考える。
gに関して偏微分した結果は「1」である。
ここから逆算して考える。
gとeの間で考える場合、gをeで微分することで答えが得られます。
g = e + fなので、e + fをeで微分すると結果は1です。
同様に、e＋fをfで微分するとiとなる。
eとaの間で考えるときは、eをaに関して偏微分すれば答えが求まる。
同様に、e＝a×bなので、eをaに関して偏微分した結果はbとなる。
bは2なので，この部分は2となる。
同じようにして，すべての辺の微分値を計算することができる．
連鎖法則の考えを見てみよう。
すべての辺の微分値がわかっていれば
aに関するgの偏微分を連鎖法則で計算することができる。
エッジの値を用いて計算することができる。
gの偏微分を元の式を使って計算するのは難しい。
しかし、計算グラフと連鎖法則を用いれば簡単に求めることができる。
gのaに関する偏微分の結果は、1* 2 = 2と求まる。
これはニューラルネットワークの計算グラフの例である。
x と w を積算する。
を積算し、目標値を与える。
すべての微分は、この計算グラフを用いて、逆モード微分で解くことができます。
プログラマがフィードフォワード伝搬と誤差関数を定義すると、ディープニューラルネットワークの枠組みで
ディープニューラルネットワークのフレームワークを用いて
フレームワークが自動的にグラフを計算する。
また、すべての勾配を得るための逆モード微分も計算する。
ただし、プログラマーはフレームワークの仕組みを考える必要がある。
バックプロパゲーションと逆モード微分の仕組みは説明しましたが。
フレームワークを使うことで、ディープニューラルネットワークのプログラミングが容易になります。

さて、AIセミナーの講義パートを始めましょう。
今日のテーマは、「注意」というメカニズムです。
まず、「注意」を説明し、次に「自己注意」を説明します。
アテンションは最近流行っている仕組みです。
アテンションはCNNと組み合わせて使われます。
CNNと組み合わせることで、比較的簡単な構造で性能を向上させることができる。
Attentionはニューラルネットワークの広い分野で利用されている。
ここでは、Attentionの概要について説明する。
まず第一段階として、CNNを用いて多くの入力データから簡単な特徴を抽出する。
第2ステップでは、入力から生成された特徴量を全て使うわけではありません。そこで、ネットワークはそのうちのいくつかに着目する。
そして、フォーカスされた特徴はニューラルネットワークの推論に使われる。
データの一部、つまり特徴に焦点を当てるので、Attentionと呼ばれる。
Attentionは自然言語処理の分野で発展してきた仕組みである。
簡単な例として、画像処理を紹介したい。
CNNを用いた画像認識について考えてみよう。
寿司の画像を例にとって考えてみよう。
寿司画像は物体認識の問題として考える。
これはブリ（鰤）の画像である。
人間は画像のブリの部分だけを見れば、その物体がブリかどうかを判断できる。
通常のCNNの場合、画像全体が入力としてネットワークに与えられます。
そのため、画像認識は背景の領域に非常に敏感です。
この画像は、黒いお皿の上に「ぶり」が乗っています。
お皿の色が変わっても、お皿の上のお寿司は同じです。
お皿の色がどうであれ、「ぶり」という答えを期待するのです。
ネットワークには背景のある画像が与えられるので、出力は背景の影響を受ける。
この影響を軽減するために、人間の注意力であるフォーカス機構を画像処理に導入しています。
ここでは、画像の寿司だけに注目する方法を考える。
これが画像処理分野における「注意」の概念である。
まず、入力画像から簡単な特徴量を抽出する。
次に、関心領域を推定するニューラルネットワークに分岐する。
このマスク画像のように、CNNから関心領域を推定するマスクを取得する。
この注目領域の画像で入力画像をマスクする。
すると、左下の画像のように、寿司の部分のみを抽出することができる。
このように、寿司の部分だけを抽出することで、後段で背景を無視した物体認識を構築することができる。
以上が画像認識分野におけるアテンションの一例です。
画像処理におけるアテンションは、もっとわかりやすい。
アテンションとは、関心領域を推定する仕組みである。
これは簡単な例であった。
通常のCNNで考えてみよう。
CNNの最初のステージで作成された特徴マップにアテンションを適用する。
この概念をSqueeze-and-Excitation Network (SENet)という。
SENet はマスク情報を構築し、各特徴量マップの関心領域を推定する。
この例では、1つ目の特徴マップと3つ目の特徴マップが認識上重要である。
そこで、1番目と3番目のマップに対してマスクを構築し、注目する特徴を推定する。
構築したマスクを元の入力画像に適用することで、このような特徴マップ群を得ることができる。
これらの特徴マップは、後段のニューラルネットワークに使用される。
以上で画像処理のAttentionの説明は終わりです。
次に、アテンションが元々開発された自然言語処理の分野でのアテンションを見てみよう。
これは文章を分類するネットワークでのAttentionの例である。
例として、入力文が肯定的な感情か否定的な感情かを判断する文の分類の問題を考えてみる。
入力文を単語に区切り、各単語の特徴量を出力する。
すると、一般にCNNやRNNは、正負のラベル情報を直接推定する。
Attention機構を導入することで、判断に有用な単語の特徴のみを抽出することが可能である
を抽出し、その特徴量に基づいて推定を行うことができる。
この例では、「おいしい」という単語が有用である。
この単語は、肯定的か否定的かの判断に大きな影響を与える。
したがって、この単語だけに注目し、他の単語は出力にほとんど影響を与えない。
このように、Attentionは文の分類に応用することができる。
次に、翻訳の例について考えてみよう。
これはAttentionを用いずにLSTMを用いて翻訳を行った例である。
どのように翻訳が行われるかを説明する。
前の例と同様に、文を単語に区切り、各単語に対してLSTMを用いて特徴量を生成する。
特徴量の生成にはエンコーダを用いる。
これはLSTMの時間発展型モデル（Sequence to Sequence）に基づいている。
このモデルでは、すべての単語の特徴を抽出する。
そして、モデルは原文全体の特徴量から翻訳後のテキストを生成する。
デコーダはそのテキストを文頭から単語単位で翻訳していく。
アテンションなしの翻訳は、このような仕組みになっている。
人間がこの文章を翻訳するとき、最初から文章全体を見ることはない。
次に、Attentionを使った翻訳を説明します。
私たち人間は、この文章を翻訳するとき、最初から文章全体を見ているわけではありません。
例えば、文中の主語をまず探します。
例えば、「ぶり」という単語に注目してみましょう。
この場合、「ぶり」という特徴を利用して訳語を作ります。
アテンションは、人間が翻訳するのと同じようなことができる。
これは、前の画像の例と似ています。
まず、各単語の特徴を抽出する。
そして、先ほどのデコーダーの出力とエンコーダーの出力を掛け合わせ、ソフトマックスを適用する。
そうすることで、どの単語に注目するかを決めることができる。
この例では、2番目の単語である「ぶり」に着目することが決定される。
そして、「ぶり」という特徴量をもとに、出力する単語を決定している。
各ステップで注目する単語を決定するために、アテンションが使われる。
このようなプロセスを経て、Attentionを使った翻訳後の文章ができあがる。
アテンションを使って翻訳の精度を上げる例を紹介する。
精度を上げるために、Query-Key-Valueを用いたSource-Target Attentionが提案された。
このモデルでは、エンコーダが検索用にKeyという素性を出力し、実際の翻訳用にValueという素性を出力する。
つまり、Encoderの出力を分離するためのモデルである。
先ほどの例と同様に、Decoderも素性を持つ。
このモデルでは、Decoderの素性はQueryと呼ばれる。
前の例と同様に、Decoderの最初の単語を決定してみよう。
最初の単語はQueryとして与えられる。
そして、最初の単語に対するすべてのキーの応答をチェックする。
Keyのレスポンスが最も大きい単語のValueを使ってFeatureを作成する。
作成した特徴量を元に、次の単語の翻訳単語を決定する。
では、その手順を順番に見ていこう。
まず、対象のQueryに全単語のKeyを掛け合わせる。
乗算結果が最も高い単語に注目する。
これにより、注目する領域の特徴量の効果が高まり、他の領域の特徴量の効果が減少する。
そして、この注目によって得られた特徴量を用いて、翻訳された単語を作成する。
これがソース・ターゲット注目の仕組みである。
次に、自己注意をみてみよう。
先に説明したソース・ターゲット注目の場合、Key / ValueとQueryは異なるソースから生成される。
翻訳の例では、Key / ValueはEncodeから、QueryはDecoderから生成される。
Self-Attentionの場合、Key/ValueとQueryは同じソースから生成される。
そこで、Self-Attentionは、自ら生成したKey/Queryを用いて、自ら生成したValueに重み付けを行い、合計する。
文中のSelf-Attentionの例をこの図に示す。
Self-Attentionは、Encoder側で特徴量間の関係を考慮することができる。
このブリはおいしい」という文では、おいしい という単語が ブリ を特徴づけている。
このブリはまずい」という文の場合、不味い という単語が ブリ を特徴付けている。
おいしい や 不味い という単語は、前の単語 ブリ を特徴付けると考えられる。
つまり、入力データの単語間の関係性を考慮することができる。
Self-Attentionのポイントは、入力データ中の関係性に着目できることである。
Self-Attentionの例を見てみよう。
まず、Queryと処理対象の単語のすべてのKeyの内積を取る。
ここでは、「ブリ」に着目し、Queryの周辺にある他のKeyを参照する。
最も関連性の高い単語である「delicious」に着目する。
そして、各単語からValueを受け取る。
Valueを受け取ると、重みが高いほど反映されるように重み付けを行う。
そして、最終的な特徴を作る。
Attention後の情報は、入力された単語と重み付けされたValueの和である。
このように、Self-Attentionは文中の単語間の関係を抽出することができる。
自然言語処理の分野でのSelf-Attentionの例を説明する。
Self-Attentionは、画像処理にも応用できる。
ニューラルネットワークによって抽出された特徴マップの集合から、各Query Key Valueを生成する。
自然言語の場合と同様に、QueryとKeyから重要な部分を探す。
そして、その値を重み付けして合計する。
そして、重み付け和から得られた特徴を、元データの特徴に追加する。
このように、重要な特徴に着目することで、Attentionを画像処理に応用することができる。
Self-Attentionを画像に利用するメリットを説明する。
まず、離れた場所にある特徴量を考慮することができる。
画像に写っているブリはおいしいか？という分類問題を解くニューラルネットワークを考えてみましょう。
左の入力画像では
はブリが小さく、通常のCNNでは分類が困難です。
しかし、空間的に離れた場所にある「華麗な笑顔」という情報を使って「ぶり」を特徴付けることができれば...。
は、ブリが美味しい と推論することが可能になる。
もし、笑顔の素敵な男性が写っていなかったら、ブリ が美味しいかどうか判断できないかもしれません。
仮にいたとしても、「笑顔の素敵な男性」と「ぶり」は、通常のCNNでは遠すぎるため、この問題を解決することは難しい。
しかし、Self-Attentionの仕組みを応用すれば、空間的に離れた画像特徴量を組み合わせることができる。
それらを組み合わせることで、「ブリが美味しい」と推論できるかもしれない。
現在、自然言語処理や画像処理など、さまざまな応用分野を持つAttentionを見てきました。
みなさん、こんにちは。AIセミナーを始めましょう。
本日のトピックは深層強化学習で、AIセミナーの最後のトピックになります。
深層強化学習を見る前に、まず強化学習の概要を見てみましょう。
そして、強化学習の問題をディープラーニングでどう解決するか。
そして、その後、演習に取り組みましょう。
まず、強化学習(RL)の構成要素を見てみましょう。
RLの構成要素の1つは環境です。
環境には状態が含まれています。
また、RLはエージェントで構成される。
RLのエージェントの例としては、ロボットや自動運転車などがあります。
エージェントは、自分の行動を決定するポリシーを持っている。
エージェントは、何らかの方法で環境の状態や状況を観察する。
エージェントは与えられた状況によって決定されたポリシーに基づいて行動する。
エージェントが行動を起こした後，環境は変化する．その変化した状態を観察し，次の行動を起こす．
RLはエージェントと環境との相互作用によって動作する。
この学習方法は、本講義で見てきた一般的なニューラルネットワーク...とは根本的に異なる。
この講義を通して見てきた
最も重要な点は、環境とエージェントの相互作用である。
RLには報酬という概念がある。エージェントは、例えば宝探しのような環境の中で活動する。(この例については後述する）。
エージェントは、宝を見つけることができれば、その良し悪しを示す報酬を受け取る。
行動履歴は報酬によって更新され、エージェントは少しずつ賢くなっていく。
これがRLの全体的な枠組みである。
機械学習は主に3つのカテゴリーに分けられる。1つ目は教師あり学習で、CNN、RNN、MLPなどがあります。
2つ目は教師なし学習で、例えばAEやGANなどがあります。
3つ目は強化学習（RL）です。
RL学習法の考え方は、教師あり学習とも教師なし学習とも根本的に異なる。
強化学習と教師あり学習の違いを見てみよう。
教師あり学習で、エージェントの行動を生成する方法を考えてみよう。
エージェントは蓄積された行動から結果を出力する。
それぞれの行動に対して、教師あり信号を作成することができれば
教師あり学習により、エージェントの行動を学習することができる。
結果は複数の行動の結果であるため。
最終的な結果に対して、各行動に対して正確な教師信号を与えることは困難である。
そのため，教師あり学習はエージェントを訓練する上で限界がある．
そこで、強化学習では、各行動にラベルをつけない。
報酬は一連の行動の結果として与えられる。
例えば、野球の打者ロボットを考えてみよう。
バットの振り方、振り角、スピードなど、すべてのデータの系列に対して、教師データを与えることは困難です。
しかし、行動の結果に対して報酬を与えることは簡単です。
例えば、ヒットを打ったらご褒美をあげる、ホームランを打ったらご褒美をあげる。
結果に対してのみ報酬を与えることが、他の学習方法との違いです。
RLは結果のみを評価するため、比較的容易に報酬を与えることができる。RLは、例えば野球のバッティングマシンのような多くのフレームワークで活用することができる。
エージェントは報酬を得たときの行動履歴を評価することで、その経験から方針を獲得することができる。
ここが教師あり学習と強化学習の大きな違いである。
ここからは、ある例を見て、RLの詳細を理解することにしよう。
この単純な迷路の問題を考えてみよう。このロボットは白い細胞の上を移動することができる。
このロボットが宝箱を見つけるのは良いことだ。
しかし、爆弾を見つけてしまうと、爆弾が爆発してゲームオーバーになります。
宝箱のマスには＋1点の報酬があり、一方、爆弾のマスには-1点の負の報酬がある。
この図では、ロボットがエージェントで、迷路が環境である。
ロボットは白いセルを移動しながら宝物を探します。
例えば、ロボットが左下にいるとき、上に行くか右に行くかはポリシーによって決定される。
ロボットが「s」という状態にあるとする。
この例では、状態 s はマップ内の座標のようなものです。この迷路には12個のセルがあるので
「s は12通りの状態を取ることができます。
例えば、ロボットが左下の状態 s にいるとします。
最適な行動は「上に行くのが良い」かもしれない。この最適な行動は、ポリシーによって決定される。
方針は関数で表現される。この例では、ポリシーは状態 s の関数πである。
アクションは、ポリシーによって与えられます。この例では，行動 a は Go up である．
状態 s で行動 a から得られる報酬を定義する．
この例では，報酬は宝箱が+1点，爆弾が-1点である．
例えば、左の角にいるロボットは、「右」「右」「右」「上」「右」と行動します。
すると、マイナスの報酬を得ることができる。
これがエピソードです。
エピソードとは、開始から終了までの期間のことです。
このように、環境、状態、エージェント、方針、行動、報酬というのが強化学習の基本的な枠組みである。
次に、モデルをどのように学習させるかを見てみよう。
状態-行動の価値関数である「Q」値について考えてみよう。
この問題では、ロボットは上下左右に動くことができる。
各状態で上下左右に進む確率はこの例のようになる。
左下のセルを状態「s0」と呼び、この例では他に状態「s1」「s2」「s3」「s4」などがある。
行動の値はQ関数で表される。
値を大きくすることができる最適な行動は、各セルの赤色で示される。
この例から、正の報酬を得るための最適な行動は、「上」「上」「右」「右」「右」であることがわかる。
この一連の行動は、報酬の総和の期待値が最も高くなる。
爆弾の隣のセルの Q 値は負である。
爆弾のセルに移動すると値が下がることをQ関数で表現している。
良いQ関数が獲得できれば，最も高い値を引き起こす政策を選択することで問題を解決することができる．
エージェントは負の値を選ばない限り，爆弾にぶつかることはない．
最適化されたQ関数が得られれば，エージェントは容易に宝箱を見つけることができる．
次に，Q 学習について見てみよう．
まず、各セルの値は一様またはランダムに初期化される。
次に、得られた値でQ関数を更新する。
学習には1回あたりの割引を適用する。
宝箱を発見する前の行動には、比較的大きな報酬が伝搬される。
ゴールから遠ざかれば遠ざかるほど、より大きな割引が適用される。
左下の例を見てみよう。初期位置 s0 から右へ移動するとき。
報酬は1であり、割引率は1より小さい。
しかし、「下」「右」「上」の3ステップで宝箱を見つけることも可能である。
このように行動し、Q関数が更新されれば。
学習率、更新率はαで決定されることになる。
このとき、割引後の報酬であるγは、この3つのステップで与えられる。
s0 において、エージェントは下へ移動するよりも右へ移動する方がより多くの報酬を得ることができる。
エージェントはこの方法で報酬を獲得し，Q値を更新する．
これが元のQ値である．Q関数は、報酬の量から決定されるパラメータで更新される。
この例では、エージェントが爆弾に当たってゲームオーバーになりました。
このエピソードを何百回、何千回と繰り返すことで
宝箱を見つけることができれば、Q関数は正の値に更新されます。
エージェントが爆弾に当たったエピソードは、負の報酬で更新されるため、負の値が蓄積される。
このように、バランスの取れた良いQ関数が獲得できることが期待されます。
環境地図ができれば、ロボットは思い通りに動けるようになります。
報酬を決めることができれば、複雑なエージェントも学習させることができる。
RLモデルは、教師あり学習や教師なし学習とは全く異なる枠組みで学習することができる。
強化学習は、そもそも報酬を決めることができれば、これほど幅広い分野に応用できるのである。
という応用の可能性も秘めていた。
強化学習やQ学習はかなり以前から研究されていたが、うまくいかなかった。
例えば、Q学習では探索空間の次元が高いことが問題である。
例えば、10×10ピクセルの2値画像について考えてみよう。
この例では、画像だけを使ってレガシーピンポンゲームを学習してみよう。
10×10の2値画像でも、組み合わせは2の100乗個。10の30乗個という膨大な状態を持っています。
Q値を全てカバーするように学習させるためには、十分な計算メモリと十分なサンプルを確保することが困難である。
ただし、先の迷路問題（12状態）は解ける可能性がある。
10×10ピクセルの2値画像さえも扱える問題は解けない。
そこで、Qを均等に分割するのではなく、Qの近似関数を解として導入する。
目標値を設定し、損失関数を定義して、勾配法によりパラメータthetaを学習させる。
これが深層強化学習である。
これによって、Q学習の状態分割を深層学習で置き換える。
深層学習でパラメータを学習することで、Qを近似するのである。
画像を10×10ピクセルに分割して一律に学習するのではなく
得られたネットワークをQ関数として、例えばCNNを用いた画像の学習を行う。
Qをディープラーニングのパラメータと同数で近似させればよい。
そうすることで、データの分割方法を学習し、報酬によってパラメータが更新されるようになります。
その結果、状態を表現する能力が高まる...。
となり、実用的なメモリ量と計算時間で様々な問題を解決できるようになります。
そして、これらの技術を訓練に利用する。
また、割引率であるγを定義しておく。
ε-greedy法がどの程度ランダムに動くかを記述することができる。
ε-グリード法を入れない場合、ランダムに生成される初期値の影響を受ける動作がある可能性がある。
ある確率εで、Q関数と相関のない振る舞いをする。
ある確率εでQ関数と無関係な振る舞いをする、つまり、今まで経験したことのない新しい行動をとるようになる。
ε-greedy法は重要である。ランダムな行動を与える最も簡単な方法である。
ランダムに行動することで、広大な環境を探索することができる。
貪欲法では、既知の行動の中で最も価値のあるものだけを取る。
ε-greedy法では、εが小さいと、ほとんどの場合、貪欲な行動をとる。
また、ε-greedy法では、εが小さいと、ほとんどの場合、欲張りな行動を取り、時々、別の方法を探す。
その結果、今まで探索されなかった最適な経路を見つけることができるかもしれない。
経験値再生は、経験をメモリに蓄積する機能である。
先の例では、数百、数千のエピソードを繰り返すことで学習させる。
これらのエピソードを記憶し、ランダムにサンプリングすることで、学習データの偏りを防ぐことができる。
私たちの海馬も、経験値再生と似たような機能を持っていると言われています。
演習に取り組みましょう。TAに気軽に質問してください。
以上で本日の講義は終了です。
次に、最適化について説明する。
バックプロパゲーションで求めた重みとバイアスをわずかに更新し
を勾配の負の方向へ繰り返し更新していく．
より誤差の少ない部分を探すことを最適化と呼びます。
更新の度合いを決めるパラメータを学習率と呼びます。
最適化の方法には様々なものがあります。
最も基本的な最適化アルゴリズムは確率的勾配降下法(SGD)です。
性能は良いのですが、学習速度が遅いのが難点です。
Adamも有名な最適化です。
学習速度が学習中に変化する。
SGDの改良版と言われています。
複数の最適化アルゴリズムを組み合わせている。
このアルゴリズムは効率的なので、学習速度が速い。
最適化アルゴリズムの詳細については、本講義では説明しません。
興味のある方は、参考文献に詳しい説明がありますので、ご確認ください。
最適化のプロセスを説明します。
まず、学習データからミニバッチを取得します。
そして、そのデータをネットワークに与え、順伝播法を実行する。
次に、ネットワークからの実際の出力と、望ましい出力である監視信号との誤差を計算する。
勾配用の変数を初期化し、バックプロパゲーションにより重みとバイアスの勾配を計算する。
勾配は変数に蓄積される。
最後に、最適化アルゴリズムによって重みとバイアスが更新される。
そして、次のミニバッチに移る。
このような最適化のプロセスが何度も繰り返される。
次に、正則化について説明します。
ニューラルネットワークを学習する際に、オーバーフィッティングが発生します。
学習データセットに対して過度に最適化されると、ニューラルネットワークはテストデータを正しく予測できなくなります。
テストデータセットを予測できない場合、オーバーフィッティングが原因である可能性がある。
オーバーフィッティングは、学習データセットを正しく分類できたとしても、テストデータセットを予測できない場合の原因である。
基本的に、モデルの複雑さが増すと、学習誤差は減少する。
このとき，誤差が大きくなるポイントがあることが実験的に知られている．
横軸はモデルの複雑さである分散である．
モデルの複雑さとは、ニューロンや層の数である。
私たちは、真のモデルからの誤差であるバイアスを最小にしたいのです。
ニューラルネットワークをある程度大きくすると、バイアスは減少する。
グラフの右側でオーバーフィッティングが起きていると考えられる。
オーバーフィッティングを防ぐ方法として正則化がある。
学習誤差は大きくなるが、テストデータの誤差である分散の増加を抑えようとする方法である。
汎化誤差とは、真の分布とモデルとの間の誤差のことである。
実際の真の分布はどうやってもわからない。
そこで、テストデータからの誤差を汎化誤差と呼ぶことにします。
正則化はモデルの分散を小さくし、バイアスの上昇を抑制する。
正則化のアルゴリズムには様々なものがある。
例えば、early stopping。モデルがオーバーフィットする前に学習を停止させる。
この方法は、オーバーフィットする前に学習ループを停止させる。
また、L1ノルムやL2ノルムの正則化もよく使われます。
この方法では、誤差関数に重みのペナルティ項を設定する。
重みの値を大きくしないようにする。
また、重みを疎にすることもできる。
そして、モデルの分散を小さくすることができる。
ドロップアウトもよく使われる。
この方法では、学習中に、ニューロンの出力をランダムにゼロにする。
これにより、ネットワークの平均化が期待できる。
これにより、アンサンブル学習が実現できる。
このようにネットワークを調節しているのです。
この講義では、このアルゴリズムの仕組みは説明しない。
このビデオのテーマは、生成的敵対モデル（generative adversarial model）です。
生成モデルとは何かについて考えてみましょう。
講義を通して見てきたネットワークは分類のために使われます。
これらのモデルは、入力データのクラスを決定します。
上の例から、「0」を入力とし、その出力「0.9」と「0.2」を取る。
そして、出力を1光子ベクトルに変換することで、入力を 0 に分類するモデルです。
下の例も同様に出力を「1」に分類しています。
これがMLP、CNN、RNNの仕組みである。
一方、生成モデルでは、フィードフォワードネットワークに入力として与えるデータを作成する。
例えば、0.9や0.2といった「0」の特徴量を入力すると、「0」のような画像が生成される。
一方、0.1や0.8といった特徴量を入力すると、1らしい画像が生成される。
生成モデルにはいくつかあり、例えば
ボルツマン・マシン、変分オートエンコーダ、GAN（Generative Adversarial Network）である。
ボルツマン・マシンは古くから研究されているネットワークである。
以下、この2つのモデルは最近発明されたものです。
GANは多くの分野で応用されているので、見たことがある方も多いのではないでしょうか。
GANの詳細は後述します。
Generative Adversarial Network (GAN)。
このモデルは多くの分野で応用され、近年注目されています。
これも生成モデルであり、教師なし学習で学習することができる。
画像クラスの確率を出力する識別器。
と、特徴量からデータを出力する生成モデル、この2つがGANで使われている。
ニューラルネットワークが他のニューラルネットワークを評価する。
識別器には、本物か偽物かの情報のみがスーパーバイザ信号として与えられる。
生成モデルは識別器によって繰り返し学習される。
生成モデルと識別器の間を行き来するための明示的な教師は必要ない。
この図にGANの概要を示す。この構造自体が大きな発明であった。
GANは生成モデルであるGenerator「G」と分類器であるDiscriminator「D」を持っている。
G」と「D」をいかにうまく学習させるかがポイントになる。
大まかな構造を見てみよう。
画像のようなデータ「Y」は、生成器が乱数を用いて生成する。
実データ「X」と偽データ「Y」を使って識別器を実行する。
D の入力が本物か偽物かの情報は、識別器によって与えられる。
「D と G は、本物か偽物かの監視データを用いて、バックプロパゲーションで学習される。
これには2つのニューラルネットワークと2つのデータが必要である。
ネットワークが生成した偽データと実データの2つのデータが必要である。
と監視データである。
識別器と生成モデルは互いに重要な役割を担っている。
識別器の役割は、監視データと生成データを区別することである。
識別器は監視データに対して Real と答える必要がある。
また、別のネットワークで生成されたデータに対しては「Fake」と答える必要がある。
生成モデルの役割は、監視データと類似したデータを生成することである
というような、識別器では識別できないデータを生成することである。
したがって、これらのモデルは互いに補完し合う必要があるため、このネットワークは敵対的ネットワークと名づけられた。
分類器はデータを正しく識別したい。
生成器は生成されたデータで判別器を欺きたい。
それぞれの目的は敵対的である。
これを学習することで、生成モデルが生成するデータはより実際の監視データに近くなる。
識別器の学習段階において、真の情報は D に与えられる。
は、生成器の学習段階において、偽の情報を D に与えている。
識別器の学習方法について見てみよう。これは2種類のデータを持つ。
実データ X は、D に 実 と答えさせるために、監視データ 実 と一緒に与える。
生成データ Y は、fake という監視データとともに与えられる。
生成画像 Y は Z のシードを与えて乱数を発生させることで生成される。
以下のパスでは、Z を入力し、G で Y を生成し、Y を D に渡している。
D は Y と監視情報 Fake を用いて学習される。
この段階では、G の値は更新されない。
一方、識別器の学習フェーズでは
「Y を乱数で生成し、識別器に渡す。
G は、生成されたデータが本物であることを判別器に 分類して欲しいと考えている。
Gは偽物とわからないようなデータを生成したいので。
偽の監視情報「real」を「D」に渡し、バックプロパゲーションを行う
このフェーズでは、生成器の値のみを更新し、識別器は更新しない。
識別器と生成器は繰り返し学習される。
識別器の学習では、実画像を識別できるようにDを学習する。
生成器の学習フェーズでは、「G」を学習させ、「G」が生成する画像は...。
を生成するように学習させる。
これを繰り返すことで、「D」も「G」も強くなり、出力される「Y」は人間の目にとってリアルなものになる。
各部の処理はそれほど難しくなく、多くの拡張が可能です。
例えば、乱数発生器をエンコーダーに置き換えることも可能である。
また、画像以外のデータも生成可能である。
D と G は、画像処理にCNNを、時系列データにはRNNを用いることができる。
GANには様々な応用がある。
例えば、深層畳み込みGANは、概念の足し算、引き算を計算することができる。
眼鏡をかけた男性マイナス眼鏡をかけない男性プラス女性は、眼鏡をかけた女性というように。
もう一つの例は、スタイル変換である。
例えば、GANは馬とシマウマの関係を学習し、馬の画像からシマウマを生成する。
また、最近悪用されている例としては、芸能人の偽の動画を作成し、SNSで拡散させるというものがある。
GANの応用分野は多岐に渡り、大きな可能性を秘めている。
例えば、画像の精度向上、線画の色付け、テキストからの画像生成などである。
しかし、ネットワークの学習が困難な場合もある。
これらの問題を解決するための研究テーマはたくさんあります。
ぜひ、GANの演習に取り組んでください。
それでは、AIセミナーの講義を始めます。
今日のテーマはリザーバーコンピューティングです。
リザーバーコンピューティングの概念を見てみましょう。
下の画像をご覧ください。
水を貯めるための容器や湖を想像してください。
その中に石を投げ入れるとします。
このとき、3つの石を連続して投げ入れると
3つの石が作る水面の波紋を観察することができます。
石の大きさや形によって波紋は変化します。
また、投げる順番によっても波紋は変化します。
石によって水面がどのように変化するか、想像できるでしょうか？
貯水池コンピューティングは、あるアイデアをきっかけに生まれました。
波紋の形状を観察することで、時系列データの特徴を抽出することができます。
石を投げた順番や時間が、湖面の波紋として現れる。
つまり、記憶が表現されているのです。
時系列処理に適している。
貯水池に簡単な識別器をつけることで
パターンを解析することができる。
読み出しは、リザーバーに取り付けたリーニングマシーンである。
複雑なパターンを表現するリザーバーと、シンプルなリーニングマシンにより
時系列データをリーン化することができる。
読み出しには線形学習機がよく使われる。
線形関数の係数は学習により決定される。
本講座では冒頭でパーセプトロンを学習した。
時系列データも簡単な学習機で学習できる。
これがリザーバーコンピューティングの利点である。
ここで、線形学習機、ディープラーニング、リザーバーコンピューティングを比較してみよう。
線形学習機1台では、非線形の入出力関係を学習することはできない。
ディープラーニングは、学習コストは高いが、計算性能は高い。
その計算性能は高い。
非線形の入出力関係を学習することができる。
リザーバーコンピューティングは、線形リーニングマシンとリザーバーの組み合わせである。
低い学習コストで非線形の入出力関係を学習することができる。
しかし、計算性能はタスクによって異なる。
下図は教科書から引用したものである。
この図は、性能と学習コストの関係を示している。
リザーバーコンピューティングの学習コストは、線形学習機とほぼ同じである。
であり、性能は高い。
同規模のディープラーニングモデルと比較すると、調整すべきパラメータ数が少ない。
学習コストが低いとはいえ
計算性能にばらつきがある。
リザーバーを利用することで
軽量なリカレントニューラルネットワークを構築することができる。
リザーバーコンピューティングは、ディープニューラルネットワークと比較して、高いポテンシャルを有しています。
最近注目されている計算モデルです。
ここでは、リザーバーコンピューティングの代表的なモデルであるESN（Echo State Network）を見ていこう。
ESNは時系列パターン認識などに利用されている。
入力履歴がエコーして残るエコー状態がリザーバーに作られる。
リザーバーに現れた特徴は、学習された読み出しによって処理される。
学習されるのは読み出しのみである。
単純な学習機械が読み出しに使われることが多い。
リザーバーでは、リカレントニューロンが複雑に接続されている。
この接続は固定されており、学習されない。
W^outのみが学習されるため、学習コストは低い。
これは下図のベクトルの定義である。
入力はN_u次元のベクトルである。
リザーバには x_1 から x_N_x までの多数のニューロンがある。
これが読み出しに接続される。
出力層の次元は N_y である。
W_in は入力層とリザーバを接続する接続重みである。
リザーバ内の接続重みはWである。
W^out はリザーバと出力層を接続する接続重みである。
W_in と W は固定で、W^out だけを学習する。
リザーバ内の時系列パターンの表現を見てみよう。
入力ベクトルと W^in は乗算され、リザーバに与えられる。
リザーバ内では、ニューロンはリカレント接続される。
追加される前の1回分のステップでのリカレント接続。
リザーバの状態ベクトルの時間発展を表現することができる。
各ニューロンには、非線形活性化関数が適用される。
活性化関数は特に断らない限り双曲線タンジェントである。
出力ベクトルは、リザーバの状態ベクトルと W^out の掛け算である。
ESNの派生モデルを見てみよう。
これが今まで見てきた基本モデルです。
これらは、ESNの派生モデルの例である。
この講義では、(a)の一般的なモデルについて説明します。
基本モデルと一般モデルには、いくつかの違いがあります。
まず、出力層とリザーバをつなぐフィードバック接続があること。
次に、入力層と出力層のディレクトリをつなぐ接続がある。
接続を追加しても、W^outだけが学習される。
緑で示されるフィードバック接続の重みは固定である。
ニューロンにはいくつかの種類があります。
ここでは、Leaky Integrator(LI)モデルについて説明する。
LIモデルを用いることで
LIモデルを用いることで、リザーバの状態ベクトルの時間発展の速度を制御することができる。
したがって、過去の情報をどの程度保存するかを制御することができる。
Î±はリーク率であり、このモデルのハイパーパラメータである。
Î±=1のとき、右項のみの効果である。
これは基本モデルに相当する。
Î±が小さくなると、リザーバの状態の変化は入力であるuに影響を与えない。
リザーバーの時間発展はより遅くなる。
この効果は、時系列入力データの高周波成分を除去するローパスフィルタのようなものである。
このモデルを用いることで、性能の向上が期待できる場合もある。
これがリザーバーコンピューティングのシンプルで効果的な微分モデルである。
次に、ESNの重要な考え方であるESP(Echo State Property)について説明する。
ESPとは、時系列入出力変換器としての再現性を保証する性質である。
これは、リザーバーが満たすべき性質の一つである。
リザーバの状態ベクトルは、初期状態と時系列入力によって決定される。
初期状態が変化すると、同じ入力を与えてもリザーバの応答が異なる場合がある。
このような初期状態の影響を防ぐために
初期状態の影響を防ぐためには、十分な時間が経過した後に、時系列入力のみによってリザーバの状態の時間発展を決定する必要がある。
これが前文の数学的表現である。
異なる状態から出発したリザーバー状態ベクトルは、例えばゼロに変換された値など、同じ値に変換される。
この図に示すように、異なる状態から始まるリザーバーの状態ベクトルは、同じ軌道に変換されることになります。
リザーバーはこの性質を満たす必要がある。
これがESPを満たす条件である。
ここで、活性化関数は双曲線タンジェントである。
指標としてスペクトル半径を用いることが多い。
スペクトル半径とは、リザーバの接続重みの最大固有値のことである。
すべての固有値は1以下であることが望ましい。
すべての固有値が1より小さい行列に掛け続けると、ベクトルは短くなる。
はベクトルが短くなる。
この操作を繰り返すことで、ベクトルは0に変換されます。
これがESPを満たす有名な条件です。
貯水池を設計する際には、この条件を考慮する必要があります。
もう一つの条件は、最大特異値である。
ここでは詳細を見ないが、この条件は必須ではないのかもしれない。
基本的にリザーバを設計する際には、スペクトル半径を考慮する。
学習方法について説明する。
学習は読み出しのみである。
他の教師あり学習と同様に、入力と目標信号を与える。
回帰問題を解く場合は、連続的な目標値が与えられる。
分類問題を解く場合は、1ホットのベクトルが与えられる。
読み出しの学習には、いくつかの方法があります。
これは典型的な読み出しの学習方法です。
これは線形回帰の例である。
ここでは、ターゲットとモデルの出力の2乗誤差を最小にするW^outを求めます。
DとXの2乗誤差を最小にすることにする。
この式は次のような式に変形することができる。
これが基本的な方法である。
次に、リッジ回帰を見てみよう。
この方法では、式に正則化項を追加する。
W^outが大きくなるのを防ぐことができる。
これが変形された式です。
この講座で見てきた正則化と似ていますね。
この式を解くことでW^outを決定することができます。
学習コストは低くなります。
最後に、タスクの例を見てみましょう。
これは線形回帰の例である。
このタスクでは、次の時間ステップでの正弦波の値を予測する。
リザーバーに入力時系列データを与え、読み出しのトレーニングを行います。
リッジ回帰を用いることで、正弦波を予測することができる。
出力は連続した値である。
次に、分類の例を見てみましょう。
これは音声認識の例である。
出力値が0の場合、「0」の出力ニューロンだけが1になります。
他のニューロンは反応しません。
ターゲット信号は、クラスに対応するワンホットベクトルである。
このリザーバーコンピューティングは、音声認識にも応用できる。
しかし、原信号の学習が困難であるため、入力音声の前処理が必要である。
そこで、音声信号を周波数帯に応じて分解する。
前処理を導入することで、リザーバコンピューティングにより音声信号の分類が可能となる。
さて、今日のトピックを簡単におさらいしておきましょう。
リザーバーコンピューティングは軽量なモデルで、ディープリカレントニューラルネットワークに匹敵する能力を持っている。
最近注目されています。
これで今日の講義は終わりです、ありがとうございました。
オートエンコーダの種類をいくつか見ていきます。
今日はオートエンコーダ（AE）を見ていきます。
多層パーセプトロン（MLP）、CNN、RNNの学習には、監視信号が必要です。
一方、オートエンコーダーは、学習時に監視信号が不要です。
このネットワークは、図のような砂時計のような形をしている。
(90度回転させると砂時計のような形をしているのがわかると思います)。
隠れ層は、入力層や出力層よりも少ないユニットで構成されています。
形が砂時計に似ていることから、砂時計と呼ばれている。
オートエンコーダーは教師なしモデルである。
入力を出力にコピーしようとするように学習される。
明示的な監視信号を持っていない。
言い換えれば、入力そのものが教師である。
入力層から隠れ層までの部分をエンコーダと呼ぶ。
隠れ層から出力層までの部分をデコーダと呼ぶ。
オートエンコーダの構成で重要なポイントは
であり、隠れ層の数を入力層の数より少なくすることである。
MLPと同様にパラメータとして重み「w」とバイアス「b」を持つ．
重みの値を共有するTied Weightは，オートエンコーダの構成方法の一つです．
入力層から隠れ層への重みと，隠れ層から出力層への重みの値を同じにすることで，学習パラメータの数を減らすことができます．
を同じにすることで、学習パラメータの数を減らすことができる。
また、正則化としても影響する。
同系列の重みは、学習性能の向上に役立つ。
このネットワークは、入力と同じ値を出力するだけで、特徴を抽出することができる。
入力をユニットの少ない隠れ層にマッピングし（エンコード）、隠れ層を出力にマッピングすることで
と、隠れ層をユニットの多い出力層に対応させる（デコード）ことで
入力よりも少ないユニット数で元の情報を表現することができる。
原画を表現するのに有効な特徴は、中間層で表現される。
そのため、特徴を抽出し、次元を減らすことができる。
例えば、10×10ピクセルの画像で100次元の画像があるとする。
このとき、次元を10に減らすことができれば、データを圧縮することができる。
エンコーダの機能を自動的に学習することから、オートエンコーダと呼ばれるようになった。
主成分分析（PCA）は、線形データ圧縮法である。
AEは、PCAを非線形分布に拡張したモデルである。
手書き数字の成分は、MNISTデータセットから学習することができる。
PCAとオートエンコーダの違いを見てみよう。
x で示されるデータは、PCAでは直線に近似することができるが
オートエンコーダでは、データ x にフィットする非線形直線に近似されます。
次に、積層型オートエンコーダ(SAE)について説明します。
ここでは、オートエンコーダの積み重ねについて説明します。
前のスライドで見たオートエンコーダは、隠れ層が1つしかない。
MLPのようにオートエンコーダの層を追加することで、MLPと同じ利点を得ることができます。
精度を上げることができ、少ないユニットでデータを表現することができます。
私たちはSAEの学習方法をいくつか持っています．
そのうちの1つがgreedyアルゴリズムであり，通常はうまく機能します．
これは隠れ層が2つのSAEである。
このネットワークは、入力と同じ出力を与えることで一度に学習することが可能です。
しかし、層ごとに学習させた方がうまくいくことが分かっています。
まず、一番大きい部分を学習させる。
次に、学習させた隠れ層を取り出し、赤色で示したデコーダを削除する。
最初に学習したネットワークの隠れ層、中間特徴量である「y」を入力とする。
次のステップでは、より小さなオートエンコーダを学習させ、特徴量 y^を返すようにする。
このとき、新しい素性「z」を得ることができるのは、オートエンコーダの役割である。
同じ操作で緑色の部分を取り除くと、x、y、zのネットワークが得られる。
x は y に圧縮され、y は z に圧縮されています。
デコーダーの部品も同じ処理方法で積み重ねます。これでようやく、大きなネットワークが構築できる。
ポイントは、このような大きなネットワークから、層ごとに処理をして小さなネットワークを作ることであり、この方法は事前学習に使われている。
2006年には、ディープニューラルネットワークのトレーニングに成功したという最初の報告がなされた。
それ以前は、ディープニューラルネットワークの学習は困難と言われていました。
それ以前は、学習の発散や勾配の消失といった問題があるため、ディープニューラルネットワークの学習は難しいと言われていました。
しかし、オートエンコーダーを用いた事前学習により、ディープニューラルネットワークを学習できることが証明されました。
黄色い部分は、スタック型オートエンコーダを用いてあらかじめ特徴を抽出しておき、最後のステップで、その特徴を用いてニューラルネットワークを学習させる。
そして、最後のステップでは、小さなMLPに対して微調整を行うことで、ネットワークを学習させることができます。
この方法はディープリーンの発展に貢献した。
これはディープラーニングの発展に貢献した最初の技術です。
以前は、ディープニューラルネットワークの初期重みを得るために、スタックドオートエンコーダが利用されていました。
次に、改良型オートエンコーダを見てみましょう。
まず、SPAE（Sparse-AutoEncoder）を見てみましょう。
SpAEでは、あらかじめ多くのユニットが隠れ層に配置されています。
この例では、隠れユニットが入力ユニットより多くなっています。
しかし、データ量は入力より多く、データを圧縮することはない。
そこで、データ圧縮が可能なように、疎なユニットを作る。
隠れ層は、入力が来たときにニューロンの出力が0になるように学習させる。
0の値を持つユニットの数を調整することで
のユニット数を調整することで、隠れ層の活性ニューロンの数を減らし、データ圧縮を可能にしています。
つまり、自由度が高く、学習による効果的なネットワーク構造を選択することができる。
これは、新たな正則化項を追加することで実現される。
仮にEをオリジナルの損失関数とする。
第二項に単位をスパースにする正則化項を追加することで
Kullback-Leibler divergence (KLD)を用いて計算を行う。
ρ_jハットは、ユニットの反応を示す出力 y の累積値である。
ハイパーパラメータであるρの値を小さくすると、0 のユニットの数が増加する。
また、ρの値を大きくすると、0の個数が減少する。
このρを適切に設定することで、適切な数のユニットが活性化され、良好な特徴量を得ることができる。
βは、ユニットをどれだけ活性化させるかを決める寄与率である。
例えば、βを0.1に設定すると、そのままの数値が出力される。しかし、これでは良い特徴とは言えない場合がある。
βの値を再調整することで、得られた特徴を調整することができる。
次に、Denoising AutoEncoder (DAE)を見てみましょう。
オートエンコーダは、入力と同じ値を出力するように学習させます。
入力データには、状況に応じてノイズが含まれることがあります。
この例では、入力 4 に意図的にガウスノイズを加えています。
出力が 9 のように見えるため、世代交代は失敗した。
解決策としては、シンプルで強力なアルゴリズムが必要である。
入力データにのみ様々なノイズを付加し、出力はオリジナルデータを与える。
ノイズに強い特徴量を得ることができる。
データを強制的に緑の線に乗せる可能性が高くなる。
DAEの考え方は、様々な応用が可能です。
次に、変分オートエンコーダ（VAE）を見てみましょう。
オートエンコーダは、隠れ層として特徴量を持つ。
出力層は入力を再構成するように動作する。
内部値は決定論的である。
一方、VAEでは隠れ層のユニットが確率的な値を表す。
これにより、出力層は入力を表す確率を生成することができる。
内部値も確率的に動作する。
例えば、内部値が「0.5」であれば、50%の確率で「1」を出力する。
ポイントは、内部値が確率であって、通常のオートエンコーダのように明確な値ではないということだ。
それでも、構造は似ている。
隠れ層は乱数を表現する。
これは乱数の平均値と分散を表しています。
この図は説明のために90度回転している。
乱数は、乱数発生器に種を入力することで得られる。
得られる値はシードで異なるが、同じシードからは同じ乱数が得られる。
入力はシードとして使用される。
上の2つの画像は、どちらも「0」の形をしている。
0 っぽい画像が 0 の種を作る。「4 のような画像は 4 の種を作る。
隠れ層の値は、乱数発生器と同じように、シードによって作られる。
このことから、このモデルの処理として、乱数生成処理を比較することができる。
出力層は、入力を表す確率を与える。
このモデルは乱数によって動作しているため、同じ入力に対して異なる出力を与えることができる。
出力層を可視化すると、ぼやけた出力が得られる。
簡単におさらいしておこう。
入力の流れは下から上に向かっています。
まず、入力層から種を生成する。
似たような画像から似たような値を生成する。
得られたシードから隠れ層用の乱数を生成する。
最後に、隠れ層から出力層に変換される。
その値は確率として表される。
以上でオートエンコーダの講義パートは終了です。
演習パートにお進みください。
皆さんこんにちは、AIゼミの講義を始めます。
今日のトピックはRecurrent Neural Networks (RNNs)です。
以上が今日の講義のトピックです。
まず、時系列データについて説明します。
次にRNNの説明をします。
では、時系列データについて説明します。
時系列データとは、時間の経過とともに変化するデータのことです。
これは時系列データの例です。
今、私が話している自然言語が時系列データです。
文章や会話も自然言語に含まれる。
他にも、株価や天気など、日々変化するデータも時系列データです。
一般に、ある時刻の値は、過去の時刻の値や未来の値と相関があることが知られている。
例えば、自然言語には文法がある。
日本語の場合、動詞や名詞は一定の規則に従って出現する。
従って、この時系列データは、過去データ未来データに関連するデータであると考えられる。
簡単な例として、気温のデータを見てみよう。
このグラフは、横軸に日付、縦軸に気温をとっています。
横軸は左から右へ時間が経過していきます。
このグラフは、温度が徐々に上がっていく関係を示しています。
このデータを使って何ができるかを考えてみましょう。
例えば、温度を予測することができます。
気温が予測できれば、天気予報に役立てることができる。
しかし、時系列データを処理するのは非常に難しい。
時系列データは、過去のデータと未来のデータに相関があることが多い。
そのため、直前に蓄積されたデータも考慮する必要がある。
例えば、1月4日の気温を予測するには、直前のデータを考慮する必要がある。
1月4日の気温を予測するためには、1月1日から1月3日までのデータを考慮する必要があります。
前回の講義で学んだMLPで、どのようにデータを予測するかを考えてみましょう。
この場合、参照する過去のデータの数によってMLPの構造が変化します。
この時系列データでは、1月3日だけを見て1月4日の気温を予測することは困難です。
先ほども申し上げたように、長い期間のデータを使った予測の方が精度が高いのです。
また、翌日の気温を予測するために、どれだけの過去のデータが必要なのか判断が難しいところです。
しかし、MLPの場合、参照範囲を決めないと構造を作ることができません。
例えば、過去の気温を3日分参照したい場合、入力層の数は3つです。
過去4日間の気温を参照するのであれば、入力層の数は4層になる。
このように、参照する量をあらかじめ決めておかないと、ネットワークの構造を定義することはできない。
また、何日分を参照すべきかを知ることも困難である。
そこで登場するのが、RNN（Recurrent Neural Networks）である。
まず、その構造を見てみよう。
Recurrent Neural Networksは、略してRNNと呼ばれます。
このニューラルネットワークは、隠れ層にフィードバック構造を持っています。
この方式は、ニューラルネットワークに時間の概念を導入しています。
MLPの場合、入力は隠れ層（複数可）で処理され、出力（複数可）を得ることができます。
RNNは、隠れ層にフィードバック構造を持つ。
フィードバックの接続も、訓練によって獲得される。
RNNの特徴の一つは、h^t が現在の入力と1回前の隠れ層の状態によって決定されることである。
まず、簡単に理解してみよう。
RNNは、隠れ層にフィードバック接続がある。
本講義では、今後、このブロック図を使用することにします。
ニューロンやシナプスを使った表記ではなく、このブロック図を使います。
右図の四角と矢印は、左図と同じ意味であることを忘れないでください。
このうち、黒い矢印だけが右を向いている図を、RNN（Forward Propagation Network）と呼びます。
RNNの重要な特徴は、自分自身へのフィードバック接続を持っていることです。
ここでも、今後は右の表記を使うことにする。
まず、RNNの順方向の計算を考えてみましょう。
隠れ層の出力「h^t」の計算方法は、MLPのそれとは異なります。
MLPの場合、隠れ層の出力は、入力に重みを掛けて活性化関数を適用することで得られます。
RNNの場合は、フィードバック接続があるため。
隠れ層の出力は、...によって得られる。
フィードバック接続と、1回前のステップの隠れ層の状態を掛け合わせる。
を、入力と出力の掛け算に加えて行う。
比較的理解しやすいと思います。
左からの入力をフィードバック接続の部分と一緒に考えるということです。
さて、前方伝搬の計算です。
ネットワークをどのように学習させるか考えてみましょう。
ネットワークの学習は，MLPのようにバックプロパゲーションによって，重みとバイアスの勾配を計算することで行います．
重みとバイアスは最適化することで更新されます。
下の図を見てみよう。
順伝播における出力は、目標と比較され、その誤差がフィードバックされます。
この黒い矢印はMLPと同じです。
RNNの場合は、このフィードバック接続も勾配を利用して更新されます。
ポイントは、黄色の部分をどう計算するかです。
隠れ層を時間方向に展開していきます。
ネットワークを時間方向に展開する方法を考えてみましょう。
隠れ層の入力は、前の時間の隠れ層の出力と、現在の時間の入力です。
黄色の部分をどのように計算するかを考えてみよう。
黄色の部分を時間軸で展開すると、右図のようにネットワークが表現されます。
t, t = 1, t = 2, t = T は時間である。
一番左の「t＝1」を考えてみましょう。
時刻1の入力「x^1」は隠れ層を通過し、出力「y^1」が得られます。
次に、「t = 2」を考えてみましょう。
現在の入力 x^2 と出力 y^2 を組み合わせて、出力 y^2 を得る。
と1回前のステップの隠れ層 h^1 の状態から得られる。
h^1」から「h^2」への矢印は、左図の「h^(t - 1)」に相当する。
t = 3」、「t = 4」についても同様である。
回数を増やすと、横方向に拡大される。
左の図では、フィードバック接続で表現されています。
この黄色い部分はすべて隠れ層です。
隠れ層が「T」個ある巨大なMLPと考えられます。
巨大なMLPですから、バックプロパゲーション方式を使うことができます。
この考え方は、時間による逆伝播という考え方です。
略してBPTTといいます。
BPTTは巨大なMLPなので、各グラフから誤差を伝播させることで、この隠れ層の重みを学習させることができます。
これは非常にシンプルな考え方です。
BPPTは時系列データの扱い方によって、大きく2つのパターンに分けられます。
1つ目：対象データを全時間ステップに提供する場合。
これは、毎回データを予測するときに使う。
誤差を計算し、全時間帯で合計してバックプロパゲートする。
2つ目：対象データが最終回のみ提供される場合。
これは、ある時間間隔で分類問題を解くときに使われる。
誤差は最終時刻に計算され、バックプロパゲートされる。
最初のパターンであるBPTT1を見てみましょう。
すべての時間ステップで対象データがあります。
この例では、3回の時間ステップを考えてみよう。
x^1 が入力されると、y^1 が出力される。
すると、「y^1」に対して「y^1_(target)」となり、誤差「E^1」が得られます。
同様に、x^2 に対して y^2 が得られ、誤差 E^2 が求まる。
E^3 も同様に求めることができる。
得られた3つの誤差を合計することで、バックプロパゲーションを計算することができる。
典型的な例として、翌日の気温を予測する問題がある。
この例では、過去数日間の気温をもとに翌日の気温を予測する。
3日分のデータを使って翌日のデータを予測したい場合、それらのデータをターゲットとして与える。
誤差はすべてのデータについて累積することができる。
このネットワークでは、1月1日の気温から翌日の気温「y^1」を2.8度予測する。
翌日の実データが得られるので、それをターゲットとする。
そして、「y^1」からの誤差をフィードバックして、「h^1」を学習させる。
次に、1月3日の気温を予測する。
このとき、ネットワークは1月1日と2日のデータを使って気温を予測する。
y^2 は、この現在のステップと前のステップの値の2つを合成することで得ることができる。
1月3日のデータをターゲットとして、y^2 から誤差を求める。
h^1」「h^2」ともに、得られた誤差「E^2」をバックプロパゲートすることで更新される。
ポイントは「h^2」で分岐があること。
ネットワークを何度も展開すると、この図のようになる。
例えば、この部分は「E^1」「E^2」「E^3」の誤差を全て合計することで更新される。
この例では、3つのタイムステップのデータを使っている。
この操作をルール化することで、多くの時間ステップのネットワークを計算するプログラムを作ることができる。
次のパターン、BPTT2を見てみよう。
この場合、ターゲットは最後の時間であるステップ3について与えられる。
この場合、誤差「E^3」だけが分岐してバックプロパゲートされる。
典型的な例としては、分類がある。例えば、気温のグラフをもとに季節を分類する。
例えば、7タイムステップのデータが与えられると、「冬」というラベルが出力される。
BPTT2の考え方は、BPTT1とほぼ同じです。
違いは、すべてのデータが入力されたときの誤差を計算することである。
フォワードプロパゲーションの場合は、この図のようにマージされます。
バックプロパゲーションの場合は、この図のようにマージされます。
このように、巨大なRNNを巨大なMLPとみなして、バックプロパゲーションを用いて学習させることができます。
しかし、RNN、特にBPTTで学習させたRNNには欠点があります。
BPTTは、巨大なMLPと考えることができます。
T=100のネットワークは、100層のMPLに相当します。
「y^T は、隠れ層へのデータの通過を100回行うことで得られます。
順伝播の場合、長期の時系列データを蓄積することはできない。
バックプロパゲーションで学習する場合、勾配消失問題や勾配爆発問題が発生する。
これらの問題は、ニューラルネットワークの問題として古くから知られている。
このため、学習がうまくいかない。
この問題を克服するために、いくつかのネットワークが設計されている。
代表的なものはLSTM(Long Short-Term Memory)である。
LSTMはBPTTの欠点を克服している。
このネットワークは長期記憶を獲得することができます。
これは、長い文章を扱うときによく使われます。
LSTMでは、隠れ層の出力が十分に活用されているかどうかを内部で判断しています。
LSTMの基本的な考え方だけ説明します。
入力ゲートは、ネットワークへの入力量を調整する。
メモリーセルは、以前の状態を記憶する。
忘却ゲートは、状態の忘却度を調整する。
出力ゲートは、次の層への出力量を調整する。
このように、LSTMは複数のゲートでネットワークの入出力と状態を制御している。
LSTMについてもっと詳しく知りたい方は、教科書やインターネットで公開されている記事などを参考にしてください。
今日は、リカレントニューラルネットワークについて見てきました。
時系列データの扱い方とBPTTについて。
最後に、実用的なRNNの1つであるLSTMの基本的な考え方について見てきました。
これで本日の講義は終了です。
今日は、畳み込みニューラルネットワーク（CNN）について勉強します。
CNNは、画像認識のために設計された強力なニューラルネットワークです。
現在のAIブームの火付け役となりました。
2012年の画像認識コンテストで、このCNNは性能に大きな差をつけて1位を獲得しました。
AIのブレークスルーとなり、その後もさまざまなCNNのモデルが開発されている。
CNNは画像処理の基本モデルであり
CNNは、物体認識、姿勢推定、物体検出、セグメンテーションなど、画像処理の基本モデルである。
また、CNNは様々な分野で応用されている。
また、自然言語処理、音声信号処理、時系列予測など、様々な分野で応用されている。
CNNは畳み込み処理とアフィン変換から構成されている。
有名なCNNはLeNetモデルである。
は、畳み込み処理として畳み込みとプーリングを2回行う。
LeNetはアフィン変換のために3つの完全連結層を持っている。
畳み込み処理では、画像の特徴を抽出する。
したがって、このネットワークは、基本的なニューラルネットワークに特徴抽出を追加したものである。
しかし，完全連結型ネットワークである多層パーセプトロン（MLP）は，分類問題を扱うことができます．
これは、特定のパターンに特化したものです。
スライドにある7という数字は、人間にとってはどちらも同じに見えますが、この2つの7はそれぞれ微妙に異なっています。
しかし、この2つの7は微妙に異なっています。
白いピクセルが下にずれているのです。
MLPでは、このようなシフトした特徴を扱うことができません。
例えば、このネットワークを学習させ、白いピクセルが中央に来たとき、ニューロンが強く発火したとします。
もし，1ピクセルだけシフトしていたらどうなるでしょうか？
このネットワークはシフトされた入力に対して学習していない。
最終層は発火しない
というのも，強く接続された部分と白いピクセルの位置が一致しないからです．
そのため，MLPでは，人間の目の中でわずかな違いがあっても，その特徴を抽出することができません．
しかし、CNNはこの問題を克服しています。
それは，局所的なパターンに特化したものです．
MLPでは、入力層と隠れ層にあるすべてのニューロンが接続されていました。
単純な接続パターンを繰り返し配置することで
隣接するニューロンと接続されます．
その結果，局所的なパターンを広範囲に抽出することができるのです．
また、平行移動に対して不変なニューラルネットワークを構築することが可能である。
CNNは、畳み込みとプーリングの組み合わせで特徴を抽出する。
小さなフィルタからエッジ情報を抽出することができる。
画像認識に有用である。
複雑なセルは単純なセルの組み合わせで構成される。
また、プーリングにより、動きに対して不変な特徴量を抽出する。
このように、認識に必要な特徴を効率的に抽出することができる。
それでは、畳み込みとプーリングについて、もう少し詳しく見ていこう。
畳み込みは、画像フィルタに似ている。
例えば、スライドに示すような入力パターンとフィルターパターンがあるとする。
この式に示すようにコンボリューションを実行します。
行列の左上の領域にフィルタを適用した結果は24です。
画像の画素とフィルタの乗算積算を実行します。
水平方向のエッジと垂直方向のエッジを抽出する画像フィルタを作ることができる。
CNNの学習により、画像フィルタはより効果的に画像を認識するようになる。
次の処理は、プーリングである。
これには2つの目的がある。
1つは、データの次元を減らすことである。
フィルタを適用して特徴量を抽出すると、特徴量の次元数が減少する。
特徴量の次元数が増えていきます。
ポイントは、重要な情報を残すために次元を減らすことである。
もうひとつは、翻訳不変性の獲得である。
プーリングは大きく分けて2種類ある。
この例では、4×4画像を2×2画像に変換している。
max-poolingは、ブロック内で最も大きな値を返す処理である。
平均値プーリングは、ブロックの平均値を出力する処理である。
ここで、なぜプーリングが移動不変性を獲得できるかを考えてみよう。
この例では、4画素に対してmax-poolingが実行されている。
右側の画素では、文字認識に有用と思われる白い画素を1画素分下方にずらしている。
どちらの場合も、max-poolingの結果は同じである。
を抽出することができる。
動き不変の特徴を獲得できる。
ここまで、CNNの基本である畳み込みとプーリングについて見てきた。
エッジ上の画素にはフィルタが適用されない場合がある．
境界付近の画素はパディングで処理することができる．
パディングを0にすることをゼロパディングという．
パディング、スライドをネットワークパラメータとして設定することができる。
パディングを行うことで、入力と同じ大きさの出力を得ることができる。
ここで、なぜプーリングが移動不変性を獲得できるかを考えてみよう。
この例では、4画素に対してmax-poolingが実行されている。ストライドとは、フィルタのシフト量である。
Stride 1は最小値。つまり、フィルタを1画素分ずらしたことになる。
右側の画素では、文字認識に有用と思われる白い画素が1画素分下方にずれている。
ストライド2は、フィルタを2ピクセル分ずらすことを意味します。
ストライドはパラメータとして設定することもできる。
得られる特徴マップの大きさは、パディングの数とストライドに依存する。
小さなフィルタによって局所的な特徴を抽出することができる．
CNNには多くのモデルがあり、有名なCNNはユニークな名前を持っている。
LeNet は古いニューラルネットワークである
2つの畳み込み層(Conv.)と3つの完全連結層(FC)で構成されている。
AlexNetは2012年のコンペティションで優勝した有名なCNNネットワークです。
他にもVGG、GoogLeNet、ResNetなど有名なネットワークがたくさんあります。
この他にも、VGGやGoogLeNet、ResNetなど、有名なネットワークがたくさんあります。
毎月のように発表されています。
演習パートでは、CNNを使った文字認識の課題に取り組んでください。
ディープラーニングの仕組みの１つに、ニューラルネットワークと呼ばれるものがあります。これは、人間の脳に存在する神経細胞を真似たものです。
オートエンコーダは、ニューラルネットワークの仕組みの１つです。入力されたデータを、後で復元できる状態に圧縮する処理を指します。通常、データを圧縮すると情報の一部が欠落します。しかし、欠損部がなくても再現できるよう、重要度の高い情報を残しておかなければなりません。そこで、オートエンコーダでは、重要度の高い情報を洗い出し、それ以外の部分を削ぎ落します。たとえば、体格を表すのに、身長と体重の関係を二次元グラフ上のプロットで示したり、プロット結果を近似した一本の直線（一次元）で表したりすることがあります。オートエンコーダでは、その意味を維持したままより少ない次元に落とし込むことが行われます。この作業を「次元削減」または「特徴抽出」と呼びます。ニューラルネットワークは、入力値と出力値の間に「隠れ層」と呼ばれる圧縮状態を挟むのが特徴です。このようにすることで、従来では不可能だった複雑な処理も可能になりました。そこで、さらに多くの隠れ層を挟むことで、より高度な処理能力が実現するのではないかと考えられるようになりました。ところが、実際に試みたところ、かえって性能が下がりました。この方法では予測と正解の誤差のフィードバックが必要ですが、階層をさかのぼるごとに誤差が減少し、学習速度が低下したのです。この問題を「勾配消失」と言います。オートエンコーダは勾配消失を解決する方法として期待されました。ニューラルネットワークの初期値に、オートエンコーダで学習させたものを用います。事前学習を行うことで、勾配消失による学習速度低下を防止します。過学習とは、特定のデータへの対応にのみ長けてしまうことです。訓練に使ったデータを完全に記憶してしまうと、処理の練習になりません。結果的に、未知のデータにはまったく対処できない状態になります。
そこで活用されたのがオートエンコーダです。オートエンコーダによりデータを粗な状態にすることで、過学習を防止します。
オートエンコーダを用いた事前学習による、勾配消失の防止手順は以下のとおりです。
１．層を分割する
ネットワークを構築する多層構造を、複数の単層に分割します。入力層、隠れ層１、隠れ層２……出力層と分割され、それぞれに事前学習を行います。
２．入力層から学習させる
分割する前の構造を想定し、入力層から順に教師なし学習を行います。隠れ層１と同じサイズの訓練用隠れ層を１つだけ用意し、圧縮・出力させます。
３．すべての隠れ層で訓練を繰り返す
ステップ２で得た訓練用隠れ層の値を入力値として、隠れ層１の訓練を行います。この際の訓練用隠れ層は、隠れ層２を想定し、それと同じサイズにします。この作業をすべての層で繰り返しましょう。
４．元のネットワークに戻す
学習させた層で元のネットワークを構築します。この状態で新しい層を追加して学習させると、勾配消失を防止できます。
オートエンコーダとは、ニューラルネットワークの1つです。入力されたデータを一度圧縮し、重要な特徴量だけを残した後、再度もとの次元に復元処理をするアルゴリズムを意味します。このように、小さい次元に落とし込む作業を次元削減や特徴抽出と呼びますが、オートエンコーダはそれだけでなく、生成モデルとしても用いられます。オートエンコーダは、2006年にトロント大学のコンピュータ科学および認知心理学の研究者であるジェフリー・ヒントン氏らによって提唱されました。ディープラーニングは、このオートエンコーダを何層にも重ね合わせてできた構造を持っています。オートエンコーダの仕組みはそのままディープラーニングの仕組みだと言えるでしょう。オートエンコーダは入力層のノードでデータを受け取り、隠れ層に圧縮します。この時に重みづけと呼ばれ、データはその重要度にあわせて点数がつけられます。この点数が低いデータは除外されます。これをエンコードと言います。データが出力層に移る時も重み付けされ、ノードが複数のエッジから受け取ったデータの合計が最終的な値になります。これをデコードと言います。ニューラルネットワークにおいて、層を重ねることで、より複雑な処理ができます。主に採用されている計算方法である誤差逆伝播法においては、層を重ねすぎると逆に精度が落ちてしまいます。誤差逆伝播法では、予測と正解の誤差を利用して学習していくのですが、その誤差は層を重ねるとともに消えていってしまい、最終的に精度が落ちてしまいます。これを勾配消失と言います。事前学習を採用しているオートエンコーダは勾配消失を避けられます。過学習とは、学習の訓練データに適合しすぎて、評価データに対応できなくなることです。過学習になると、学習データだけに最適してしまい、汎用性が失われ、実際に使うことができません。この解決策として、ちょうど良いタイミングで学習をストップさせる方法や、学習データ数を増やして解決する方法もありますが、オートエンコーダを用いることでも解決できます。次元を圧縮するとデータが荒くなるので、過学習を防ぐことができます。積層オートエンコーダは、先述のオートエンコーダを何層にも重ねたシンプルな構造をしています。1層ずつ学習していくのがポイントで、一気にエンコードして、一気にデコードするのではなく、エンコードとデコードを交互に実施しています。1層ずつ学習していくことで、初期値を最適解に近づけることができ、ファインチューニングすればすぐに使えるようになります。現在、技術の発展により、積層オートエンコーダのメリットをカバーできるようになったため、あまり使われていません。積層オートエンコーダは、始めに示したシンプルなオートエンコーダのエンコーダおよびデコーダ部分を多層化した構造となります。入力データはエンコーダにおいて段階的に次元を減らし、デコーダで復元されます。エンコーダおよびデコーダを多層化することで、より複雑で高度な特徴量抽出を狙っています。
当初このような深いニューラルネットは、勾配消失という問題によってうまく学習することができませんでした。これは、層を深くしていくと前半の層で勾配がほとんどゼロになってしまい学習が進まなくなる、という問題です。これを解決するために、積層オートエンコーダは、ニューラルネットの各層における学習を一層ずつ行って、最後にすべて積み重ねる、という手順で構築します。入力データセットを復元するように1層目の学習を行った後、得られた重みを固定値として使って次の層である2層目の学習を行います。これを繰り返して、3層目までの重みを得ることができます (事前学習）。最後に1層目から3層目をすべて繋げて元々のネットワークを構築し、得られた重みを初期値として設定したのち、全体の学習を通して重みを微調整（ファインチューニング）します。1層ずつの学習とすることで初期値が最適解に近き、適切な学習が可能となります。他にも、ノードを減らしていくというオートエンコーダのネットワーク構造自体が、ディープニューラルネットワークのもう一つの課題であった過学習の抑止に繋がるというメリットがあります。
しかし、その後の技術の発達により、ディープニューラルネットワークは事前学習無しでも適切な学習が可能となり、積層オートエンコーダをこのような事前学習に用いることはほぼなくなっています。
オートエンコーダは、ただデータの圧縮と復元をするだけでしたが、変分エンコーダはデコードする際に変数を混ぜることで、入力とは少し違う出力をします。そのため、変分エンコーダは生成モデルとして有名です。変分オートエンコーダは、生成モデルとして有名です。通常のオートエンコーダとの大きな違いとして、入力データを圧縮して得られる特徴ベクトル（潜在変数）を確率変数として表します。一般的にはＮ次元の潜在変数が、Ｎ次元正規分布に従うように学習します。これまでに紹介したオートエンコーダでは、次元削減後の特徴ベクトルには特に制約はありませんでした。そのため、特徴空間上でデータがどのように表現されているかはわかりません。しかし、VAEでは、ここに正規分布という制約を設けることでデータの潜在空間上での分布に連続性が生じ、似た潜在変数からは似たデータが生成されるようになります。エンコーダの出力として平均と標準偏差を推定し、それらで表される正規分布からランダムサンプリングによりデコーダに入力する潜在変数を決めます。しかし、このランダムサンプリングという操作は微分不能という問題があります。誤差逆伝搬（バックプロパゲーション）し、ネットワークを学習させるためには、各ノードをつなぐエッジが微分可能な演算でなければいけません。そこでReparameterization trickという方法を用います。標準正規分布からランダムにサンプリングして得る確率変数εを導入し、これを用いて  として潜在変数 を決定します。このようにすることで と は微分可能なエッジで繋がり、バックプロパゲーションが可能となります。条件付き変分オートエンコーダは、変分オートエンコーダを拡張したもので、指定したクラスのデータを生成できるようになっています。変分オートエンコーダでは、デコーダによるデータ生成の際、潜在変数を指定することはできますが、出力データのクラスを直接指定できません。言い換えますと、潜在変数を変えると出力クラスも変わってしまう可能性があります。条件付き変分オートエンコーダは、出力クラスを指定したうえで、潜在変数も自由にコントロールすることができます。変分オートエンコーダのネットワークと異なる点は、エンコーダ、デコーダともにラベル情報を入力していることです。ラベル情報の入力にはいくつかやり方があります。例えば、デコーダの場合1次元の潜在変数ベクトルに結合する方法や、足し合わせる方法などです。このようにラベル情報を学習に含めることで、学習後、指定したラベルのデータを生成することができるようになります。

畳み込みオートエンコーダ(CAE)は、畳み込みニューラルネットワーク（CNN）を用いたオートエンコーダです。CNNとは、入力層と出力層の間に、入力データの特徴量を捉える「畳み込み層」と、その特徴への依存性を減らす「プーリング層」を加えたニューラルネットワークのモデルです。
畳み込み層により特徴を抽出できるため、画像の処理に利用することが多いです。
先述の通り、学習データを入力すると、まだ存在しないデータを出力できます。たとえば、数字が書かれた画像を入力すると、あたかも他の人が書いたかのような新しい画像を生成できます。異常検知は、入力されたデータが正常か異常か判断し、異常を検知することです。オートエンコーダは教師なしで異常検知できることが特徴です。教師ありで異常検知する場合は正常データと異常データの2つをデータセットとして学習させなければいけませんが、教師なしでは、正常データのみで学習できます。また、正常データを集めるのは容易なので、異常検知はオートエンコーダにうってつけの作業です。
正常データだけで学習すると、復元後は正常データに近いデータが出力されます。つまり、異常データを入力すると、正しく復元できず、もとの入力データとの誤差が大きくなります。その誤差を用いることで、正常か異常か判断します。また、誤差が大きい場所を見れば、異常が発生している場所を特定できます。
ノイズ除去は、入力データ中の不必要な部分を除去することです。オートエンコーダにノイズのあるデータを入力し、ノイズのないデータを正解データとして、教師あり学習をさせます。学習後にデータを入力すると、ノイズのない綺麗なデータを出力するように、エンコードされます。
クラスタリングは、データの特徴ごとに分類することです。オートエンコーダでは、エンコード時に特徴が抽出されるので、その特徴ごとにクラスタリングできます。こちらも正常データと異常データの2つを学習させれば、異常検知に用いることができます。勾配消失や過学習を避けるために開発されたオートエンコーダですが、現在はその目的で利用されてはおらず、データ生成や異常検知のために使用されています。オートエンコーダ とは、NN (Neural Network) を用いて実現される次元圧縮・特徴抽出に使われる機械学習アルゴリズムです。2006 年に Geoffrey Hinton 教授が提案しました。
基本のオートエンコーダは入力層と出力層が同じになるように学習をすすめます。
オートエンコーダの学習は、入力データと一致するデータを出力することを目的とする教師なし学習です（後述のように教師あり学習とすることもできます）。オートエンコーダのネットワークは、入力したデータの次元数をいったん下げ、再び戻して出力するという構造になっています。このため、入力から出力への単なるコピーは不可能です。オートエンコーダの学習過程では、入出力が一致するように各エッジの重みを調整していきます。この学習を通して、データの中から復元のために必要となる重要な情報だけを抽出し、それらから効率的に元のデータを生成するネットワークが形成されます。こうしてオートエンコーダの前半部分は次元削減、特徴抽出の機能を獲得し、後半部分は低次元の情報をソースとするデータ生成機能を獲得します。前半部分をエンコーダ、後半部分をデコーダと呼びます。
学習後、この2つのネットワークは別々に使うことができます。すなわち、エンコーダは特徴抽出器、デコーダは生成器として独立に用いることができます。
Attention 機構は、現代の深層学習の花形と言える仕組みです。
歴史的には、Seq2Seq と呼ばれる自然言語処理などで使用されるリカレントニューラルネットワークベースのモデルに対して組み込まれ、大きな注目を集めました。近年では、機械翻訳のために提案された Transformerとそれをベースにした BERTなどの高度なモデルで全面的に使用され、様々な自然言語処理のタスクで最高精度を更新し続けています。
一方で、Attention 機構は画像認識の手法でも使用されています。日本で有名なのは、、Attention Branch Network（ABN）などがあります。ABN では、Attention 機構によって、CNN が出力した特徴量マップのどの領域に注目すればいいのかを自動的に学習し、自動車の車種やメーカーを高精度で認識できるようなモデルの構築に成功しています。また、動画中で人が行っている行動を認識するタスクのための手法である Video Action Transformer Networkのような研究もあり、動画に対する認識でも、その有効性が確認されています。


空間方向の Attention機構の一つのやり方である、空間方向の Attention について説明します。「空間」というのは、例えば自然言語処理では入力された単語列の一つ一つの単語の位置に相当し、画像認識では二次元画像中の位置に相当します。下図に、画像認識手法における一例を示します。入力された画像から、CNN を通じて特徴量マップが抽出される、というのはごく普通の処理です。この特徴量マップに対して、Conv 層を適用し、活性化関数を通すことで、幅や高さはそのままですがチャネル数が 1 の Attention マスクを作成します。活性化関数は Sigmoid を適用することが多いようですが、ReLU を使うケースもあれば、マスクの全体を足すと1になるような Spatial Softmax（空間的なSoftmax）を適用することもあります。このようにして得られた Attention マスクを特徴量マップに掛けることで、重み付き特徴量マップを得ることができます。
Attention マスクは、その値が大きければ大きいほどその領域に注目する、というマスクとして機能します。そのため、犬や猫の画像であれば、犬や猫を識別するのに役立つ部分はマスクの値が大きく、分類に役立ちそうにない領域（例えば背景領域）は値が小さくなることが理想的です。
空間方向の Attention は、Attention マスクを通じて、画像中のこの領域に着目していることがわかり、予測根拠と見なすこともできるため、XAI（Explainable AI）の代表的な実装例となっています。


チャネル方向の Attention
CNN のある段階で得られる特徴量マップに対して Global Pooling を適用し、幅と高さが 1x1 で、チャネル数はそのままの特徴量を得ます。その後、この特徴量に対して2層の全結合層を適用することで、チャネル数はそのままのマスクを得ることができます。このマスクは、各チャネルをどの程度強調するべきかを表しており、このマスクを元の特徴量マップに掛け合わせることで、注目すべきチャネルを強調した特徴量マップを得ることができます。


Attention 機構を使った画像分類モデル
今回は最もシンプルに、CNN によって得られた特徴量マップに対する空間方向の Attention を適用し、そのまま分類の全結合層につなぐ、というモデルを構築する。
Attention 機構を用いたモデルを実装するにあたってぜひ実施していただきたいのが、Attention の可視化のためのメソッドなり関数を早めに実装し、訓練途中で行われた処理結果を、画像として確認できるようにしておくということです。というのも、せっかく Attention マスクというわかりやすく訓練の進行を示してくれる指標があるわけですから、じっと訓練が終わるのを待ってから確認するよりは、訓練途中であっても随時 Attention マスクを確認し、訓練がうまく進んでいるか否かを確認したほうが、不具合を早期に発見できるためです。実際、Attention マスクは Weight Decay（L2 正則化）などの正則化の影響を受けやすく、結構極端な Attention マスクになることが多いです。例えば、画像に対する Attention マスクでは、画像の四隅に Attention が集中してしまい、そこから動かないといった極端な状態になることがあります。そのため、Attention 機構を用いたネットワークの訓練をするときは、各エポックの終わりなどに、いくつかのサンプルに対してどのような Attention が得られるのかを可視化し、確認しながら進めることをお勧めします。


マルチヘッド Attention 機構を使った画像分類モデル
上記のようなシンプルな Attention 機構は、Attention マスク自体は適切に訓練できる一方で、精度がいまひとつ、という欠点を抱えていました。これには様々な理由が考えられますが、一般には、Attention マスクが注目する領域にメリハリがなくなってしまったり、ごく一部の領域しか注目しなくなってしまう、という問題があります。例えば、犬や猫の画像だと、Attention マスクが犬や猫の全身を含むかなり広い領域になっていたり、逆に犬や猫の顔だけに集中したりという結果となることが多いです。そのため、前回のマルチタスク学習で見たときのように、得られる特徴量の中に、品種を見分けるときに抑えておきたいポイントが得られないという可能性が出てきます。Attention 機構をマルチヘッド化することを試みます。ここで言うマルチヘッド化とは、Attention 機構を複数用意して、それぞれが微妙に異なる役割を分担させることで、画像中の情報を漏れなく反映できるようにする、ということを意味します。理想的には、あるヘッドは犬や猫の耳に着目し、別のヘッドは犬や猫の顔に、また別のヘッドは犬や猫の足に、とヘッドごとに異なる部位の特徴を収集し、最終的に品種の分類に活かせる特徴量をつくる、ということが求められます。
Attention 機構は、現代の深層学習の花形と言える仕組みです。
歴史的には、Seq2Seq と呼ばれる自然言語処理などで使用されるリカレントニューラルネットワークベースのモデルに対して組み込まれ、大きな注目を集めました。近年では、機械翻訳のために提案された Transformerとそれをベースにした BERTなどの高度なモデルで全面的に使用され、様々な自然言語処理のタスクで最高精度を更新し続けています。
一方で、Attention 機構は画像認識の手法でも使用されています。日本で有名なのは、、Attention Branch Network（ABN）などがあります。ABN では、Attention 機構によって、CNN が出力した特徴量マップのどの領域に注目すればいいのかを自動的に学習し、自動車の車種やメーカーを高精度で認識できるようなモデルの構築に成功しています。また、動画中で人が行っている行動を認識するタスクのための手法である Video Action Transformer Networkのような研究もあり、動画に対する認識でも、その有効性が確認されています。


空間方向の Attention機構の一つのやり方である、空間方向の Attention について説明します。「空間」というのは、例えば自然言語処理では入力された単語列の一つ一つの単語の位置に相当し、画像認識では二次元画像中の位置に相当します。下図に、画像認識手法における一例を示します。入力された画像から、CNN を通じて特徴量マップが抽出される、というのはごく普通の処理です。この特徴量マップに対して、Conv 層を適用し、活性化関数を通すことで、幅や高さはそのままですがチャネル数が 1 の Attention マスクを作成します。活性化関数は Sigmoid を適用することが多いようですが、ReLU を使うケースもあれば、マスクの全体を足すと1になるような Spatial Softmax（空間的なSoftmax）を適用することもあります。このようにして得られた Attention マスクを特徴量マップに掛けることで、重み付き特徴量マップを得ることができます。
Attention マスクは、その値が大きければ大きいほどその領域に注目する、というマスクとして機能します。そのため、犬や猫の画像であれば、犬や猫を識別するのに役立つ部分はマスクの値が大きく、分類に役立ちそうにない領域（例えば背景領域）は値が小さくなることが理想的です。
空間方向の Attention は、Attention マスクを通じて、画像中のこの領域に着目していることがわかり、予測根拠と見なすこともできるため、XAI（Explainable AI）の代表的な実装例となっています。


チャネル方向の Attention
CNN のある段階で得られる特徴量マップに対して Global Pooling を適用し、幅と高さが 1x1 で、チャネル数はそのままの特徴量を得ます。その後、この特徴量に対して2層の全結合層を適用することで、チャネル数はそのままのマスクを得ることができます。このマスクは、各チャネルをどの程度強調するべきかを表しており、このマスクを元の特徴量マップに掛け合わせることで、注目すべきチャネルを強調した特徴量マップを得ることができます。


Attention 機構を使った画像分類モデル
今回は最もシンプルに、CNN によって得られた特徴量マップに対する空間方向の Attention を適用し、そのまま分類の全結合層につなぐ、というモデルを構築する。
Attention 機構を用いたモデルを実装するにあたってぜひ実施していただきたいのが、Attention の可視化のためのメソッドなり関数を早めに実装し、訓練途中で行われた処理結果を、画像として確認できるようにしておくということです。というのも、せっかく Attention マスクというわかりやすく訓練の進行を示してくれる指標があるわけですから、じっと訓練が終わるのを待ってから確認するよりは、訓練途中であっても随時 Attention マスクを確認し、訓練がうまく進んでいるか否かを確認したほうが、不具合を早期に発見できるためです。実際、Attention マスクは Weight Decay（L2 正則化）などの正則化の影響を受けやすく、結構極端な Attention マスクになることが多いです。例えば、画像に対する Attention マスクでは、画像の四隅に Attention が集中してしまい、そこから動かないといった極端な状態になることがあります。そのため、Attention 機構を用いたネットワークの訓練をするときは、各エポックの終わりなどに、いくつかのサンプルに対してどのような Attention が得られるのかを可視化し、確認しながら進めることをお勧めします。


マルチヘッド Attention 機構を使った画像分類モデル
上記のようなシンプルな Attention 機構は、Attention マスク自体は適切に訓練できる一方で、精度がいまひとつ、という欠点を抱えていました。これには様々な理由が考えられますが、一般には、Attention マスクが注目する領域にメリハリがなくなってしまったり、ごく一部の領域しか注目しなくなってしまう、という問題があります。例えば、犬や猫の画像だと、Attention マスクが犬や猫の全身を含むかなり広い領域になっていたり、逆に犬や猫の顔だけに集中したりという結果となることが多いです。そのため、前回のマルチタスク学習で見たときのように、得られる特徴量の中に、品種を見分けるときに抑えておきたいポイントが得られないという可能性が出てきます。Attention 機構をマルチヘッド化することを試みます。ここで言うマルチヘッド化とは、Attention 機構を複数用意して、それぞれが微妙に異なる役割を分担させることで、画像中の情報を漏れなく反映できるようにする、ということを意味します。理想的には、あるヘッドは犬や猫の耳に着目し、別のヘッドは犬や猫の顔に、また別のヘッドは犬や猫の足に、とヘッドごとに異なる部位の特徴を収集し、最終的に品種の分類に活かせる特徴量をつくる、ということが求められます。
深層強化学習（DRL）は、強化学習（RL）と深層学習を組み合わせた機械学習のサブフィールドです。RLは、試行錯誤によって意思決定を行うことを学習する計算エージェントの問題を考慮します。Deep RLは、ソリューションにディープラーニングを組み込んでいるため、エージェントは、状態空間を手動で設計しなくても、非構造化入力データから意思決定を行うことができます。。Deep RLアルゴリズムは、非常に大きな入力（たとえば、ビデオゲームで画面にレンダリングされるすべてのピクセル）を取り込み、目的を最適化するために実行するアクション（たとえば、ゲームスコアの最大化）を決定できます。深層強化学習は、ロボット工学、ビデオゲーム、自然言語処理、コンピュータービジョン、教育、輸送、金融、ヘルスケアなど、さまざまなアプリケーションに使用されてきました。深層強化学習とは強化学習と深層学習の手法を組み合わせた物で、代表的な手法に、Deep-Q-Network(DQN)があります。DQNは、Q学習における行動価値関数(Q関数)を、畳み込みニューラルネットワークに置き換えて近似したものです。
ただ単純にQ関数を畳み込みニューラルネットワークを置き換えても、学習がうまくいくわけではないので、学習を収束させるための工夫がなされています。Q学習では、状態数 s × 行動数 a のテーブルを更新することによってQ関数を更新していましたが、状態数が大きくなってくると、テーブルによってQ関数を表すことが現実的ではなくなってきます。これを解決するために、Q関数を畳み込みニューラルネットワークで表現するアプローチをとり、学習が収束するための工夫をしたものがDeep Q Networkです。Deep-Q-Networkの略称で、前述の強化学習における行動価値関数の部分を、畳み込みニューラルネットワーク(CNN)で近似した手法。学習を収束させるための工夫がなされています。DQNでは行動価値関数を畳み込みニューラルネットワーク(Convolutional Neural Network, CNN)で近似しています。CNNは入力値に画像などの行列をとり、畳み込み層のフィルターとプーリング層で入力の特徴量を処理していき、出力に画像のクラス分類予測などの確率を出力します。DQNでは、リサイズされた84×84グレースケールの4時刻分のゲーム画面を入力にとり、ゲーム環境での行動を出力します。DQNの学習を収束させるための工夫として、Experience Replayがあります。Experience Replayは、エージェントが経験した過去の体験をreplay memolyに一定期間保存して置き、過去の経験をランダムにサンプリングして学習を行う手法です。データ間に強い相関があると学習が収束しなくなってしまうことを防ぐために、学習に使用するデータの偏りを無くすのが目的です。深層強化学習では、Q関数の更新が1時刻前(t−1)のニューラルネットワークのパラメータ(重み・バイアス)に依存しています。パラメータは学習ごとに更新されるので、目標とする評価関数が安定しなくなり、学習が収束しにくくなってしまいます。そこで学習を安定させるために、Q関数の更新の際に参照するネットワークを固定します(過去のネットワークで固定して、更新の際に参照するQ関数のパラメータを固定します)。この固定された過去のQ関数は、一定周期ごとに新しいQ関数に置き換えられます。AlphaGoは深層強化学習の技術を用いた囲碁のAIです。2015年10月にヨーロッパ王者のプロ2段の棋士に19路盤でのハンデ無し戦で勝利し、2016年3月にはLee Sedol(9段)に4-1で勝利しました。
囲碁の盤面を19 × 19の画像として扱うことによって、方策・価値ネットワークへの入力としています。AlphaGOはモンテカルロ木探索と複数のネットワークが使用されており、その一部にDQNの技術が使用されています。Qテーブルの学習にディープラーニングを活用することで、たとえば、ビデオゲーム画面などの画像を状態sとしてディープニューラルネットワークに入力して学習し、AIが次のアクションを決定することも可能になります。
深層強化学習は、ゲームだけでなく、エネルギー需給バランス、物流の輸送経路やスケジューリング、生産や在庫の管理、製造装置の制御など、様々な最適化や制御の高度化への適用が期待されます。東芝デジタルソリューションズでは、深層強化学習の応用研究を進め、産業分野を中心に、AIによるシステムの最適化・自律化の実現を目指します。深層学習による特徴抽出と強化学習による予測制御を組合せることで、ゲームAIやロボット制御などの複雑なシステムの制御ができるようになりました。本節では、深層学習が強化学習において果たす役割について考察します。
強化学習においては、環境についての知識は未知であるため（モデルフリー）、探索によって環境について情報収集する必要があります。例えば、囲碁やビデオゲームを習得する場合、囲碁の盤面の石の配置とか、ビデオゲーム画面のキャラクターの配置などから、ゲームの状況を把握して次の一手を決定しなければなりません。このような場合、盤面やゲーム画面などの2次元の画像情報から、ゲームの局面という高次の特徴量を抽出することが要求されます。車の自動運転においても、センサーが取得した画像情報から歩行者や障害物の特徴を把握して適切な操作をしなければなりません。さらに、囲碁の場合で考えると、盤面の特徴を把握して局面を理解できたとして、その後の戦略を何手先までも先読みして最適な一手を選択する必要があります。その場合、一連の行動とそのもたらす結果として、状態と報酬の系列を事前にシミュレーションする必要があります。つまり、状態・行動・報酬の系列データを逐次的に生成する仕組みが必要です。このように、強化学習の適用には、観測データからの特徴量抽出と、予測シミュレーションを可能とする系列データ生成とが必要となります。これら2つの重要な仕組みを提供するのが深層学習です。深層学習（Deep learning）は、ニューラルネットワークの層を多数積んだ深層ニューラルネットワーク（DeepNeural Network, DNN）とそれらを学習するための一連の技術体系をまとめたものです。観測データからの特徴量抽出器としては、畳み込みニューラルネットワーク（Convolutional Neural Network, CNN）が有効であることが知られています。これは、畳み込み演算と言われる演算操作により、一定の拡がりを持つ空間情報を集約して次の層に渡す処理を繰り返しながら、空間情報の特徴量を抽出する方法です。CNNでは、こうして得られた特徴量を利用して様々なタスクに適用することができます。例えば、画像分類問題に適用する場合、画像に対して被写体の分類ラベル（ネコ、イヌ、ヒト）が一意に紐づく画像セットを訓練データとして、画像から分類ラベルを予測するモデルを学習します（図1.9）。その際、CNNによる特徴量出力を全結合ニューラルネットワークからなるラベル予測モデルに渡して学習することで、人間を超える分類精度を達成することができました。系列データ生成器としては、再帰型ニューラルネットワーク（Recurrent Neural Network, RNN）が有効なモデルとして知られています。これは、ニューラルネットワークの層の情報を時間方向にも伝播することで系列データを学習できるようにしたものです。具体的には、ある層の情報を次の層に渡すだけでなく、次時点の同じ層にも再帰的に渡すことで系列データの特徴を把握します。また、こうして学習されたRNNを用いて、逐次的に系列要素を1個ずつ予測しながら系列データを生成することができます。この技術は、言語の自動翻訳に応用され、高い精度の翻訳を実現できるようになりました。こうした深層学習による特徴量抽出と系列データ生成を強化学習に適用して、これまで制御が難しいと考えられていたタスクを制御できるようにする試みが深層強化学習です。機械学習の一分野である深層強化学習により、ロボットや自律システムなどの複雑なシステムのコントローラーや意思決定システムを実装できます。深層強化学習では、シミュレーションや物理システムから動的に生成されたデータを使用して学習を行うことで、複雑な動作を学習できるディープ ニューラル ネットワークを実装することができます。他の機械学習技術とは異なり、ラベルがあるかないかにかかわらず、事前定義された学習データセットは必要ありません。一般的に必要なものは、対象の環境を表現するシミュレーション モデルだけです。深層強化学習エージェントは、入力としての状態から出力としてのアクションへのマッピングを実行するディープ ニューラル ネットワーク方策と、該当する方策を更新するアルゴリズムで構成されます。広く利用されているアルゴリズムは、Deep Q-Network (DQN)、Deep Deterministic Policy Gradient (DDPG)、Soft Actor Critic (SAC)、Proximal Policy Optimization (PPO) などです。アルゴリズムは、環境から収集した観測値と報酬に基づいて方策を更新し、期待される長期的な報酬を最大化します。深層強化学習アルゴリズムを用いた学習は、エージェントが周囲の環境とやりとりする動的なプロセスです。ロボティクスや自律システムのなどのアプリケーションの場合、実際のハードウェアを使用してこの学習を行うことは費用がかかるうえに危険を伴います。そのため、深層強化学習では、シミュレーションを通じてデータを生成する環境の仮想モデルが推奨されています。倒立振子のバランス制御、迷路の探索、カートポールでのバランス制御などの簡単な問題について方策の学習を行うことで、深層強化学習を開始します。自律走行車向けに、アダプティブ クルーズ コントロール (ACC) および車線維持支援用のシステムを設計することもできます。また、深層強化学習は、軌道計画などのロボティクス アプリケーションや、歩行などの動作指導にも利用できます。強化学習とディープラーニングの融合は、旧来の強化学習に大きな技術的進展をもたらし、社会・ビジネスへの活用が大幅に進む契機となりました。深層強化学習アルゴリズムを使用してタスクを解決するためのポリシーをトレーニングするためのさまざまな手法が存在し、それぞれに独自の利点があります。最高レベルでは、モデルベースの強化学習とモデルフリーの強化学習が区別されます。これは、アルゴリズムが環境ダイナミクスのフォワードモデルを学習しようとするかどうかを示します。モデルベースの深層強化学習アルゴリズムでは、通常、ニューラルネットワークを使用した教師あり学習によって、環境ダイナミクスのフォワードモデルが推定されます。次に、学習したモデルを使用したモデル予測制御を使用してアクションを取得します。真の環境ダイナミクスは通常、学習したダイナミクスとは異なるため、エージェントは環境内でアクションを実行するときに頻繁に再計画します。選択されたアクションは、クロスエントロピー法などのモンテカルロ法、またはモデル学習とモデルフリー法の組み合わせを使用して最適化できます。強化学習とは、機械学習のアルゴリズムのひとつであり、「システム自身が試行錯誤を繰り返して最適なシステム制御を実現していく仕組み」のことを指します。機械学習には、教師あり学習や教師なし学習のように、明確なデータをもとにした学習方法も存在しますが、強化学習の場合は明確なデータをもとにするわけではありません。プログラム自体が与えられた環境の観測を行い、一連の行動結果を踏まえた上で、より価値のある行動を学習していくという仕組みです。そして、その行動についての評価も自ら更新していきます。さまざまな行動を試しながら、より価値のある行動を探していくという点を踏まえると、人間の動作に近いものといえるかもしれません。
そんな強化学習ですが、この概念自体は近年のAIブームよりも前から存在していました。強化学習の原型といえるものは、機械の自律的制御を実現する「最適制御」の研究が行われていた1950年代から存在していたのです。なお、1990年代には強化学習の生みの親とされるリチャード・サットン教授（カナダ・アルバータ大学）を中心としたチームにより研究が進められていたといいます。このように、強化学習の原型といえるものは古くから存在していたわけですが、そこに飛躍的な進歩をもたらしたのが「深層強化学習」というものです。これは、従来の強化学習に深層学習（ディープラーニング）を応用したものであり、強化学習を軸として稼働するAIが世間に広まるきっかけとなりました。強化学習には、Q-Learning、SARSA、モンテカルロ法という3つのアルゴリズムが存在します。3つの手法の中で一番多く用いられているのが、Q-Learning（Q学習）です。強化学習について勉強していく際は、まずQ-Learningから学ぶことになるでしょう。Q-Learningは、Q関数という行動価値関数を学習し、制御を行っていく仕組みです。行動価値関数Q(a|s)は、状態s(t)において行動aを行った場合、その先の報酬はどれくらいもらえるかの予想を出力していきます。Q関数に行動「右に押す」と「左に押す」を入力した場合の出力を比較し、より報酬が多いほうを選択すると、CartPoleが立ち続けることになるわけです。SARSAは、Q関数を学習するという点ではQ-Learningと同じですが、学習の仕方に違いがあります。「実際に行動した結果」を用いて、期待値の見積もりを置き換えていくのが特徴です。そのため、現在の価値を更新するには、再度エージェントが行動を行わなくてはなりません。モンテカルロ法は、Q値の更新において「次の時点のQ値」を使用しないという点が特徴です。何かしらの報酬が得られるまで行動を行い、報酬値を把握します。そして、辿ってきた状態・行動に対して、報酬を分配するという仕組みです。ディープラーニングの発達に伴い、現在は強化学習においてもディープラーニングが積極的に活用され始めています。ディープラーニングの発達によって登場したのが深層強化学習と呼ばれる技術です。これまでは、Q関数を表すために表を使用するのが一般的となっていました。表のサイズは「状態sを離散化した数」×「行動の種類」という計算によってに決まるため、限りがあります。
しかし、ディープラーニングを用いたDQN(Deep Q-Network 、Deep Q-Learning Network)が実用化されたことで、これまで以上に複雑なゲームや制御問題を解決できるようになったのです。このディープラーニングを活用した強化学習のことを「深層強化学習」と呼びます。自動車における自動運転も、強化学習が活用されている分野のひとつです。Prefferd Networks社という日本の企業が行っている研究では、自動車の幅に対して道の幅が狭く、車が密集している交差点のような難しい状況下において、強化学習でどれだけ運転の精度を高められるかという実験が行われています。この技術を用いることにより、前後左右のすべての方向を集中してみることが可能になるため、前方向と同じように後方向にも躊躇なく移動することができるそうです。近年は高層ビルが増加していることもあり、エレベーターの制御は極めて重要な役割を担っています。ただし、エレベーターの安全性を高めるのはもちろんのこと、エレベーターの利便性を高めることも、良いエレベーター制御システムの条件のひとつです。そのため、客の待ち時間が長くなってしまうエレベーターの制御システムは高く評価できません。特に、デパートやオフィスビル、タワーマンションといった、毎日大勢の人々が乗り降りする場所には欠かせない条件といえるでしょう。エレベーターは簡単に増設することもできませんから、台数と定員を変えずに待ち時間を短くする必要があるのです。
そこで活用されるのが、強化学習です。数理的な手法で割り当てを行うにしても限界があるため、強化学習によって過去の経験にもとづいた最適な選択肢を選ぶことで、より柔軟にエレベーターを稼働させることが可能になります。もちろん、日々の運行履歴も学習していくため、新たな学習データを追加した上で、より最適な判断方法にアップデートしていくことも可能なのです。
そのため、エレベーターの待ち時間が長くなってしまっている建物などは、特に強化学習を導入するメリットが大きいといえるでしょう。強化学習の代表的な活用事例として挙げられるのが、2016年に登場した囲碁AIの「AlphaGo」です。プロ棋士に勝利をしたことで大きな話題を呼びました。そんなAlphaGoには、Googleが開発したディープラーニングと強化学習を組み合わせた深層強化学習が活用されています。囲碁だけでなくさまざまなゲームにおいて圧倒的な強さを発揮しており、もはや深層強化学習の知名度を高めるきっかけとなった存在と言っても過言ではありません。AlphaGoには、「モンテカルロ木探索」と呼ばれる探索型AIが搭載されています。このAIは、「統計的に勝つ確率の高い一手」を算出することを目的としたもので、囲碁のように「明らかな正解が存在するケース」に対応可能です。ただし、モンテカルロ木探索は、ある程度盤面の選択肢が絞り込まれた状況でなければ使用することができません。そのため、「盤面評価」と「戦術予測」を実行する「深層強化学習を搭載したAI」を活用し、分析を進めていくわけです。そんな優れたAIを搭載するAlphaGoですが、現在は進化版として「AlphaZero」というものも登場しています。この「AlphaZero」は、AlphaGOを破ることにも成功しており、チェスや将棋といった別の種目にも対応していることから大きな注目を集めているのです。今後どのレベルまで成長を続けるのか、期待を寄せられています。レコメンドサービスやレビュー管理サービスなどを提供している「ナビプラス」では、自動最適化機能に強化学習を活用しています。ナビプラスが提供するサービスの一つに「NaviPlusレコメンド」というものがあります。このサービスは、Webサイトのパーソナライゼーション強化を支援することが目的です。
そのWebサイトにとって最適なレコメンドを実現するためには、一連の行動結果を踏まえた上で、より価値のある行動を学習していく仕組みが欠かせません。その仕組みを実現することができるのが、強化学習なのです。
たとえば広告を表示させるとき、「AとBどちらが高いクリック率を実現できるか」という点においては、初めから正解が存在するわけではありません。実際に試しながらデータを収集していく必要があります。
「実際に試して得られる報酬を最大化するための戦略」を練る上で、強化学習は非常に重要な役割を担っているのです。コンテンツのレコメンドにも、強化学習は活用されています。例えばNetflixでは、「流行」「視聴率」「ストーリー性」「離脱率」といったさまざまなデータをAIに学習させ、ユーザーごとに最適なコンテンツをレコメンドする仕組みとなっているのです。そのため、「ユーザーが満足しそうなコンテンツ」が優先的に表示されるようになり、より満足度を高めやすい環境が実現されています。Netflixではオリジナル作品の制作にも力を注いでいるため、今後データが蓄積されることによって、よりユーザーに適した作品が増加していくことも期待できるでしょう。
近年、いわゆるAI を構成する要素技術として機械学習の発展が著しい。とくにディープラーニングはその火付け役であり、画像分類、物体検出、セグメンテーションなどの画像領域をはじめ、自然言語処理、音声認識といった分野にまで広く応用されている。その表現力の高さから、今や従来の機械学習手法を凌ぐ結果を見せている。ディープラーニングの技術は日進月歩で進化しており、新たな研究が発表されると、すぐに実装コードが公開されたり、応用研究が進められたり、ビジネスに適用されたりする。なかでも最近注目されている技術の1つに、「敵対的生成ネットワーク」（Genera tive Adversarial Networks。以下、GAN）がある。GANは生成モデルの一種であり、データから特徴を学習することで、実在しないデータを生成したり、存在するデータの特徴に沿って変換できる。GANは、正解データを与えることなく特徴を学習する「教師なし学習」の一手法として注目されている。そのアーキテクチャの柔軟性から、アイデア次第で広範な領域に摘用できる。応用研究や理論的研究も急速に進んでおり、今後の発展が大いに期待されている。学習データから特徴を獲得し、画像の合成や画風の変換、特徴の付与などが可能である。まだビジネス面での応用例はまだ少ないが、GANを利用した自動着色アプリや画像を自動生成するAPIが公開されたり、 GANを実装したソフトウェアが登場している。後述するように、GANが本物と偽物を見分けるように学習する特徴に着目した応用研究も着々と進んでいる。GANは、イアン・グッドフェローらが2014年に発表した論文で、2つのネットワークを競わせながら学習させるアーキテクチャとして提案された。この斬新な構成により、従来の生成モデルより鮮明で本物らしい画像生成が可能になった。さらに2015年には、畳み込みニューラルネットワーク（CNN）で見られるような畳み込み層をネットワークに適用したDCGAN（Deep Convolutional GAN、が提案された。後述のように、GANは学習時に不安定になるケースが見られ、意味をなさない画像を生成したり、生成データの種類が偏るなどの課題があった。これに対してDCGANはより鮮明な画像生成を行うとともに、学習を安定化させる手法を提案したため、GANブームの火付け役となった。GANは生成モデルであり、データの特徴を抽出して学習し、実在しないデータを生成できる。生成モデルに分類される手法としては、変分オートエンコーダやボルツマンマシンなども以前からあるが、GANはそれらの手法と比べてより鮮明な画像の生成が可能である。また、GANは教師なし（ラベルなし）学習を基本とする。後述するConditional GANのように、学習と同時にデータにラベルを与えるケースもあるが、基本的には用意したデータにラベリングすることなく使用できるので、教師データを作成する手間が省ける（手法によって異なるので確認が必要である）。GANは、2つのニューラルネットワークで構成される。1つはGeneratorであり、その名のとおりデータを生成する。Generatorは、生成データの特徴の種に相当するランダムノイズを入力することで、このノイズを所望のデータに近づけるようにマッピングする。もう1つはDiscriminatorであり、Generatorが生成した偽物のデータと本物のデータが与えられ、その真偽を判定する。この2つのネットワークを交互に競合させ、学習を進めることで、Generatorは本物のデータに近い偽物データを生成できるようになる。この関係性は、しばしば紙幣の偽造に例えられる。偽造者（Generator）は本物に近い偽札を作ろうとし、警官（Discriminator）はそれが偽物であると見抜く。するとGeneratorは、より精巧な偽札を作り出すように技術を発展させる。こうした「いたちごっこ」が繰り返され、最終的には本物に近い偽札が生成されるようになる。2つのネットワークの競合関係は、ロス（コスト）関数を共有させることで表現される。すなわち片方のロスが小さくなれば、もう一方にとってはそのロスが大きくなる。Generatorはロス関数の値を小さくすることを目的に、Discriminatorはロス関数の値を大きくすることを目的に学習させる。ここが、CNNや再帰型ネットワーク（RNN）などロスの最小化を目指す他のアーキテクチャと異なる点であり、GANがぼやけにくい画像を生成するポイントである。ただし後述するように、これが学習を難しくさせている点でもある。また学習させたGANから類似データを生成する場合、Generatorにはランダムノイズを入力するが、ここにランダム性をもたせることで、生成されるデータにもランダム性が生まれる。すなわち、サンプルするたびに異なる類似データが生成されることになる。GANを扱う場合に、留意すべき点が2つある。1点目は学習の不安定さ、つまり生成されるデータに偏りが生じることである。これはmode colla pseと呼ばれる現象であり、GANが備える構成の複雑さから生じる。発生の予見は難しく、回避するにはひたすら試行錯誤を重ね、パラメータやネットワーク構成の見直しが必要となる。2点目は、生成モデル全般に共通する点であるが、生成データの評価が難しいことである。分類問題や将来値の予測が対象であれば、定量的にその学習結果を評価することは可能である。しかし生成モデルの場合は、生成された類似データを定量的なメトリックを用いて評価すること（何をもって「よい類似」とみなすか）は難しいので、学習データに精通した者が、実際に生成された画像を見て学習結果を評価することが望ましい。通常のGANのデータ生成は、ランダムにサンプルされるので、生成されるデータの種類を制御できない。Conditional GANは学習時にラベルを与えることで、推論時にもラベルを入力してデータを生成できる。SRGANは低解像度の画像を高解像度に復元する、超解像を目的としたGANである。Bicubic法のような従来の超解像手法による復元では、ぼやけた画像になりやすい。SRGANでは、GANの特性を利用することで、ぼやけの少ない画像の復元が可能である。物体検出やセグメンテーション、異常検知などへの改善手法としてGANを利用する例、さらにテキストや音声、音楽、動画、3次元データ、医療データを対象にした拡張研究も見受けられる。また学習時の安定性の向上や理論的背景を探る研究など、多種多様な研究が次々と公開されており、今後も目が離せない。GANではオープンに、さまざまな研究や適用の試みがなされている。この手法が初めて登場してから、研究が急速に進展していることを考えると、生成モデルとしての注目度は非常に高く、今後もさらなる発展が期待される。GANは生成ネットワーク（generator）と識別ネットワーク（discriminator）の2つのネットワークから構成される。例として画像生成を目的とするなら生成側がイメージを出力し、識別側がその正否を判定する。生成側は識別側を欺こうと学習し、識別側はより正確に識別しようと学習する。このように2つのネットワークが相反した目的のもとに学習する様が敵対的と呼称される所以である。GANの学習は不安定で膨大な量のデータを必要とする。その安定性を高めまたデータ量を補い汎化性能を高めるために、GANにおいてもデータ拡張が利用される。ただしDiscriminator/Critic側のみにデータ拡張を行うとむしろ性能が低下することが知られている。これはDがデータ拡張の有無を判別に用いてしまい、それを騙すためにGがデータ拡張を模倣した出力を生成してしまうためと考えられている。これを避けるため、Generator出力にもデータ拡張を行うことで有効なデータ拡張が可能になる。GANとは、Generative Adversarial Network（敵対的生成ネットワーク）と呼ばれるAIの一種です。GANはデータから特徴を学習することで、実在しないデータを生成したり、存在するデータの特徴に沿って変換できます。GANのネットワーク構造は、Generator（生成ネットワーク）とDiscriminator（識別ネットワーク）の２つのネットワークから構成されており、互いに競い合わせることで精度を高めていきます。例えるならば、「偽物を作り出す悪人（Generator）」と「本物かどうか見破る鑑定士（Discriminator）」のような役割をネットワーク内に組み込み、競争させるような形で学習させます。一番の特徴は、教師なし（データのラベリングが不要）で学習が可能になることです。
従来の深層学習（ディープラーニング）では、データのラベリングが必須でしたが、GANではその必要がなくなります。一方、ラベリングが無いことによって学習が不安定になるので、学習を安定させる工夫を施したGANも存在します。GANを使うことで、品質の低い画像から高品質の画像を作ることが出来ます。ピンボケした写真や昔の写真の解像度を上げるなどといった活用法があります。また、複数の顔の画像を取り入れることで、実際には存在しない人の画像を作ることも出来ます。絵の特徴を書いた文章から画像を生成することが出来ます。音声による画像の修正というように、様々な場面での活用が期待されています。1つの画像から雰囲気の異なる別の画像を作ることが出来ます。ただの風景の画像からイラスト風の画像に変換したり、印象派の作品のようなテイストの画像に変換したりすることが出来ます。近年フェイクニュース等で話題になっている技術ディープフェイクにもGANが使われています。ある人の動画に別の人の顔を載せたり、別人の２人の動きをシンクロさせたりすることが出来ます。GANは画期的な技術ですが、オリジナルGANは動作が不安定になりやすいといった欠点があります。そのためそれらを解決するために様々な種類のGANが開発されています。最近白黒写真がカラー写真になったり、この世に存在しない人の顔が作られたりしています。これらの技術は画像に特化して開発されたわけではなく、元はデータを増やす・質を上げる仕組みとして開発されたものが画像に応用されて実現可能となりました。その技術が「GAN（敵対的生成ネットワーク）」です。GAN（Generative Adversarial Network）は、2014年にイアン・グッドフェローらが「Generative Adversarial Nets」という論文で発表したアーキテクチャ（論理的構造）です。
2つのニューラルネットワークを互いに競わせて入力データの学習を深めていくことから、敵対的生成ネットワークとも呼ばれています。コンピューターに大量のデータを学習させ、分類・予測などの動作を遂行するモデルやアルゴリズムを自動的に構築する技術を機械学習と言いますが、機械学習には主に2種類の分類・予測モデルがあります。1つは画像認識などの識別モデル、入力されたデータや画像を何であるかコンピューターに判断させるモデルです。そしてもう1つが生成モデル、入力されたデータや画像から新しい擬似データを生成するモデルです。GANはこの生成モデルに該当します。GANの用途としては画像生成が有名ですが、データを生成するという点でディープラーニングを補う技術としても注目度が高いです。従来の、サンプル画像を傾けたり、色を変えたりしてデータを増やすというやり方ではなく、特徴を含んだデータを新たに作り出すことで、データ不足が課題になりがちなディープラーニングに応用できます。この「オリジナルの特徴を含んだデータを増やす」というやり方を応用することで、新しい画像を生成したり、低画質の画像を変換して高画質にしたり、音声を生成したりするというアプローチが可能になりました。機械学習には2つのモデルがあります。その1つは入力したデータ画像が何かをコンピュータに判断させるモデルです。
そしてもう1つが生成モデルといわれる、入力したデータから新しく擬似データを作るモデルです。GANはその生成モデルの1つで入力されたデータや情報の特徴を学習していくことで、新たに実在しないデータや情報を作ったり、その特徴に近づけて作り出します。GAN（ガン）の使いみちとしては画像生成が特に有名です。
データや情報を生成するということで、ディープラーニング（深層学習）を補うことができるのです。
今までは、データや情報を増やすために、サンプル画像を用意して色を変えたり傾けたりしてきました。しかし、GAN（ガン）の技術によって、新たに特徴を持ったデータが作り出せるので、データ不足に陥りがちなディープラーニングにも応用することができるのです。「独自の特徴を持ったデータを増やす」ということから、音声を生成したり、新しい画像を作ったり、低解像度の画像を変換して高解像度にしたりすることにも期待が持てるようになりました。GAN（ガン）は「敵対的」生成ネットワークといわれるように、競い合う仕組みになっています。
Generator（ジェネレーター）とDiscriminator（ディスクリミネイター）という2つのネットワークがあり、最終的には本物とほとんど変わらないデータ・情報を作成するために、いたちごっこのように競り合うのです。
ジェネレーターは、本物そっくりに描こうとする贋作師、ディスクリミネイターはそれを判別する鑑定士に例えてみましょう。
贋作師のジェネレーターは、元にした絵画（入力データ）から特徴を捉えて似せた「贋作」を作成し、鑑定士であるディスクリミネイターは、ジェネレーターの偽物を「本物の絵画」と比較することで判別します。このように、生成するネットワークと識別するネットワークの相互作用によって、少しずつデータは本物に近いものになっていきます。初期の段階では、一目見ただけで偽物とわかるような質の低い絵画を作っているので、すぐに違いを見破られてしまいます。
すると贋作師は、前よりも似せたものを作り出して鑑定士をだまそうとします。
徐々に繰り返すうちに、本物にそっくりなものを作成してくるので、鑑定士も判断するのが難しくなってくるのです。
最後には鑑定士が本物だと判断する偽物を作り出すことが目標となっているのです。ジェネレーターとディスクリミネイターが競い合うことで、最終的には本物と見間違えるほどのレベルまで生成データを持っていきます。
これからはGANで生成された画像は画質だけでなく、本物と相違のないレベルの高い画像が生成されるでしょう。GANのモデルには「生成器」「識別器」の2つのネットワークがあります。それぞれに役割があり、生成器は本物と同じようなモノを作り出し、識別器は生成器が作ったモノが本物かニセモノかを判断します。生成器の精度が上がれば、識別器の精度も上がっていくという、お互いに切磋琢磨してGANのモデルが成長していきます。
GANを利用して、高品質な画像を作成したり、画像から文章を作ったり(この逆もあり)、ある画像を雰囲気の違う画像にしたり、動画内の人物を別の人物に変えて動画にしたりできます。GANは、Generative Adversarial Networksの略で、AIのアルゴリズムの一つです。現在、このGANが異常検知の領域に対して応用されることで、品質管理に役立つ画期的なAI技術として注目されているのです。 敵対的生成ネットワークと呼ばれているGANは一般的にAIのアルゴリズムの一種であると捉えられており、2014年にイアン・グッドフェロー博士によって導入された機械学習システムです。GANは敵対的生成ネットワーク、あるいは競争式生成ネットワークと呼ばれています。これは機械学習におけるAIのアルゴリズムの一種であり、教師データなしで学習することができるアルゴリズムとして注目されています。 従来の機械学習では人間が人的コストを払って、ラベル付けした訓練用データを用いることで特徴やパターンを理解して学習を行っていました。しかしこれではAIの学習のためにラベル付けした膨大な訓練用のデータが必要になってしまいます。そこで、グッドフェロー博士は2つのニューラルネットワークを協力させることで、この問題を解決させる方法を思いつきました。 ラベル付けされたデータ、つまり人間による訓練用データがない、教師がいない状態での学習を行うことを実現させたのです。GANを利用することで非常に良好な結果をもたらし、実際に教師なしでの学習を実現しました。 GANは2つのニューラルネットワークを敵対させ、互いに競わせて学習を深めていきます。この動作メカニズムから、GANは敵対的生成ネットワークと呼ばれるようになりました。 実際には、一つのネットワークのことを生成ネットワーク、もう一方は識別ネットワークと解釈されており、生成ネットワークの役割はデータセットについて学習を行い、訓練用データを作成するのが役割です。識別ネットワークの役割は、生成ネットワークが作成した訓練用のデータを本物か偽物か判別させることです。 最初の段階では学習が浅いため、すぐに偽物と判断されます。その結果をもとに、生成ネットワークは識別ネットワークが偽物と識別できないデータを作るように試みます。このプロセスが繰り返されることで、双方のネットワークが学習するというのがGANのメカニズムとなっています。 結果的にグッドフェロー博士の想定通り、ニューラルネットワーク同士を敵対させるGANのアプローチによって、ラベル付けされていないデータによる学習を大幅に改善することにつながりました。例えば、収集したイメージの特性を学習することによって、イメージの解像度の向上につなげることや、本物と遜色ない模造写真を作成することが可能になっています。GANが画期的とされるのは、今までの機械学習に必要だった訓練用のデータの準備が必要なくなることです。特に2つ生成したニューラルネットワークがお互いの存在のみで「鍛え合う」ことで自動的に学習するというところに特徴があります。2つの生成されたニューラルネットワークと、収集されたラベル付けしていないデータが膨大に存在していれば自動的に効率の良い機械学習が行われるということです。
従来の機械学習に必要とされた教師ありの概念を覆す画期的な特徴であるといえるでしょう。 GANを利用した、異常検知の領域への応用が近年試みられています。AnoGANをはじめとした、様々なGANの応用によって、異常検知の手法が高度化し、より一層GANの利用頻度は高まってきているのです。AnoGANとはAnomaly Detection with Generative Adversarial Networksの略で英訳の通り、GANを用いて異常検知をするという意味となっています。最初の異常検知の試みとしてAnoGANは認知されているのです。AnoGANのメカニズムは非常にシンプルで、GANには正常な画像だけ絞って十分に学習を行わせます。これによって識別ネットワークは「正常な画像」が判断できるようになり、「正常な画像ではないもの」＝「正常ではない画像」という識別も可能になります。また、異常がある画像を与えられることで、正常か異常かを高度に識別できるようになるため、異常検知が実現されます。 その結果、従来の精密でリアルな画像を作り出すことが主流と考えられていたGANの役割から、画像が異常なのかどうかの判別ができるようになるAnoGANというGANによる異常検知をはじめて提案したとされています。 この背景には、医療現場などが関わっています。例えば特例の病理など、正常な画像はあっても異常な画像のデータセットが少ないパターンが存在しています。最適な学習を行おうと思ってもパターンがなければ検知はできません。そのため正常な画像を学習することで、異常があれば検知するという逆転の発想が生まれたといえるでしょう。ディープラーニングの技術は日進月歩で進化しており、新たな研究が発表されると、すぐに実装コードが公開されたり、応用研究が進められたり、ビジネスに適用されたりします。なかでも最近注目されている技術の1つが、「敵対的生成ネットワーク」（Generative Adversarial Networks。以下、GAN）です。GANは生成モデルの一種であり、データから特徴を学習することで、実在しないデータを生成したり、存在するデータの特徴に沿って変換できます。GANはそのアーキテクチャの柔軟性から、アイデア次第で広範な領域に摘用できます。応用研究や理論的研究も急速に進んでおり、今後の発展が大いに期待されています。現在、GANは文化の領域でもちょっとした話題になっています。AIによって制作された芸術作品として初めて競売された作品に関わっていたり、「ディープフェイク」と呼ばれる一連のフェイク画像や動画の背後にも存在したりしています。GANは物体検出やセグメンテーション、異常検知などへの改善手法としても利用できます。さらに、テキストや音声、音楽、動画、3次元データ、医療データを対象にしたGANの拡張研究も見受けられます。また「本物に近い画像の生成ができる」ということは、他のディープラーニングの学習用のデータセットとして使えるケースもあるということです。実際、サンプル数が限られている医療画像データを生成して医療トレーニングに使う、などの用途も発表されています。
少ないデータセットでもパワフルに動作するといった点から、今後もGANはディープラーニング方面でよく使われていくのではないでしょうか？実際、歴史はまだ浅いともいえ、どんどん早いスピードで発展しており、優れた論文が毎週のように出たりしています。もっとも期待されるのが、クリエイティブ分野での活用でしょう。
ビジネス面での実用例はまだ少ないですが、GANを利用した自動着色アプリや、画像を自動生成するAPIが公開されたりなどなど、GANを実装したソフトウェアが登場しはじめています。デザイナーや流行のファッションスタイルを学んだGANが、新しいデザインを創り出す、ということもすでに実用化が進んでいます。
およその絵だけ描けば、残りを人工知能が完成させてくれるというような、GANを使ったデザインソリューションがすでに運用されています。
また、アニメやゲームなどのエンタメ分野でも実用化に向けて動きはじめています。2018年、アニメーション画像形成技術「PSGAN」がDeNAにより開発されましたが、現在のレベルで、商用のアニメーションでも使用できるレベルまではきているとのこと。DeNAは、この技術を利用してのゲーム制作や、アニメーション技術の他社への提供を進めています。いわゆるアーティスト・芸術家と呼ばれる世界にもGANが入ってくるかもしれません。たとえば、米ニュージャージー州ラトガース大学の「アート＆人工知能研究所（Art＆Artificial Intelligence Lab）」は、ディープニューラルネットワーク技術を活用し、画家のように絵を描くアルゴリズムを開発。実際にアート作品を制作し披露しています。肖像画、風景画、宗教画など多様なジャンルが含まれているとか。その他にも、囲碁や将棋の世界でトップ棋士たちがAIを使って新手を発見したり自己研鑽したりしているように、絵画や書道、彫刻などの美術家をはじめ、小説や詩、映画制作など、さまざまなアート・クリエイティブの世界にGANが利用される可能性があります。
MLPは一般には3層のニューラルネットワークのことで、深層学習の深層でないバージョンと考えることのできるモデルです。
パーセプトロンは単純パーセプトロンとも呼ばれ、人間の脳の神経細胞を模して考えられたアルゴリズムです。パーセプトロンは2層のニューラルネットワークと考えることができ、複数の入力からスカラーの出力を得るモデルです。しかし一度の線形変換のみから構成されるこのモデルでは線形分離不可能なデータをうまく識別することができず、うまく識別できるデータは限られていました。
ニューラルネットワークは脳の神経細胞を模して作られた機械学習モデルの総称であり、単純パーセプトロン、MLP、深層学習のいずれもニューラルネットワークと呼ばれます。
深層学習は多層（4層以上）から成るニューラルネットワークのことで、各層での線形変換に加え別途活性化関数と呼ばれる非線形な変換を行うことで、単純パーセプトロンで表現しきれない線形分離できないデータに対してもよくフィットするモデルです。現在の人工知能ブームを牽引しているのがこの深層学習であり、画像認識の分野で広く成功を収めている畳み込みニューラルネットワーク、自然言語処理の分野で成功を収めていた（現在はAttentionベースの手法が多いが）再帰ニューラルネットワークなどの手法が様々提案されています。
MLPとは一般には3層から成るニューラルネットワークのことであり、2回の線形変換とそれぞれに対応する活性化関数で構成されます。MLPはパーセプトロンの欠点であった線形分離不可能な問題に対応するために、一度線形変換を行った後にsigmoid関数と呼ばれる非線形関数を適用しています。
しかしMLPには層を何層も重ねると勾配消失問題によって学習がうまく進まなくなるという問題がありました。勾配消失問題とは誤差逆伝播で学習を行う際に、途中で勾配の値が小さくなりすぎることで手前の層の学習がうまく行えなくなる現象のことです。深層学習ではReLUと呼ばれる活性化関数やその他学習のテクニックを用いることでこの問題を解決し、さらなる進化を遂げました。
機械学習のアルゴリズムの１つにニューラルネットワークがあります。このニューラルネットワークの基本になっているのが、人間の脳を模した構造を数理モデル化した「パーセプトロン」です。人間の神経細胞のように細胞と細胞を多層に組み合わせたモデルを多層パーセプトロン（MLP）と呼んでいます。
AI（人工知能）は、Artificial　Intelligenceの略で、一般的には、「コンピュータ上に人間のような知能を再現する技術」のことです。もともと1956年夏にアメリカのダートマス大学で開催されたダートマス会議において、初めて初めて人間のように考える機械のことを「AI（人工知能）」と呼ぶようになりました。
AI（人工知能）には、3回のブームがあります。第1次AIブームは、1950年代〜1960年代で、コンピュータに「探索・推論」させることによって、問題を解かせる研究が中心でした。第2次AIブームは、1980年代。コンピュータに「知識」を入れるアプローチでの研究が盛んに行われました。そして、現在、ブームの真っただ中である、第３次AIブームではデータからコンピュータが特徴量を抽出し、予測や回帰を行うディープラーニングがブームを牽引しています。
ディープラーニングは、ニューラルネットワークの一種です。ニューラルネットワークとは、脳機能に見られるいくつかの特性に類似した数理的モデルのことで、分類や回帰などで非常に高い精度で予測できることが期待されます。
人間の脳は、神経細胞（ニューロン）と神経回路網（シナプス）で構成されておいます。神経細胞（ニューロン）は電気信号として情報を伝達し、神経細胞（ニューロン）と神経細胞（ニューロン）をつなぐ神経回路網（シナプス）のつながりの強さによって、情報の伝わりやすさが変わります。この構造を模して、ニューラルネットワークでは、神経細胞（ニューロン）にあたるものを「ノード」、神経回路網（シナプス）にあたるものを「エッジ」として呼び、神経回路網（シナプス）のつながりの強さを「重み」として表現をしています。
ニューラルネットワークは、複数のノードとエッジが2層以上につながってネットワーク上の構造のことで、入力データと出力データの誤差が最も小さくなるように重みを調整して、学習を行います。
また複数のノードが多層につながっており、ネットワーク上になっているものをディープニューラルネットワークと呼びます。ディープニューラルネットワークによって、複雑な情報に対応できるようになりました。
そのため、現在のディープラーニングは、画像認識、音声認識、自然言語処理や異常検知など様々な分野に応用されています。
多層パーセプトロン（MLP）は、ニューラルネットワークの1つで少なくとも３つのノードの層から構成されます。多層パーセプトロンの特徴は、複雑な問題を解ける点です。
多層パーセプトロン（MLP）の学習では、バックプロパゲージョン（誤差逆伝播法）と呼ばれる学習方法を用いて行います。これは、入力データに対して、最初に重みを設定して、出力データを作ります。その出力データに対して、入力データを与え、入力データとの誤差が最小になるように各重みの値を少しだけ増減させ調整をします。それを繰り返すことによって、データをうまく分類できるようにします。これにより、あらゆるデータに対して機械学習を行えるようになりました。
多層パーセプトロン（MLP）では、層を重ねると複雑な処理ができる一方で、誤差伝播法においては、層を重ねすぎると誤差が入力層に伝えられるときには非常に小さくなってしまって、精度が逆に下がってしまうという勾配消失という欠点があります。誤差伝播法とは、出力と正解データとの誤差を入力とは逆方向にフィードバクをすることで、学習の訓練データに適合過ぎて汎用性が失われてしまう過学習という状態に陥りやすい傾向があります。
多層パーセプトロン(MLP)と単純パーセプトロンの違いとは、パーセプトロンが多層かどうかの差です。単純パーセプトロンは、複数の入力に対して出力は単一になるパーセプトロンのこと。つまり、多層パーセプトロン(MLP)にあった中間層がありません。そのため、単純パーセプトロンは０か１のどちらかの出力しか表現できません。
例えば分類問題を解くときは、線形の分類しか対応できません。一方多層パーセプトロン(MLP)は、多層な構造で出力も様々な形で表現ができるため非線形の分類問題にも対応できます。
そのため、例えば線形の分類のような単純な分類などは単純パーセプトロン、それ以外の非線形の分類のような複雑な事項の処理は多層パーセプトロンを使います。
ディープラーニングと多層パーセプトロン（MLP）の関係とは、多層パーセプトロン（MLP）の欠点を解決したものがディープラーニングです。
多層パーセプトロン（MLP）は、層が多層になればなるほど、誤差が入力層に伝わりにくく、精度が落ちてしまう欠点や学習データに適合しすぎて汎用性が失われてしまいます。しかし、ディープラーニングを行うためには、ニューラルネットワークを多層にする必要があります。そうすると、誤差が消失したり、過学習を起こしてしまいます。
そこで多層にしても学習がうまくいくようにブレイクスルーを起こしたのが、オートエンコーダ。オートエンコーダとは、入力データを中間層で次元を圧縮することです。オートエンコーダの登場により、多層パーセプトロン（MLP）の層を多層にした際も、誤差をうまく調整でき、過学習を防ぐということを実現できました。
オートエンコーダが登場して、多層パーセプトロンからさらに進化したディープラーニングが登場しました。
AI（人工知能）は、一般的には、コンピュータ上に人間のような知能を再現する技術である
ディープラーニングとは、人間が行うタスクをコンピュータに学習させる機械学習の手法のひとつである
多層パーセプトロン（MLP）は、機械学習の一種であるニューラルネットワークの1つである
多層パーセプトロン（MLP）と単純パーセプトロンの違いは、複雑な出力が表現できるかどうかである
多層パーセプトロン（MLP）とディープラーニングの関係性としては、多層パーセプトロンの欠点を解決したものがディープラーニングである
多層パーセプトロン（MLP）が使えると、ニューラルネットワークの基礎を習得できます。この記事で多層パーセプトロン（MLP）について理解して、ディープラーニングの実装に役立てましょう。
多層パーセプトロンは複数の形式ニューロンが多層に接続されたネットワークを指します。
単純パーセプトロンは入力層と出力層のみであったのに対し、多層パーセプトロンは中間層(隠れ層)と呼ばれる、層が複数追加されたネットワーク構造を持ちます。
単純パーセプトロンと違い複数のクラス分類を可能とし、線形分離不可能な問題も解くことができます。
現在は、この多層パーセプトロンの形式を拡張したものがよく使われています。
多層パーセプトロン（たそうパーセプトロン、英: Multilayer perceptron、略称: MLP）は、順伝播型（英語版）ニューラルネットワークの一分類である。MLPは少なくとも3つのノードの層からなる。入力ノードを除けば、個々のノードは非線形活性化関数を使用するニューロンである。MLPは学習のために誤差逆伝播法（バックプロパゲーション）と呼ばれる教師あり学習手法を利用する。その多層構造と非線形活性化関数が、MLPと線形パーセプトロンを区別している。MLPは線形分離可能ではないデータを識別できる。
多層パーセプトロンは時折、特に単一の隠れ層を持つ時、「バニラ」ニューラルネットワークと口語的に呼ばれることがある。
多層パーセプトロンが全てのニューロンにおいて線形活性化関数、すなわち、個々のニューロンの出力に重み付けされた入力（英語版）をマップする線形関数を持つとすると、線形代数から、いかなる数の層も2層からなる入力-出力モデルに削減することができることが示される。MLPでは、一部のニューロンは、生物学的ニューロンの活動電位の頻度および発火をモデル化するために開発された「非線形」活性化関数を用いる。
MLPは、非線形的に活性化されるノードの3つ以上の層（入力層と出力層と1つ以上の「隠れ層」）からなり、ディープニューラルネットワークを作り出す。MLPは全結合（fully connected）のため、1つの層中のそれぞれのノードは次の層中の全てのノードと任意の重みで結合している。
学習は、個々のデータが処理された後に、期待される結果と比較した出力中の誤差の大きさに基づいて、結合加重を変化させることによってMLPにおいて起こる。これは教師あり学習の一例であり、誤差逆伝播法（バックプロパゲーション）を用いて実行される。誤差逆伝播法は、線形パーセプトロンにおける最小二乗法アルゴリズムの一般化である。
「多層パーセプトロン」という用語は、複数の層を持つ単一のパーセプトロンを意味しない。むしろ、層へと組織化された多くのパーセプトロンを含む。代替用語は「多層パーセプトロンネットワーク」である。さらに、MLP「パーセプトロン」は、最も厳密に言えばパーセプトロンではない。真のパーセプトロンは正式には、ヘヴィサイドの階段関数といった閾値活性化関数を用いる人工ニューロンの特殊な場合である。MLPパーセプトロンは任意の活性化関数を用いることができる。真のパーセプトロンは二項分類を実行する。対して、MLPニューロンは、その活性化関数に依存して分類あるいは回帰のどちらを実行するかは自由である。
「多層パーセプトロン」という用語は後に、ノード/層の特性に関係なく適用されるようになった。ノード/層は、パーセプトロンに限定されず、任意に定義されたニューロンから構成することができる。この解釈は、一般に人工ニューロンを意味するところの「パーセプトロン」の定義の緩和を避けている。
MLPは、確率的に問題を解くことができるため研究において有用である。MLPは適応度近似（英語版）のような極めて複雑な問題に対する近似解をしばしば与える。
MLPはCybenkoの定理（英語版）によって示されているように普遍的な関数近似器であるため、回帰分析によって数理モデルを作成するために使うことができる。分類は、応答変数がカテゴリ一変数である時の回帰の特殊な例であり、MLPはよい分類アルゴリズムを作る。
MLPは1980年代に人気のある機械学習法であり、音声認識や画像認識、機械翻訳ソフトウェアといった多様な分野に応用されたが、その後より単純なサポートベクターマシンとの激しい競争に直面した。ディープラーニングの成功によってバックプロパゲーションネットワークへの関心が戻った。
多層パーセプトロン（MLP）とは人間の脳の神経細胞であるニューロンの働きをモデル化したパーセプトロンの構造を多層化したものです。ただし、これだけではわかりにくいのでそれぞれについて順に説明しましょう。
まず、人間の脳にあるニューロンとは電気信号を伝達するための細胞です。具体的にはコアとなる細胞体と軸索、樹状突起、シナプスなどによって1つのニューロンを構成しており、これらが複雑に結びつくことで脳という回路を構成します。
仕組みとしては上位のニューロンから流れてきた電気信号を樹状突起で受け取り、シナプスと細胞体で処理を行って化学反応させます。そして、この反応によって電気信号を発生させるのですが、この際に出力するかどうかは閾値を超えるのかで決めます。なお、閾値を超えない場合には電気信号は出力されません。
あとは発生した信号を軸索を経由して次のニューロンに伝えます。以上がニューロンの働きと仕組みになり、これを以下のようにモデル化したのがパーセプトロンです。
パーセプトロンは入力層と出力層の2層で構成され、入力に対して重みを付加して総和し、ニューロンで行った出力の判断は活性化関数（ステップ関数など）を利用して処理される仕組みです。このパーセプトロンの考え方は画期的なものであり、実際に人間の小脳も同じような仕組みで働いていることが後に実証されました。
しかし、パーセプトロンは単純なものであれば問題ないですが、複雑な事象を扱うと線形分離できない課題があるとわかったのです。この線形分離可能とは、平面上にあるAという分布とBという分布が存在していた場合に、両者の間に直線を引くことが可能であるということで、可能であれば機械にも数式として理解できます
一方で直線が引けない場合では、Aという分布とBという分布の区切りがぐちゃぐちゃな線になってしまうので、数式化することができず機械には理解できません。そこで考え出されたのがパーセプトロンを多層化する多層パーセプトロンでした。
パーセプトロンに中間層が加えられることで出力層への重みvが加わり、それぞれの層で総和されます。そして、最終的な出力の判断は非線形分離可能な非線形活性化関数（ReLU）で行われる仕組みです。
こちらは(−∞, +∞)の入力に対して(−1, +1)を出力するハイパボリックタンジェント（tanh）を利用する方法で、xが0以上であればそれに対応した値を出力し、0以下であれば出力を行いません。これによってパーセプトロンの活性化関数よりも多層パーセプトロンでは複雑な表現が可能になるため、線形分離できないものでも理解できるようになります。
多層パーセプトロンについての解説は以上です。これらを踏まえて多層パーセプトロンの長所と課題点をお伝えしましょう。
多層パーセプトロンの長所は、パーセプトロンの短所である線形分離不可能なものでも対応できることでした。ここではこの長所について論理演算を利用して説明しましょう。
なお、論理演算とはコンピューターや回路などで利用されている演算で、真と偽の状態を1、0で表すことでさまざまな演算を行えるものです。こちらについては論理和，論理積，否定論理積，排他的論理和の４種類の論理演算などがあります。
1つの直線で両者を分けることができないため、線形分離不可能です。つまり、排他的論理和をパーセプトロンでやろうとすると機械は数式化できないので理解できません。
しかし、これを多層パーセプトロンで行った場合には線形分離が可能になります。具体的には複数の論理演算のパーセプトロンを組み合わせ、以下のような多層パーセプトロンにすることで排他的論理和を表現できます。
これがどういうことかといえば排他的論理和は入力1、入力2のどちらかが真であり、両方が真でない場合に出力が真になる演算です。そのため、右の論理積の入力1を論理和、入力2を否定論理積のパーセプトロンにして多層パーセプトロンを構成すれば実現できます。
したがって、パーセプトロンでは線形分離不可能だった排他的論理和も多層パーセプトロンでは線形分離が可能になります。以上のように多層パーセプトロンであれば対象が複雑な場合でも線形分離できます。
多層パーセプトロンであれば線形分離不可能な問題もクリアできますが、課題点がないわけではありません。なぜなら、多層化することで学習が進まなくなる勾配消失問題が生まれるからです。
パーセプトロンや多層パーセプトロンなどでは何をもって学習を進めていくのかといえば、重みWを更新していくことがポイントになります。こちらについては出力結果のデータと教師データを比較した誤差関数、または損失関数と呼ばれるものを使い、その関数の傾き（勾配）がどうなっているのかで重みを更新します。
イメージとしては高校数学で習ったy=x2 +1などの関数の極小値を求めていく作業に似ているでしょう。具体的にはある地点の重みの傾きを微分して求め、そこからさらに傾きの小さい地点に移動し、誤差関数の値が最小になる重みWをみつけ出していく作業を行います。
この際に学習を進める対象がパーセプトロンであれば、決まった誤差関数を使い、ランダムの重みを設定して教師データと比較する作業をしていけば問題はありません。しかし、中間層を持つ多層パーセプトロンでは連鎖律による問題「勾配消失問題」が発生するのです。
これの何が問題かというと利用する活性化関数によっては微分した値が1未満であることがあり、この場合は層が増えるほど重みが更新されなくなります。
例えば、よく利用されるシグモイド曲線「A(x)=1/1+ e-x」の場合は微分した関数の極大値は0.25です。そのため、上のn層の多層パーセプトロンでシグモイド曲線を使うと、結果としてw1の勾配は限りなく0に近づきます。
そうなると重みの更新が行われなくなるため、学習が進まなくなります。これが勾配消失問題であり、層の数が少ない場合でも学習に影響があり、層を増やすほど大きく顕在化します。
ちなみに多層パーセプトロンよりも層の大きいディープラーニングでもこの問題は付きまとうため、現在では微分した値が1を超えるような活性化関数を利用します。具体的には「多層パーセプトロン（MLP）とは」で取り上げている、ハイパボリックタンジェント（tanh）を利用するReLUなどが当てはまるでしょう。
したがって、多層パーセプトロンなどを扱う際には勾配消失問題が起こらないような活性化関数を利用しなければなりません。
これまで解説してきた多層パーセプトロンと今話題のディープラーニングの関係性とはどのようなものなのでしょうか。こちらについては、多層パーセプトロンを発展させたものがディープラーニングだといえます。
具体的には中間層を含む3層で構成されるのが多層パーセプトロンであり、中間層をよりたくさん設けたものがディープラーニングです。ちなみにディープラーニングのディープは中間層（隠れ層ともいう）が深層化（ディープ）したものであるという意味から名付けられています。
背景としては1980年代に多層パーセプトロンが考えられ、線形分離不可のものでも扱えることがわかったことから研究が盛んになりました。ところが中間層をより多層化しようとすると勾配消失問題や当時の技術的な問題などにより上手くいきませんでした。
しかし、そのそれらの問題を解決するアプローチや技術が追い付いてきたことにより、中間層をより多層化することが可能になりました。そのため、多層パーセプトロンからディープラーニングという名称が名付けられ、両者は明確な違いを持ったのです。
ちなみに現在では教師あり学習だけでなく、教師なし学習や強化学習といった学習方法でラーニングさせたり、ニューラルネットワークの構造を変えたディープラーニングも生まれています。
まず、CNN「Convolutional Neural Network」は畳み込み処理とプーリング処理を行って入力する手法です。こちらは画像に対して利用されることが多いニューラルネットワークであり、画像データを圧縮して特徴量を検出して学習を進めます。
この時には畳み込み処理で元データの特徴を持った圧縮データを作成し、プーリング処理でさらに特徴を強調しながらより小さくします。これによって最終的には画像データが特徴量を記載したデータになるため、これを基に画像認識などに活用します。
次に、RNN「Recurrent Neural Network」は再帰的な構造を持ったニューラルネットワークです。学習においては時系列データなど連続したデータを扱うことがありますが、その場合には通常のニューラルネットワークでは対応が難しいです。
これはある時点でのデータがその後のデータに影響を与えるからです。したがって、RNNでは最初のデータx1の入力による出力を次の層y1だけでなく、次の入力データx2の中間層に出力することを繰り返すことで学習します。
RNNの構図としては以下のように再帰的なやり取りをする中間層（セルとも呼ばれる）を持ったニューラルネットワークを構成します。
多くの場合では店舗の売上データや気象データなどを扱う際にRNNは利用されることが多いです。ただし、時系列データの中には長期的なデータを扱うことがありますが、その場合ではRNNでは難しいことがあるので、3のLSTM(Long short-term memory)を利用します。
こちらは上のセルをLSTM blockと呼ばれるものに置き換えて構成されたニューラルネットワークです。
RNNに似ていますがLSTM blockはより構造が複雑になっており、内部にはセルだけでなく入力ゲートと出力ゲート、忘却ゲートと呼ばれる機構が備わっています。
仕組みとしては実際の入力値と入力データ、1つ前の出力データが三か所の入力ゲートに同時に入力。そして、重み付けなどがなされてセルを通り、入力ゲートや出力ゲートなどに出力されます。
この際に入出力ゲートの値が0に近い場合はデータを伝えず、1に近い場合のみ伝えることで適切にコントロールします。また、忘却ゲートではセル内部のメモリを状況に応じてクリアするもので、これによって時系列データのパターンが変わった場合でも機構が動くようにします。
Reservoir Computing
RNN(Recurrent Neural Network: 過去の出力を現在の入力として扱えるようにすることで時系列などの順番を扱えるようにしたNN)の一種として認識されている。
Echo State Network(以下ESN)とLiquid State MACHINE(以下LSM)というほぼ同時期に考えられた2つのモデルが始まりと言われています。
現状ESNのほうが知られているかもしれません。少なくともGoogleの検索に引っかかりやすい。
ここで述べる内容も主にEcho State Networkについて考える。
Reservoir Computingは入力層、中間(Reservoir)層、出力層の3層からなっています。
Reservoir特有の特徴としては入力層とReservoir層において重みの更新が行われない点です。
最初にランダムな値で決め打ちしてしまったら、あとのことはReservoir層のノードの内部状態と出力層の重みに仕事を丸投げして出力層の重みを調整することで学習していきます。
Reservoir層でのみ同層間の結合を許しReservoir層のノードが内部状態を持つことでモデル内に短期記憶を用意し、時系列データの持つ因果関係を扱えるようにしているみたいです。


RNNの場合、過去の情報についても遡って逆伝搬で学習するため回帰なしのNeural Networkに比べて学習コストがとんでもないことになるという問題がありました。
Reservoir Computingの場合、学習の部分を出力層のみに任せることでこの大量の学習コストの問題を解消できる。

また学習方法として出力層の重みは最小二乗誤差をベースに学習するため従来のRNNだと時間を遡りすぎたことにより起こる伝搬の消失や局所解に嵌ったりといったことが起こりにくくなる。


手順としては以下のようになります。

1. 各種重みを初期化
2. 訓練データを流し込む
3. 流し込まれたデータに対し全時刻での内部状態を記憶
4. 記憶した内部状態を基にここで初めて出力層の重みを更新
5. 更新データと内部状態からデータを予測


Echo State Property(以下ESP)とは与えられたタスクをReservoir Computingが達成するために満足すべき条件のことです。ざっくり言ったらfading Memoryという現在の入力は現在の内部状態に過去の入力や状態よりも大きな影響を及ぼすという機能を満たす必要があります。
つまりESPを持つということはESN は最終的に安定状態になった時に初期状態の影響を全て洗い流してしまえるということみたいです。
ここでESPを満たす場合、W_resの固有値の絶対値が最大のもの(spectral radius)を1ではないが1に限りなく近い値として持たなければいけないようです。
このあたりは正直自分も全然理解できていない場所で又別の論文を読む必要があるようです。
プログラム的にはspectral radiusでW_resを割ったあとに0.99をかけておけばなんとかなります。

ここでspectral radiusとx(t)の式との関係を考えた人は鋭いです。W_resが変化するということはu(t)の大きさがモデルに与える影響とほとんど同じ影響をspectral radiusはモデルに与えます。

違う点としては、u(t)の大きさはx(t+1)におけるu(t)項に影響を与え、直近の入力u(t)のx(t+1)に対する寄与率に影響する。
spectral radiusはx(t+1)におけるx(t)に影響を与え、過去のReservoir層の状態すべてのx(t+1)に対する寄与率に影響するという点です。
リカレントニューラルネットワークをを理解するキーワードは2つです。前の出力を次の計算に利用するセル（Cell）。単語が一直線に並んだ文章や、順次変化する株価変動などの、一列に並んだ時系列データという概念。通常のニューラルネットワークはこの時系列データを取り扱うことが難しい、あるいはできないのですが、リカレントニューラルネットワーク（RNN）は、セル（Cell）というものを利用する事で時系列データを取り扱う事ができます。
リカレントニューラルネットワーク（RNN）とは、時系列データを取り扱えるようにニューラルネットワークにセル（Cell）というものを付けたもの。と言っても、リカレントニューラルネットワーク（RNN）の中心概念、”時系列データ”が普通のデータとどのように違うのか、一見するとわかりにくいですよね。「セル」が普通の伝搬と何が違うのかを理解する為に、リカレントニューラルネットワーク（RNN）による文章生成を見てみましょう。
文章生成とはその字面の通り、人工知能（AI）に文章を生成させるものです。例えば、夏目漱石の”吾輩は猫である”の文体を学習した人工知能（AI）に、適当な冒頭の1単語を与えると文章を生成しましょう。例えば「吾輩」という単語を「吾輩は猫である」を学習した人工知能（AI）入力すると、「は」と次の単語を予測して出力します。そして、「は」という出力を次の入力として再度、次の単語を推測して「猫」や「ここで」、「藁」といった”それらしい単語”を出力します。これが文章生成の基本です。
まず、文章生成を用いて時系列データを考えてみましょう。仮に「吾輩は猫である」という文章を見てみると、”吾輩”、”は”、”猫”、”で”、”ある”という5つの単語が連なったデータであると考えることができます。そして、この4つの単語には順列性があります。”吾輩”の後に続くのはいわゆる”てにをは”である格助詞でなければいけません。そして”吾輩”、”は”と続いた後にまた”吾輩”が出てきては”吾輩は吾輩”となってしまって、文法的には間違っていなくてもヘンテコな文章になってしまいます。
この種の普通のデータとは異なり、過去のデータに大きく影響されますよね。このように過去のデータが将来のデータに影響を及ぼすデータを、時系列データと呼びます。リカレントニューラルネットワーク（RNN）とは、時系列データを取り扱えるようにニューラルネットワークにセル（Cell）というものを付けたもの。と言っても、リカレントニューラルネットワーク（RNN）の中心概念、”時系列データ”が普通のデータとどのように違うのか、一見するとわかりにくいですよね。「セル」が普通の伝搬と何が違うのかを理解する為に、リカレントニューラルネットワーク（RNN）による文章生成を見てみましょう。
文章生成とはその字面の通り、人工知能（AI）に文章を生成させるものです。例えば、夏目漱石の”吾輩は猫である”の文体を学習した人工知能（AI）に、適当な冒頭の1単語を与えると文章を生成しましょう。例えば「吾輩」という単語を「吾輩は猫である」を学習した人工知能（AI）入力すると、「は」と次の単語を予測して出力します。そして、「は」という出力を次の入力として再度、次の単語を推測して「猫」や「ここで」、「藁」といった”それらしい単語”を出力します。これが文章生成の基本です。
まず、文章生成を用いて時系列データを考えてみましょう。仮に「吾輩は猫である」という文章を見てみると、”吾輩”、”は”、”猫”、”で”、”ある”という5つの単語が連なったデータであると考えることができます。そして、この4つの単語には順列性があります。”吾輩”の後に続くのはいわゆる”てにをは”である格助詞でなければいけません。そして”吾輩”、”は”と続いた後にまた”吾輩”が出てきては”吾輩は吾輩”となってしまって、文法的には間違っていなくてもヘンテコな文章になってしまいます。
この種の普通のデータとは異なり、過去のデータに大きく影響されますよね。このように過去のデータが将来のデータに影響を及ぼすデータを、時系列データと呼びます。
そして、リカレントニューラルネットワーク（RNN）は時系列データを扱うために”セル（Cell）”というものを利用します。リカレントニューラルネットワーク（RNN）の中間層は、通常の入出力の他に、ループ状の入出力を持ちます。単語予測の時に、中間層の出力を二つ用意し、片方は通常の単語予測の為の出力、もう片方は次の単語予測の時の為の出力します。
普通のニューラルネットワークとリカレントニューラルネットワーク（RNN）の唯一にして最大の違いとは、セル（Cell）があることです。セル（Cell）とは、今回の出力の計算過程を、次回の計算に利用するループ状のニューラルネットの構造の事です。文章生成においては、次の単語予測の為の出力が、次の単語予測の時に通常の入力と併せて中間層に入力される事により、過去の中間層の状態を考慮した予測を実現するのに使われます。このループ状の中間層の構造を、”セル（Cell）”と呼びます。
リカレントニューラルネットワーク（RNN）は文章生成の他にも、株価変動や天気予報などの、時系列を考慮しなくてはいけないタスクに活用されています。
しかし、リカレントニューラルネットワーク（RNN）の主な活用分野は文章生成です。今回は夏目漱石を例としましたが、シェイクスピアっぽい詩や、ハリーポッターの次回作に近い小説を自動生成させる事などが試みられています。どちらも同じ手法を用いていて、例えばシェイクスピアに関してはシェイクスピアの作品を収録したデータセットを利用して人工知能（AI）を学習させた後、その人工知能（AI）に適当な単語や文章を与えて次の単語を予測させるというものです。
これらもリカレントニューラルネットワーク（RNN）を活用しています基本概念として使っています。また、Googleが開発しているGPT-3も文章生成の一分野ですが、これは文章によって説明された機能を実装するHTMLコードとCSSコードを生成するという事も成し遂げています。例えば、「黄色いボタン」という物を入力すると、人工知能（AI）がHTMLとCSSを用いて実際に黄色いボタンを実装するコードを生成できるとのこと。
リカレントニューラルネットワーク（RNN）は時系列を考慮する関係上、長い文章を処理しようとすると、後半になるにつれて最初の方の文章を忘却する事があります。例えば「吾輩は猫である」を使って、以下の文章を生成するとしましょう。
「（前略）吾輩は藁の上から急に笹原の中へ棄てられた。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである」」
この時、「吾輩は藁の上から急に笹原の中へ棄てられた」まで生成したものの、過去の文章がどういったものかを忘却してしまい、「られた」につながる単語として「ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである」を参考に「時」を出力し、「吾輩は藁の上から急に笹原の中へ棄てられた時何だかフワフワした感じがあったばかりである」というようなヘンな文章を生成してしまう場合があります。
これを解決する為に、リカレントニューラルネットワーク（RNN）の拡張であるLong shot-term memory（LSTM）では”セル（Cell）”に改善を加え、メモリを持たせて記憶を実現する事で、長い文章も問題なく処理できるようにしました。現在は研究、開発、どのような分野であれLong shot-term memory（LSTM）のようなリカレントニューラルネットワーク（RNN）の進化系を用いる事が普通です。特に理由がなければ、リカレントニューラルネットワーク（RNN）よりもLong shot-term memory（LSTM）を用いましょう。
リカレントニューラルネットワーク（RNN）は”時系列データ”を扱うために”セル（Cell）を用いるニューラルネットワークの一種。畳み込みニューラルネットワーク（CNN）は”畳み込み”という”画像の特徴的な部分だけを抽出する計算”によって画像データを圧縮し、推論しやすいようにするものであり、この二つは根本的に用途が異なるという事。そして、リカレントニューラルネットワーク（RNN）の拡張・改善としてLong shot-term memory（LSTM）のような新技術です。
文章生成や文章翻訳は”自然言語処理”というジャンルで、突き詰めればとても深いものですが、リカレントニューラルネットワーク（RNN）の事が分かった皆さんは、既にその入口を突破しているといえるでしょう。
RNN(Recurrent Neural Network)は、ある層の出力が遡って入力される再帰結合を持つリカレント（再帰型）ニューラルネットワークです。自分の出力をもう一度自分に食わせる、そんなややこしい構造を持つネットワークなのですが、なぜ、このような処理をする必要があるのでしょうか。 
その鍵は時系列と可変長です。前回学んだ画像の文字がマルかバツか判断するCNNは、固定長の静止画を畳み込み処理するだけでした。しかし、例えば自然言語処理(NLP)などの領域では、時間の概念が必要となり、入出力のデータサイズも可変となります。例をあげて説明しましょう。 
あなたはTV番組でどちらが「麻里ちゃん通」なのかAIと勝負することになりました。相手は麻里ちゃんを学習済のAIなので、早押ししないと勝てないかも知れません。さて、図3のような問題が流れてきました。「麻里ちゃんの」「好きな」。ここではまだ解答がわかりませんが、すでに頭の中は激しく回転していて続く言葉として「人は誰か」とか「食べ物はなに」「場所はどこ？」などが浮かんでいます。 
続く音声の「ケーキは」まで聞いたとき、AI野郎が勢いよくボタンを押しました（しまった、わかっていたのに…）。そして、自信たっぷりに「チーズケーキ」と答えたのです。 
この流れを時系列でみてみましょう。時点(t-1)で「麻里ちゃんの」、時点(t)で「好きな」と聞き、時点(t+1)で「ケーキは」ときたから問題文の意図が理解できたわけです。この人間なら当たり前のことが、順伝搬NNでは少し難しいのです。なぜなら、「ケーキは」だけだと文の意図が読み取れないので、その前の「麻里ちゃんの」や「好きな」を記憶しておく必要があり、それを可能にする構造が再帰型なのです。 
リカレントニューラルネットワークの展開図とは，時間軸に展開して説明しましょう。再帰型構造を時系列に展開したモデルです。問題文は4つの文節からなるので、4ステップのニューラルネットワークに展開できます。時点(t)の時の隠れ層には、入力層からの「好きな」と前ステップからの「麻里ちゃんの」という2つの入力があり、これを組み合わせ計算します。続く時点(t+1)で、入力層からの「ケーキは」と前ステップ隠れ層からの「麻里ちゃんの×好きな」を組み合わせ、「あ、わかったぞ」とAIはピンポーンしたわけです。 
lights.png通時的誤差逆伝搬(Back propagation Through time)
組み合わせるって簡単に言っていますが、実際は2つの入力の重みを考慮した数学的な行列計算です。前回マルという記号を判別するのにも苦労したように、人間が頭の中で自然とできている処理がAI君は結構大変なのです。AIがかしこくなる仕組みは、誤差逆伝搬(Back propagation)で重みを調整するからでしたね。RNNの学習法も同じく逆伝搬による重み調節なのですが、前ステップからの情報も加わるために特別に通時的誤差逆伝搬(BPTT)と呼ばれています。
お、そう来ましたか。確かに時点(t)で出力層って何が出力されているのでしょうか。わかっていたつもりで分かっていなかったかもです。う~ん、いつもながらシンプルな疑問ほど難しい…。 
実は、この答えはAIが何を目的に学習したかによって異なります。今回のモデルは、”問題文の意図”を出力するテキスト解析(Text Analytics)としてトレーニングしている想定です。時点(t)では「麻里ちゃんの好きな人を尋ねる」のか「麻里ちゃんの好きな場所を問う」のか文意がまだわからず、どの候補も信頼度が低い状態です。そして時点(t+1)で出力層に「麻里ちゃんの好きなケーキを問う問題」という候補ができ、その信頼度が閾値を超えたからAI野郎はボタンを押したのです。 
普通に、問題文の意図ではなく、文章全体を予想するモデルでもいいでしょう。時点(t)では、「麻里ちゃんの好きな〇〇」という候補が多すぎて判断つかなかったものが、時点(t+1)までくれば、後ろに「なんですか？」をくっつけた「まりちゃんの好きなケーキはなんですか？」という候補が浮上します。つまり、①文章全体を想定出力する→②文章の意図を読み取る→③意図と正解を学習(Classification)した分類器で解答を導き出す、というような3ステップの処理になります。ただし、この場合、疑問文になると分かっていないので、「チーズケーキ」という候補を選んでしまう可能性も大きいので、クイズ専用に学習したモデルには負けます。     
別の目的ならどうでしょうか。例えば、感情分析(Emotion Recognition)なら、時点(t+1)「麻里ちゃんの好きなケーキは」までで「ジョイ」という感情が0.8出力され、時点(t+2)まで来ると0.3くらいに落ちる感じです(Google Cloud Natural Languageで試したらこんな感じでした）。
機械翻訳(Machine Translation)はどうでしょうか。時点(t+1)では"Mari's favorite cake is" と出力され、時点(t+2)で"What is Mari's favorite cake？"と疑問文に変化して出力されます(こちらはGoogle Cloud Translationで試してみました）。  
ちなみに、ネットなどでは、次に来る単語を予測出力するという例で説明していることが多いようです。まあ、将棋の「次の一手名人」のような設定ならそういう出力もアリだと思いますが、正直、文章生成（deep Writing）のような実用イメージしか湧かなかったので麻里ちゃんに登場してもらいました。 」 
lights.png人生は、リカレントでない方がいい場合もある
実は、将棋の「次の一手」は、これまでの手順にとらわれずに、その局面だけを見て最前手を見つけ出す方がいいです。とかく、人間は”前の手の顔を立てて”とか”勢いで”とかを重視して手を選びがちなのですが、AI君がそんな要素を全く無視して最強手段を出すようになってから、人間もそれを見習うように変わりつつあります。
小さな嘘をついてしまったから大きな嘘をつく、ああ言ったからそれを正当化する発言をしてしまう、そんなふうに過去に引きずられて悪い方向に行ってしまうこともあるので、リカレントも時には考えものです。
リカレントニューラルネットワークが使われる技術分野は，自然言語処理，自然言語理解，機械翻訳，テキスト分析，音声認識，画像分析，感情分析，文章生成，音声合成，動画分析，パーソナルアシスタント，チャットボットなどが挙げられます。
代表的な適用分野は自然言語処理(NLP)です。自然言語理解(NLU：Natural Language Understanding)という言葉もよく使われますので合わせて覚えておきましょう。NLPはいかにも人間らしい分野で、図5に示すさまざまな用途の総称でもあります。 
・テキスト分析(Text Analysis) 
テキスト（文章）を読んで、〇〇を出力する処理です。〇〇のところは目的に応じていくつかあります。図6は、それらの例です。機械翻訳なら英文、感情分析なら感情と度合い、タグ付けなら「麻里ちゃん」と「ケーキ」が出力されていますね。入力が音声の場合は、普通は音声認識（Speech to Text）でいったんテキストにした後で処理しますが、感情分析のようにそれに加えて声のトーンなど音声情報も使って処理する場合もあります。  
テキスト分析は、社内ドキュメントに自動タグ付けしてドキュメント検索を楽にするとか、ネット上の技術情報を自動的にスクレイピングするなどに役立ちます。また、感情分析も、コールセンターのお客が怒っているかとか、従業員で鬱予備軍がいないか、などさまざまな用途で使われつつあります。 
・機械翻訳(Machine Translation) 
2016年にGoogle翻訳の精度が飛躍的に向上したのは、エンジンをディープラーニングに切り替えたからです。翻訳は、入力と出力で言語の長さが異なりますが、元の言語の理解と変換言語の文章生成という2つにリカレント技術が大活躍しています。  
・音声認識(Speech to Text) 
日本でもAmazon EchoやGoogle Homeなどのスマートスピーカーが発売されましたね。私も両方使っていますが、その音声認識精度の高さにはいつも関心しています（といいつつ、以前からブログをGoogleの音声入力で書いていましたが…）。音声認識は入力が可変長の音声で、出力も可変長のテキストです。単なる音の文字化ではなく、文脈をきちんと理解しているからこそ高い精度で変換できるのです。  
・画像分析(Image Analysis) 
前回説明した画像認識(Image Recognition)はCNNで処理しました。でも、画像を見て何をしているのか説明するような場合は、CNNにRNNを組み合わせて使います。例えば、麻里ちゃんがおいしそうにモンブランを食べている写真をインプットすると、最初にCNNで画像認識した後、認識した「1人の女性」「ケーキ」「食べている」などのタグを使ってRNNが「1人の女性がケーキを食べている」というように説明してくれます。こうした技術はネット上にアップされた写真の自動説明付けなどに応用されつつあります。 
・動画分析 
静止画でなく動画の分析はどうでしょうか。前回のマルを認識するAIで説明しましょう。今回はマルという文字の全体がわからず、図7のように上からスキャンして1/3ずつ画像が渡されて、それをCNNで認識するとします。時点(t+1)のとき、真ん中の画像しか渡されないより、時点(t)の画像も渡してもらう方が的確に判断できますね。こんな感じで動画の場合は、前の映像との連続した情報で判断する必要がありますのでリカレントNN向きです。この技術を使って、ネット上の写真（静止画）だけでなく、動画もタグ付けされている例を見かけます。 
・チャットボット(Chat bot) 
チャットボットは、話しかけられた言葉から意図を理解して、テキストや音声で適切な応答を行います。いろいろな自然言語理解の技術を組み合わせた1階層上位の技術ともいえます。「ああ言えば、こう言う」というのは親子喧嘩の決まり文句ですが、botもそんなフロー作成を基本としています。でも、言い方を変えても意図をきちんと理解できる柔軟性があり、そのあたりが第2次AIブームのエキスパートシステムから進化しているところです。botの利用範囲はかなり広く、ECサイトの接客ロボだったり、社内システムに対する指示(会話型UI)だったり、いろいろな面で使われつつあります。  
・パーソナルアシスタント(Personal Assistant) 
パーソナルアシスタントも上位階層の技術です。チャットボットが単なる会話のやり取りなのに対し、こちらは会話以外にメンバー全員のスケジュールを見て会議室を予約してくれたり、タクシー（米国だとUBER）の手配をしてくれたり、よりインテリジェントなイメージでしょうか。こうした上位層での応用には、さらにいろいろな技術が組み合わさって使われていますが、人間とのインターフェースに関わる自然言語理解が活躍しています。 
リカレントニューラルネットワークは、過去を記憶して利用する技術です。これがずらずらと長い文章だったと想像してください。記憶する範囲が大きくなると、勾配（過去のどの情報がどれくらい影響を及ぼすか）が複雑になり過ぎて伝えるべき誤差が消滅したり（勾配消失問題）、記憶したものをどう生かすかという計算量も爆発的に増えます。そのため、リカレントNNは記憶する範囲をちょっと前までに限定していて、それ以前のものは切り捨てています。このことを打ち切り型通時的逆伝搬(Truncated Back propagation Through time)とも言います。 
でも、現実社会では、もう少し前までの情報を使わないとうまくいかないものもあります。そのためRNNを改良する形で登場したのがLSTM(Long Short-Time Memory)という技術で、日本語では長・短期記憶ユニットと呼ばれています。次回は、このLSTMについて解説しますのでお楽しみに。 
ニューラルネットワークは入力を線形変換する処理単位からなるネットワークである。このネットワーク内に循環が存在する、すなわちユニットの出力が何らかの経路で自身へ再び入力する場合、これを回帰型ニューラルネットワークという。回帰のないネットワーク（順伝播型ニューラルネットワーク（英語版）; Feed-Forward Network; FFN）と対比される。
RNNは任意のひと続きの入力を処理するために内部状態（記憶）を使うことができる。これによって、時系列のための時間的な動的振る舞いを示すことが可能となる[2]。これによって、分割化されていない、つながりのある手書き文字認識や音声認識といった課題に応用が可能になっている。
「回帰型ニューラルネットワーク」という用語は、類似した一般構造を持つ2つの広いネットワークのクラスを指し示すために見境なく使われる。1つは有限インパルス、もう1つは無限インパルスである。どちらのネットワークのクラスも時間的な動的振る舞いを示す。有限インパルス回帰型ネットワークは厳密な順伝播型ニューラルネットワークに展開でき、置き換えることができる有向非巡回グラフであるのに対して、無限インパルス回帰型ネットワークは展開できない有向巡回グラフである。
有限インパルスと無限インパルス回帰型ネットワークはどちらも追加の保管状態を持つことができ、この保管場所はニューラルネットワークによる直接的な制御下とすることができる。保管場所は他のネットワークやグラフが時間遅延を取り込むか、フィードバックループを持つのであれば、それらで置き換えることもできる。こういった制御された状態はゲート状態またはゲート記憶と呼ばれ、長・短期記憶ネットワーク（LSTMs）およびゲート付き回帰型ユニット（GRUs）の一部である。
Attentionとは
入力側のどこに注目するかを表現する仕組み、あるいは注目した入力データに従って出力を行う仕組み
複数のベクトル (行列) があったとき、どのベクトルを重要視するかも含めて学習させる仕組み.
Attentionの基本的な処理
Query と Key をスコア関数に入力して重要度スコアを計算
Softmax 関数で割合化・確率化し重要度確率を計算
Value と重要度確率で、重み付き平均の計算を実施し、Value から有益な情報(コンテキストベクトル)を取得
Attention の目的
Attentionは、Query と Key が似ているかどうかで、行列のどの要素(ベクトル)を読み込むかどうかを制御している. 画像データの位置関係 (空間的に近いデータ点同士の方が強い関係性を持つ)や、時系列データの系列関係 (時系列的に近いデータ同士の方が強い関連性を持つ)を 度外視して(空間的、時系列的に遠い情報でも重要かどうかを見ているため)、重要な情報同士をまとめることができる.
Attentionの構造
Attentionのキーワードは「Query」「Key」「Value」です。
Query : 「これに関係するものは何か」を知りたい要素です。
上記の例で言えば「簡単」の1つ前の「は」のhidden stateがqueryです。
Key : 「これらの中のどれに注意を向ければよいか」を知りたい要素です。
上記の例で言えばEncoderの全てのhidden stateです。
Value : 多くの場合Keyと同じ要素です。queryとkeyから計算した重みを掛けるために使われます。


Attentionの計算は次のような手順で行われます。
queryとkeyのエネルギーe(類似度、重要さ)をCompatibility functionで計算します。
eをDistribution functionで正規化し、Attention weights aを計算します。
aとValueの各要素の積を取り、context vectorを計算します。


Attentionは最近流行っている仕組みです。
Attentionは、CNNと組み合わせて使われます。
CNNと組み合わせることで、比較的簡単な構造で性能を向上させることができる。
Attentionはニューラルネットワークの広い分野で利用されている。


Attentionの概要について説明する。
まず第一段階として、CNNを用いて多くの入力データから簡単な特徴を抽出する。
第2ステップでは、入力から生成された特徴量を全て使うわけではありません。そこで、ネットワークはそのうちのいくつかに着目する。
そして、フォーカスされた特徴はニューラルネットワークの推論に使われる。
データの一部、つまり特徴に焦点を当てるので、Attentionと呼ばれる。
Attentionは自然言語処理の分野で発展してきた仕組みである。


簡単な例として、画像処理を紹介。
CNNを用いた画像認識について考えてみよう。
寿司の画像を例にとって考えてみよう。
寿司画像は物体認識の問題として考える。
これはブリ（鰤）の画像である。
人間は画像のブリの部分だけを見れば、その物体がブリかどうかを判断できる。
通常のCNNの場合、画像全体が入力としてネットワークに与えられます。
そのため、画像認識は背景の領域に非常に敏感です。
この画像は、黒いお皿の上に「ぶり」が乗っています。
お皿の色が変わっても、お皿の上のお寿司は同じです。
お皿の色がどうであれ、「ぶり」という答えを期待するのです。
ネットワークには背景のある画像が与えられるので、出力は背景の影響を受ける。
この影響を軽減するために、人間の注意力であるフォーカス機構を画像処理に導入しています。
ここでは、画像の寿司だけに注目する方法を考える。
これが画像処理分野における「Attention」の概念である。


まず、入力画像から簡単な特徴量を抽出する。
次に、関心領域を推定するニューラルネットワークに分岐する。
このマスク画像のように、CNNから関心領域を推定するマスクを取得する。
この注目領域の画像で入力画像をマスクする。
すると、左下の画像のように、寿司の部分のみを抽出することができる。
このように、寿司の部分だけを抽出することで、後段で背景を無視した物体認識を構築することができる。
以上が画像認識分野におけるアテンションの一例である。
画像処理におけるAttentionは、もっとわかりやすい。
Attentionとは、関心領域を推定する仕組みである。
これは簡単な例であった。


通常のCNNで考えてみよう。
CNNの最初のステージで作成された特徴マップにアテンションを適用する。
この概念をSqueeze-and-Excitation Network (SENet)という。
SENet はマスク情報を構築し、各特徴量マップの関心領域を推定する。
この例では、1つ目の特徴マップと3つ目の特徴マップが認識上重要である。
そこで、1番目と3番目のマップに対してマスクを構築し、注目する特徴を推定する。
構築したマスクを元の入力画像に適用することで、このような特徴マップ群を得ることができる。
これらの特徴マップは、後段のニューラルネットワークに使用される。
以上で画像処理のAttentionの説明は終わりです。



Attentionが元々開発された自然言語処理の分野
これは文章を分類するネットワークでのAttentionの例である。
例として、入力文が肯定的な感情か否定的な感情かを判断する文の分類の問題を考えてみる。
入力文を単語に区切り、各単語の特徴量を出力する。
すると、一般にCNNやRNNは、正負のラベル情報を直接推定する。
Attention機構を導入することで、判断に有用な単語の特徴のみを抽出することが可能である
を抽出し、その特徴量に基づいて推定を行うことができる。
この例では、「おいしい」という単語が有用である。
この単語は、肯定的か否定的かの判断に大きな影響を与える。
したがって、この単語だけに注目し、他の単語は出力にほとんど影響を与えない。
このように、Attentionは文の分類に応用することができる。


Attentionの翻訳の例について
これはAttentionを用いずにLSTMを用いて翻訳を行った例である。
どのように翻訳が行われるかを説明する。
前の例と同様に、文を単語に区切り、各単語に対してLSTMを用いて特徴量を生成する。
特徴量の生成にはエンコーダを用いる。
これはLSTMの時間発展型モデル（Sequence to Sequence）に基づいている。
このモデルは全ての単語に対して特徴を抽出する。
次に、このモデルは原文全体の特徴から翻訳後のテキストを生成する。
デコーダはそのテキストを文頭から単語単位で翻訳していく。
Attentionなしの翻訳はこのような仕組みになっている。
人間がこの文章を翻訳するとき、最初から文章全体を見ているわけではありません。

Attentionを使った翻訳について説明します。
私たち人間は、この文章を翻訳するとき、最初から文章全体を見ているわけではありません。
例えば、文中の主語をまず探します。
例えば、「ぶり」という単語に注目してみましょう。
この場合、「ぶり」という特徴を利用して訳語を作ります。
アテンションは、人間が翻訳するのと同じようなことができる。
これは、前の画像の例と同様です。
まず、各単語の特徴を抽出する。
そして、先ほどのデコーダーの出力とエンコーダーの出力を掛け合わせ、ソフトマックスを適用する。
そうすることで、どの単語に注目するかを決めることができる。
この例では、2番目の単語である「ぶり」に着目することが決定される。
そして、「ぶり」という特徴量をもとに、出力する単語を決定している。
各ステップで注目する単語を決定するために、Attentionが使われる。
このようなプロセスを経て、Attentionを使った翻訳後の文章ができあがる。


Attentionを使って翻訳の精度を上げる例
精度を上げるために、Query-Key-Valueを用いたSource-Target Attentionが提案された。
このモデルでは、エンコーダが検索用にKeyという素性を出力し、実際の翻訳用にValueという素性を出力する。
つまり、Encoderの出力を分離するためのモデルである。
先ほどの例と同様に、Decoderも素性を持つ。
このモデルでは、Decoderの素性はQueryと呼ばれる。
前の例と同様に、Decoderの最初の単語を決定してみよう。
最初の単語はQueryとして与えられる。
そして、最初の単語に対するすべてのキーの応答をチェックする。
Keyのレスポンスが最も大きい単語のValueを使ってFeatureを作成する。
作成した特徴量を元に、次の単語の翻訳単語を決定する。
では、その手順を順番に見ていこう。
まず、対象のQueryに全単語のKeyを掛け合わせる。
乗算結果が最も高い単語に注目する。
これにより、注目する領域の特徴量の効果が高まり、他の領域の特徴量の効果が減少する。
そして、この注目によって得られた特徴量を用いて、翻訳された単語を作成する。
これがソース・ターゲット注目の仕組みである。


Self-Attention
先に説明したソース・ターゲット注目の場合、Key / ValueとQueryは異なるソースから生成される。
翻訳の例では、Key / ValueはEncodeから、QueryはDecoderから生成される。
Self-Attentionの場合、Key/ValueとQueryは同じソースから生成される。
そこで、Self-Attentionは、自ら生成したKey/Queryを用いて、自ら生成したValueに重み付けを行い、合計する。
文中のSelf-Attentionの例をこの図に示す。
Self-Attentionは、Encoder側で特徴量間の関係を考慮することができる。
「このブリはおいしい」という文では、"おいしい "という単語が "ブリ "を特徴づけている。
「このブリはまずい」という文の場合、"不味い "という単語が "ブリ "を特徴付けている。
"おいしい "や "不味い "という単語は、前の単語 "ブリ "を特徴付けると考えられる。
つまり、入力データの単語間の関係性を考慮することができる。
Self-Attentionのポイントは、入力データ中の関係性に着目できることである。


Self-Attentionの例
まず、Queryと処理対象の単語のすべてのKeyの内積を取る。
ここでは、「ブリ」に着目し、Queryの周辺にある他のKeyを参照する。
最も関連性の高い単語である「delicious」に着目する。
そして、各単語からValueを受け取る。
Valueを受け取ると、重みが高いほど反映されるように重み付けを行う。
そして、最終的な特徴量を作成する。
Attention後の情報は、入力された単語と重み付けされたValueの和となる。
以上のように、Self-Attentionは文中の単語間の関係性を抽出することができる。


自然言語処理の分野でのSelf-Attentionの例
Self-Attentionは、画像処理にも応用できる。ニューラルネットワークによって抽出された特徴マップの集合から、各Query Key Valueを生成する。
自然言語の場合と同様に、QueryとKeyから重要な部分を見つける。
そして、その値を重み付けして合計する。
そして、重み付け和から得られた特徴を、元データの特徴に追加する。
このように、重要な特徴に着目することで、Attentionを画像処理に応用することができる。


Self-Attentionを画像に利用するメリット
まず、離れた場所にある特徴量を考慮することができる。
"画像に写っているブリはおいしいか？"という分類問題を解くニューラルネットワークを考えてみましょう。
左の入力画像ではブリが小さく、通常のCNNでは分類が困難です。
しかし、空間的に離れた場所にある「華麗な笑顔」という情報を使って分類することができれば、「ブリ」を「美味しい」と感じることができる。
もし、笑顔の素敵な男性が写っていなかったら、"ブリ "が美味しいかどうか判断できないかもしれません。
仮にいたとしても、「笑顔の素敵な男性」と「ぶり」は、通常のCNNでは遠すぎるため、この問題を解決することは難しい。
しかし、Self-Attentionの仕組みを応用すれば、空間的に離れた画像特徴量を組み合わせることができる。
それらを組み合わせることで、「ブリが美味しい」と推論できるかもしれない。
現在では、自然言語処理や画像処理など、さまざまな応用分野を持つAttentionを見られる。
ディープラーニングに畳み込みニューラルネットワークを使うことが人気を集めるのには、3つの理由があります。
	畳み込みニューラルネットワークは、人手による特徴抽出を必要とせず、畳み込みニューラルネットワークが直接特徴を学習できる
	畳み込みニューラルネットワークは認識精度が高い。
	畳み込みニューラルネットワークは、新しい認識タスクのために再学習することができるため、既存のネットワークを利用することができる。
畳み込みニューラルネットワークは、画像や時系列データの重要な特徴を明らかにし、学習するために最適なネットワークアーキテクチャを提供します。畳み込みニューラルネットワークは、以下のようなアプリケーションで重要な技術となっています。
	医用画像: 畳み込みニューラルネットワークは、何千もの病理報告書を調べて、画像中のがん細胞の有無を視覚的に検出することができます。
	音声処理: キーワード検出は、マイクを備えたあらゆる機器で使用でき、特定の単語やフレーズが話されたときに検出することができます (例えば「Hey Siri!」)。畳み込みニューラルネットワークは、環境に関わらず他のフレーズを無視して、キーワードを正確に学習・検出することができます。
	一時停止標識検出: 自動運転では、畳み込みニューラルネットワークを用いて標識などの存在を正確に検出し、その結果に基づいて判断を行います。
	合成データの生成: 敵対的生成ネットワーク (GAN) を用いて、顔認識や自動運転などのディープラーニングアプリケーションに使用する新しい画像を生成することができます。

畳み込みニューラルネットワークは、数十から数百の層を持ち、それぞれが画像の様々な特徴を検出することを学習します。各学習画像には、異なる解像度のフィルターが適用され、各畳み込み画像の出力が次の層の入力として使用されます。フィルターは、明るさやエッジなどの非常に単純な特徴から始まり、物体を定義するユニークな特徴へと複雑化していきます。
特徴学習、層、分類
他のニューラルネットワークと同様に、畳み込みニューラルネットワークは入力層、出力層、そしてその間にある多くの隠れ層で構成されています。
これらの層は、データに特有の特徴を学習する目的で、データを変更する動きを行います。代表的な層の例としては、畳み込み層、活性化またはReLU、プーリング層の3つがあります。
	畳み込み層では、入力画像を一連の畳み込みフィルターに通すことで、それぞれのフィルターが画像の特定の特徴を活性化します。
	l  正規化線形ユニット (ReLU) は、負の値をゼロにマッピングし、正の値を維持することで、より速く、より効果的な学習を可能にします。活性化された特徴だけが、次の層に引き継がれることから、これは活性化と呼ばれることもあります。プーリング層は、非線形ダウンサンプリングを行うことで出力を単純化し、ネットワークが学習する必要のあるパラメータの数を減らします。
これらの作業が数十から数百の層で繰り返し行われ、層ごとに異なる特徴を識別するように学習していきます。

重みとバイアスの共有
従来のニューラルネットワークと同様に、畳み込みニューラルネットワークには重みとバイアスを持つニューロンがあります。モデルは学習プロセス中にこれらの値を学習し、新しい学習例ごとに継続的に更新します。ただし、畳み込みニューラルネットワークの場合、重みとバイアスの値は、ある層のすべての隠れニューロンで同じです。
これは、すべての隠れニューロンが、エッジやブロブなどの同じ特徴を、画像の異なる領域で検出していることを意味します。これにより、ネットワークは画像内の物体の移動に耐性ができます。例えば、車を認識するように学習されたネットワークは、画像内のどこに車があっても認識することができます。
分類層
多くの層で特徴を学習した後、畳み込みニューラルネットワークのアーキテクチャは分類に移行します。
最後から2番目の層は、全結合層であり、K次元のベクトルを出力します (Kはネットワークが予測できるクラスの数)。このベクトルには、分類される画像の各クラスの確立が含まれています。
畳み込みニューラルネットワークアーキテクチャの最終層では、ソフトマックスなどの分類層を使って分類出力を行います。
MATLAB® と Deep Learning Toolbox™ を使用することで、畳み込みニューラルネットワークの設計、学習、展開が可能になります。
MATLABは、新しいデータセットから特徴を学習して識別する際に使用できる、ディープラーニングの事前学習済みモデルの多くのセットを提供しています。転移学習と呼ばれるこの方法は、学習をゼロから始めることなくディープラーニングを適用する便利な方法です。専門家によって構築された実証済みのアーキテクチャであるGoogLeNet、AlexNet、Inceptionなどのモデルを利用して、ディープラーニングを始めるためのよい出発点となります。
ネットワークの設計および学習
ディープネットワークデザイナーを使うことで、事前学習済みモデルのインポートしたり、ゼロから新しいモデルを作成したりすることができます。

また、アプリ内で直接ネットワークを学習させることができ、精度、損失、検証の指標をプロットして学習状況をモニタリングすることができます。
転移学習への事前学習済みモデルの利用
事前に学習したネットワークを転移学習で微調整することは、一般的にゼロから学習するようにもはるかに高速で簡単です。また、データや演算リソースの使用量も最小限で済みます。転移学習は、ある種の問題から得られた知識を、類似した問題の解決に利用します。事前に学習されたネットワークを用いて、新しい課題を学習します。転移学習のメリットは、事前学習済みのネットワークが豊富な特徴をすでに学習していることにあります。これらの特徴は、他の様々な類似したタスクに適用することができます。例えば、何百万枚もの画像を使って学習したネットワークを、数百枚の画像だけを使って新しい物体分類のために再学習することが可能です。Hardware Acceleration with GPUs
GPUによるハードウェアアクセラレーション
畳み込みニューラルネットワークは、数百、数千、あるいは数百万枚の画像を使って学習されます。大漁のデータや複雑なネットワークアーキテクチャを扱う場合、GPUはモデルの学習にかかる処理時間を大幅に短縮することができます。
畳み込みニューラルネットワークを使用したアプリケーション
物体検出
物体検出は、画像やビデオの中で物体の位置を特定し、分類するプロセスです。Computer Vision Toolbox™ は、YOLOやFaster R-CNNを用いたディープラーニングベースの物体検出器を作成するための学習フレームワークを提供します。
ディープラーニングを使用した物体検出
この例では、ディープラーニングとR-CNN (Regions with Convolutional Neural Networks) を使って物体検出器を学習する方法を示しています。
キーワード検出
音声からテキストへの変換の例としては、特定のキーワードやフレーズを認識して、それを指示として利用できるキーワード検出があります。一般的な例としては、デバイスの起動や照明の点灯などが挙げられます。

ディープラーニングを使用したキーワード検出
この例では、MATLABを使用して音声中の音声コマンドの存在を識別および検出する方法を説明しており、音声アシスト技術に使用することができます。
セマンティックセグメンテーション
畳み込みニューラルネットワークは、セマンティックセグメンテーションに使用され、画像の各ピクセルを対応するクラスラベルで識別します。セマンティックセグメンテーションは、自律走行、工業検査、地形の分類、医療画像などのアプリケーションに利用できます。畳み込みニューラルネットワークは、セマンティックセグメンテーションのネットワークを構築するための基礎となります。

ディープラーニングを使用したセマンティックセグメンテーション
この例では、MATLABを使ってセマンティックセグメンテーションのネットワークを構築し、画像の各ピクセルを対応するラベルで識別する方法を紹介します。
MATLABは、あらゆるディープラーニングのためのツールと機能を提供します。信号処理、コンピュータビジョン、通信・レーダーなどのワークフローを強化するために畳み込みニューラルネットワークをお使いいただけます。
畳み込みニューラルネットワークについてさらに詳しく
畳み込みニューラルネットワークを用いた画像解析をサポートする製品には、MATLAB、Computer Vision Toolbox™、Statistics and Machine Learning Toolbox™、Deep Learning Toolbox があります。
畳み込みニューラルネットワークを使用するには、Deep Learning Toolboxが必要となります。学習と予測は、計算能力3.0以上のCUDA® 対応GPUでサポートされます。GPUの使用が強く推奨されており、Parallel Computing Toolbox™が必要となります。
コンピュータビジョンの分野では、Convolution Neural Network (CNN ) が長らく最もポピュラーな規格であり、最近では Vision Transformer (ViT ) のような Attention-based Network も注目されている。しかし、本論文では、Convolutional Networks に注目する。しかし、本論文では、Convolution と Attention の両方が不要であることを示す。MLP-Mixerは2つの層からなり，1つ目は画像パッチごとに適用されるMLP（位置情報を混合するため），2つ目は画像パッチ間にわたって適用されるMLP（空間情報を混合するため）である．MLP-Mixerは、十分な量のデータセットと正規化手法を用いることで、SoTAに匹敵する画像分類性能を達成している。
入力として、画像を16x16のパッチに分割します。次に、各パッチに対して線形埋め込みを適用する。そして、それらにMixer Layerを繰り返し適用する。最後に、一般的なCNNと同様にGlobal Average Poolingを適用し、画像を分類する。



次に、上図に示すMixer層の内容を説明します。ミキサーは、チャンネルミキシングMLPとトークンミキシングMLPの2種類のMLPを使用します。トークンミキシングMLPは、異なる空間位置（トークン）間の特徴を混合する役割を担っています。トークン混合MLPは，異なる空間位置（トークン）間の特徴を混合する役割を担っており，テーブルの各列を入力として扱い，各チャネルに独立して適用されます．一方，チャンネル混合MLPは，異なるチャンネル間の特徴を混合することを担い，各トークンに独立して適用され，テーブルの各列を入力として扱います．
チャンネルミキシングMLPは1×1畳み込みのCNNとみなすことができ，トークンミキシングMLPは1チャンネルの深さ方向の畳み込みとみなすことができる．しかし，このMLP-Mixerは，そのようなCNNに比べ，はるかにシンプルなアーキテクチャを有しています．

また，トークンミキシングMLPとチャンネルミキシングMLPを数式に変換してみましょう．MLP-Blockの構造を以下に示します．全結合層＋GELU＋全結合層で構成されており、CNNやTransformerよりもはるかにシンプルな構成になっています。また、各MLPはResNetのようなスキップ結合で接続されています。また、各MLPの前には、Layer Normalizationが行われる。
MLPの実験では、従来手法と同様に大規模データセットで事前学習を行い、小規模データセット（下流タスク）で微調整を行う。目的は以下の3つである。

1.下流タスクでの精度 
2.事前学習にかかる計算コスト 
3.推論時のスループット

なお、この論文の目的はSoTAの実現ではなく、MLPベースのモデルが現在のCNNやTransformerに匹敵し、それを超える可能性を持っていることを示すことです。

パーセプトロンとは、動物の脳は多数(人間の大脳の場合140億個位)の「神経細胞」という細胞がシナプス結合されて出来ています。 そこでまず研究者達はこの「神経細胞」を数学的にモデル化した「パーセプトロン」を考案しました。

次に研究者達は「パーセプトロン」を多数結合して「多層パーセプトロン(MLP: Multi-Layer Perceptron)」と呼ばれるネットワークを考えました。
MLP は「入力層(Input Layer)」、「隠れ層(Hidden Layer, 中間層とも言う)」、「出力層(Output Layer)」ごとに層(Layer)分けされた多層構造になっています。 各層は更に多数の層に分かれている場合もあります。
そして MLP の各層間のパーセプトロンは全て互いに結合しているような構造になっています。 この様な構造を持つニューラルネットワークの事を「全結合型(Fully Connected)ニューラルネットワーク」と呼びます。
ところでネットワークの構造として回帰(または再帰)構造を考えることも出来ますが、一般に MLP は回帰構造を持たない「フィードフォワードニューラルネットワーク(FFNN)」とし、回帰構造を持つ「回帰型ニューラルネットワーク(RNN)」とは区別しています。

単純パーセプトロンは入力層と出力層のみであったのに対し、多層パーセプトロンは中間層(隠れ層)と呼ばれる、層が複数追加されたネットワーク構造を持ちます。 単純パーセプトロンと違い複数のクラス分類を可能とし、線形分離不可能な問題も解くことができます。

多層パーセプトロン（たそうパーセプトロン、英: Multilayer perceptron、略称: MLP）は、順伝播型ニューラルネットワークの一分類である。 MLPは少なくとも3つのノードの層からなる。 入力ノードを除けば、個々のノードは非線形活性化関数を使用するニューロンである。

多層パーセプトロンとは、1980年代に登場した機械学習手法の一種です。多層パーセプトロンはもともと1960年代に登場した単純パーセプトロンの欠点を補う目的で開発されました。
単純パーセプトロンとは複数の入力値（x1, x2, x3とする）に対し、それぞれ調整した重み（w1, w2, w3とする）をかけて出力値（y = w1x1+w2x2+w3x3）を計算し、その結果を使って入力データを分類する手法です。二次元平面に直線を引き、入力データがその直線の右側か左側かによって分類をします。例えば、測定結果がある閾値に対して大きいか小さいかを判定するような場合がこれに当てはまります。

単純パーセプトロンは二次元平面を直線で左右に分けるまでが限界でした。そうすると、例えばOKデータが平面上の右上（第一象限）と左下（第三象限）、NGデータが右下（第四象限）のそれぞれ全域に分布しているような場合、OKデータとNGデータを分けられる直線は存在しません。すると当然、直線を曲線にしたいというニーズが発生しますよね。

そしてこれら中間層の値にさらに重み(W1, W2, W3とする)をかけ、すべて足した値を出力値(z = W1y1 + W2y2 + W3y3)として算出するのです。
つまり、出力値z = W1(w11x1 + w12x2 + w13x3) + W2(w21x1 + w22x2 + w23x3)+W3(w31x1 + w32x2 + w33x3)と表すことができ、単純パーセプトロンより調整できるパラメータがずっと増えています。

多層パーセプトロンの良いところは、データをうまく分類できるよう教師データを使って重みの値を自動調整できることです。この自動調整の手法を「逆誤差伝播法」と言います。
逆誤差伝播法を非常に簡単に説明すると、各重みの値を少しだけ増減させ、多層パーセプトロンによる予測値が教師データの正解に近づくように重みを変化させます。これにより、人がパラメータを調整することなく教師データを使って分類することができる、いわゆる機械学習が実現できました。
また、もう一つの良いところは、あらゆるデータに対して機械学習が可能な点です。画像データは機械学習が最も得意とする対象ですが、画像データも元々は0と１で表現されたデータです。同じように多層パーセプトロンでは0と１で表現された音声データのような時系列データも扱うことができます。パソコンで扱うデータはすべてこのような0と1で表されたデータなので、工夫によって機械学習ができると言えますよね。
多層パーセプトロンの欠点は分類の根拠がわからないことです。仮に100％思い通りに分類できていたとしても、どういう根拠で分類されているのかが判断できないため、作成した多層パーセプトロンに対して完全に信頼することができません。

例えば、製品の画像検査を多層パーセプトロンで学習させた場合を考えます。開発者としては製品の欠陥があればNGと判断して欲しいのですが、NGデータとして用いた画像すべてに埃が写っていたとすると、実は多層パーセプトロンは欠陥の有無でなく埃の有無でOKかNGかを判断している可能性があります。そして、学習結果の重みなどを確認しても欠陥の有無で判断しているのか埃の有無で判断しているのかはわかりません。

多層パーセプトロンを実装して動作を検証したいとき、最もオススメの方法はPythonを利用し、Tensorflowライブラリを利用する方法です。Pythonとはプログラミング言語の一種で、近年機械学習分野で最も広く使われている言語です。機械学習に必要な機能がすべて揃えられる上、ネットや書籍など情報量も多いため、機械学習をするなら迷わずPythonを選びましょう。
ライブラリとは、Pythonの標準機能にはない機能を追加する拡張キットのようなもので、Tensorflowライブラリを用いると、高度な計算に必要なあらゆる機能が使えるようになります。
まずはPythonをインストールしましょう。インストール方法は、Python公式サイトからインストーラーをダウンロードし、実行するだけ。なお、TensorflowライブラリはPythonバージョン3.7までしか対応していないので、注意が必要です。
Python本体のインストールが終わったらTensorflowライブラリのインストールをしましょう。コマンドプロンプトにてpip install Tensorflowと入力するだけでインストールができます。以上で多層パーセプトロンを実装する下準備は完了です。
Reservoir Computingの概念
下の画像をご覧ください。
水を貯めるための容器や湖を想像してください。
その中に石を投げ入れるとします。
このとき、3つの石を連続して投げ入れると
3つの石が作る水面の波紋を観察することができます。
石の大きさや形によって波紋は変化します。
また、投げる順番によっても波紋は変化します。
石によって水面がどのように変化するか、想像できますか？
貯水池コンピューティングは、あるアイデアをきっかけに生まれました。
波紋の形状を観察することで、時系列データの特徴を抽出する。
石を投げた順番や時間が、湖面の波紋として現れる。
つまり、記憶が表現されているのです。
時系列処理に適している。
貯水池に簡単な識別器をつけることで
パターンを解析することができる。
読み出しはリザーバーに取り付けたリーニングマシンである。
複雑なパターンを表現するリザーバーと簡単なリーニングマシンにより
時系列データをリーン化することができる。
読み出しには、線形学習機がよく使われる。
線形関数の係数は学習により決定される。
パーセプトロンを学習した。
時系列データも簡単な学習機で学習できる。
これがReservoir Computingの利点である。


linear learning machine、Deeplearning、Reservoir Computingを比較
線形学習機1台では、非線形の入出力関係を学習することはできない。
Deeplearningは、学習コストは高いが、計算性能は高い。
非線形の入出力関係を学習することができる。
Reservoir Computingは、linear learning machineとReservoirの組み合わせである。
低い学習コストで非線形の入出力関係を学習することができる。
しかし、計算性能はタスクによって異なる。
下図は教科書から引用したものである。
この図は、性能と学習コストの関係を示している。
Reservoir Computingの学習コストは、linear learning machineとほぼ同じであり、性能は高い。
同規模のDeeplearning modelと比較すると、調整すべきパラメータ数が少ない。
学習コストが低いとはいえ計算性能にばらつきがある。


Reservoirを利用することで軽量なリカレントニューラルネットワークを構築することができる。
Reservoir Computingは、ディープニューラルネットワークと比較して、高いポテンシャルを有している、最近注目されている計算モデルです。

Reservoir Computingの代表的なモデルであるESN（Echo State Network）について
ESNは時系列パターン認識などに利用されている。
入力履歴がエコーして残るエコー状態がリザーバーに作られる。
リザーバーに現れた特徴は、学習された読み出しによって処理される。
学習されるのは読み出しのみである。
単純な学習機械が読み出しに使われることが多い。
リザーバーでは、リカレントニューロンが複雑に接続されている。
この接続は固定されており、学習されない。
W^outのみが学習されるため、学習コストは低い。
これは下図のベクトルの定義である。
入力はN_u次元のベクトルである。
リザーバには x_1 から x_N_x までの多数のニューロンがある。
これは読み出しに接続されている。
出力層の次元は N_y である。
W_in は入力層とリザーバを接続する接続重みである。
リザーバ内の接続の重みは W である。
W^out はリザーバと出力層を接続する接続重みである。
W_inとWは固定で、W^outのみを学習させる。
リザーバ内の時系列パターンの表現を見てみよう。
入力ベクトルと W^in は乗算され、リザーバに与えられる。
リザーバ内では、ニューロンはリカレント接続される。
追加される前の1回分のステップでのリカレント接続。
リザーバの状態ベクトルの時間発展を表現することができる。
各ニューロンには、非線形活性化関数が適用される。
活性化関数は特に断りのない限り双曲線タンジェントである。
出力ベクトルは、リザーバの状態ベクトルと W^out の掛け算である。

ESNの派生モデルの例で
基本モデルと一般モデルには、いくつかの違いがあります。
まず、出力層とリザーバをつなぐフィードバック接続があること。
次に、入力層と出力層のディレクトリをつなぐ接続がある。
接続を追加しても、W^outだけが学習される。
緑で示されるフィードバック接続の重みは固定である。
ニューロンにはいくつかの種類があります。
ここでは、Leaky Integrator(LI)モデルについて説明する。
LIモデルを用いることで
LIモデルを用いることで、リザーバの状態ベクトルの時間発展の速度を制御することができる。
したがって、過去の情報をどの程度保存するかを制御することができる。
Î±はリーク率であり、このモデルのハイパーパラメータである。
Î± = 1のとき、右項のみの効果である。
これは、基本モデルに相当する。
Î±が小さくなると、リザーバの状態の変化は入力であるuに影響を与えない。
リザーバーの時間発展はより遅くなる。
この効果は、時系列入力データの高周波成分を除去するローパスフィルタのようなものである。
このモデルを用いることで、性能の向上が期待できる場合もある。
Reservoir Computingのシンプルで効果的な微分モデルである。


ESNの重要な考え方であるESP(Echo State Property)について
ESPとは、時系列入出力変換器としての再現性を保証する性質である。
これは、リザーバーが満たすべき性質の一つである。
リザーバの状態ベクトルは、初期状態と時系列入力によって決定される。
初期状態が変化すると、同じ入力を与えてもリザーバの応答が異なることがある。
このような初期状態の影響を防ぐためには、十分な時間が経過した後に、時系列入力のみによってリザーバの状態の時間発展を決定する必要がある。
これが前文の数学的表現である。
異なる状態から出発したリザーバー状態ベクトルは、例えばゼロに変換された値など、同じ値に変換される。
この図に示すように、異なる状態から始まるリザーバーの状態ベクトルは同じ軌道に変換されます。
リザーバはこの性質を満たす必要がある。
これがESPを満たす条件である。
ここで、活性化関数は双曲線タンジェントである。
指標としてスペクトル半径を用いることが多い。
スペクトル半径とは、リザーバの接続重みの最大固有値のことである。
すべての固有値は1以下であることが望ましい。
すべての固有値が1より小さい行列に掛け続けると、ベクトルは短くなる。
この操作を繰り返すことで、ベクトルは0に変換されます。
これがESPを満たす有名な条件です。
貯水池を設計する際には、この条件を考慮する必要があります。
もう一つの条件は、特異値の最大値である。
詳細は省くが、この条件はあまり重要ではないかもしれない。
基本的には、スペクトル半径を考慮してリザーバを設計する。


学習方法について
学習は readout のみである。
他の教師あり学習と同様に、入力と目標信号を与える。
回帰問題を解く場合は、連続的な目標値が与えられる。
分類問題を解く場合は、1ホットのベクトルが与えられる。
読み出しの学習には、いくつかの方法があります。
これは典型的な読み出しの学習方法です。
これは線形回帰の例である。
ここでは、ターゲットとモデルの出力の2乗誤差を最小にするW^outを求めます。
DとXの2乗誤差を最小にすることにする。
この式は次のような式に変形することができる。
これが基本的な方法である。
次に、リッジ回帰を見てみよう。
この方法では、式に正則化項を追加する。
W^outが大きくなるのを防ぐことができる。
これが変形された式です。
この講座で見てきた正則化と似ていますね。
この式を解くことでW^outを決定することができます。
学習コストは低くなります。

タスクの例
これは線形回帰の例である。
このタスクでは、次の時間ステップでの正弦波の値を予測する。
リザーバーに入力時系列データを与え、読み出しのトレーニングを行います。
リッジ回帰を用いることで、正弦波を予測することができる。
出力は連続した値である。


分類の例
これは音声認識の例である。
出力値が0の場合、「0」の出力ニューロンだけが1になります。
他のニューロンは反応しません。
ターゲット信号は、クラスに対応するワンホットベクトルである。
このリザーバーコンピューティングは、音声認識にも応用できる。
しかし、原信号の学習が困難であるため、入力音声の前処理が必要である。
そこで、音声信号を周波数帯に応じて分解する。
前処理を導入することで、Reservoir Computingにより音声信号の分類が可能になる。

Reservoir Computingは軽量なモデルで、ディープリカレントニューラルネットワークと同等の能力を持ち最近注目されています。


Reservoir Computingは一般的に入力層、リザバー、出力層から構成されます。まずはリザバーコンピューティングの概念を理解するために、簡単な例を考えてみます。リザバー（Reservoir）という単語には、貯水池という意味があります。いま、水面に石を投げ込むとします。すると、水面に波紋が生じます。その波紋は石の大きさや形、スピードによって変化します。つまり、この波紋は投げた石の情報を反映していると考えることができます。では、複数の石を次々と投げるとどうなるでしょうか。今度は石が1つの場合よりもはるかに複雑な波紋が生まれます。このような波紋の動的パターンは、投げ込まれた複数の石の大きさや形、さらにはそれらの投げ込まれた順番にも依存します。
つまり、リザバーは系列入力を波紋の時空間パターンに変換する装置とみなすことができます。Reservoir Computingは、このリザバーが生成する動的パターンから、簡便なアルゴリズムを用いて系列入力の識別を行います。


Reservoir Computingの代表的なモデルの1つにエコーステートネットワークがあります。エコーステートネットワークでは、リザバーとして結合重みを固定したリカレントニューラルネットワークを用いて、時系列入力の過去の情報が反響して残る状態（エコーステート）を作り出し、そこから入力の特徴の読み出し（リードアウト）を行います。入力層とリザバーの間の結合重みとリザバー内のフィードバック重みはあらかじめ固定しておき、リザバーと出力層の間の結合重みだけを計算量の小さい線形学習器で最適化します。この工夫によって、全ての結合重みを学習する一般のリカレントニューラルネットワークよりも高速な学習が可能となります。ただし、高い性能を実現するには、あらかじめ固定する結合重みの値を適切に設定しておく必要があります。


Reservoir Computingは、従来のリカレントニューラルネットワークに比べて学習時に変更する要素が少ないため、ハードウェア実装が比較的容易であると考えられます。また近年、リザバーの非線形変換機能を物理現象によって実現する物理Reservoir Computingも注目されています。電子、光、スピン、流体、機械、ナノ粒子、培養細胞などさまざまな媒質・基質を利用する物理リザバーが提案されています。
このようにReservoir Computingは物理系のダイナミクスによっても実現できるため、ソフトウェア実装に比べて高速性や低消費電力性を持つ実装が可能であり、高効率機械学習デバイスを実現するための基盤技術として注目されています。



Reservoir Computingは，近年様々な実データの時系列パターン認識に応用されてきている．その最大のメリットは，他の再帰的ニューラルネットワークモデルに比べて，学習が極めて高速であるという点である．また，「作り込まない」物理的実装が可能であることから，新たな機械学習デバイスの開発につながると期待されている．そのため，Reservoir Computingは，機械学習やニューラルネットワークのコミュニティだけではなく，エレクトロニクスを含む他の分野からも高い関心を集めている．
回帰型ニューラルネットワークは1986年のデビッド・ラメルハートの研究に基づく[9]。ホップフィールド・ネットワークは1982年にジョン・ホップフィールドによって見出された。1993年、ニューラルヒストリー圧縮システムが、時間に展開されたRNN中で1000以上の層を必要とする「非常に深い学習」問題を解決した。
長・短期記憶（LSTM）は2007年頃から音声認識に革命をもたらし始め、特定の音声認識への応用において伝統的なモデルをしのいだ。2009年、コネクショニスト時系列分類（英語版）（CTC）で訓練されたLSTMネットワークは、パターン認識大会で優勝した初のRNNとなった。このネットワークはつながった手書き文字認識の複数の大会で優勝した[12][13]。2014年、中国の大手検索サイト百度は、伝統的な音声処理法を用いることなくSwitchboard Hub5'00音声認識ベンチマークを破るためにCTCで訓練されたRNNを用いた[14]。
LSTMはまた、大規模語彙音声認識およびテキスト音声合成を改良し、Google Androidにおいて使われた.。2015年、GoogleはCTCで訓練されたLSTMによって音声認識の劇的な性能向上が達成された[17]と報告され、この技術はGoogle Voice Search（英語版）で使用された。
LSTMは機械翻訳、言語モデリング、多言語処理[20]の記録を破った。畳み込みニューラルネットワーク（CNN）と組み合わされたLSTMは自動画像キャプション（短い説明文）付けを向上させた。

基本的なRNNは連続する「層」へと編成されたニューロン的ノードのネットワークであり、所定の層中の個々のノードは次の層中の全てのノードと有向（一方向）結合により結合されている。個々のノード（ニューロン）は時間変動する実数値の活性化を有する。個々の結合（シナプス）は変更可能な実数値の重みを有する。ノードは（ネットワーク外からデータを受け取る）入力ノード、（結果を得る）出力ノード、（入力から出力への途上でデータを修正する）隠れノードのいずれかである。
離散時間設定における教師あり学習のため、実数値入力ベクトルの配列は入力ノードに到着する（一度に1つのベクトル）。任意の時間ステップにおいて、個々の非入力ユニットはそれに結合した全てのユニットの活性化の加重和の非線形関数としてその現在の活性化（結果）を計算する。ある時間ステップにおける一部の出力ユニットのために教師が与えられた目標活性化を提供することができる。例えば、入力配列が数字音声に対応した音声シグナルであるならば、配列の最後における最終目標出力は数字を分類するラベルとなるだろう。
強化学習のセッティングでは、教師は目標シグナルを与えない。代わりに、適合度関数または報酬関数がRNNの性能を評価するために使われることがある。これは環境に影響を与えるアクチュエータに結合された出力ユニットを通してその入力ストリームに影響する。これは、進行が勝ち取った点数によって測定されるゲームをプレーするために使うことができるかもしれない。
個々の配列は、全ての目標シグナルのネットワークによって計算された対応する活性化からのずれの和として誤差を生じる。膨大な配列のセットを訓練では、全誤差は全ての個別の配列の誤差の和である。
エルマンネットワークとジョーダンネットワーク
エルマンネットワークは、一連の「文脈ユニット」（右図中のu）を追加した3層ネットワーク（右図中でx、y、zとして垂直に配置されている）である。中央（隠れ）層は1の重みに固定されたこれらの文脈ユニットに結合されている]。個々の時間ステップにおいて、入力は順伝播され、学習規則が適用される。固定された逆結合は文脈ユニット中の隠れユニットの以前の値のコピーを保存する（これは、それらが学習規則が適用する前に結合を通じて伝播されるためである）。したがって、ネットワークは一種の状態を維持することができ、これによって標準的な多層パーセプトロンの能力を超える時系列予測といった課題を実行することが可能となる。
ジョーダンネットワークはエルマンネットワークと似ている。文脈ユニットは隠れ層の代わりに出力層から入力を得る。ジョーダンネットワーク中の文脈ユニットは状態層とも呼ばれる。それらはそれら自身への回帰的結合を持つ。
エルマンネットワークとジョーダンネットワークは「単純回帰型ネットワーク（SRN）」としても知られている。

ホップフィールド
ホップフィールドネットワークは全ての結合が対称的なRNNである。定常入力を必要とし、複数パターンの配列を処理しないため、汎用RNNではない。ホップフィールドネットワークは収束することを保証している。もし結合がヘッブの学習を用いて訓練されるならば、ホップフィールドネットワークは結合変化に抵抗性のある頑強な連想メモリとして機能することができる。
Bart Koskoによって発表された双方向連想メモリネットワークは、ベクトルとして連想データを貯蔵するホップフィールドネットワークの一変型である。双方向性は行列とその転置行列を通って情報が流れることから来ている。典型的には、双極符号化が連想対の二値符号化よりも選好される。最近、マルコフ飛び（ステッピング）を用いた確率的BAMモデルが増強したネットワーク安定化ために最適化され、現実世界の応用と関わりを持った。
BAMネットワークは2つの層を持ち、そのうちのどちらかを、連想を思い出し、もう一方の層上へ出力を生成するための入力として動作させることができる。
エコー状態
エコー状態ネットワーク（Echo state network、ESN）は、疎らに結合されたランダム隠れ層を持つ。出力ニューロンの重みは変更可能な（訓練可能な）ネットワークの一部でしかない。ESNは特定の時系列の再現に秀でている。スパイキングニューロンのための派生形式は液体状態マシンとして知られる。
独立RNN (IndRNN) 
独立回帰型ニューラルネットワーク（Independently recurrent neural network、IndRNN）は、従来の完全結合型RNNにおける勾配消失および爆発問題に対処する。1つの層中の個々のニューロンは（この層中の他の全てのニューロンへの完全な結合の代わりに）文脈情報としてそれ自身の過去状態のみを受け取り、ゆえにニューロンは互いの履歴に独立である。勾配バックプロパゲーションは、長期または短期記憶を保持するため、勾配消失および爆発を避けるために制御することができる。ニューロン間情報は次の層において探索される。IndRNNはReLUといった非飽和非線形関数を使って確実に訓練することができる。スキップコネクションを使うことで、深いネットワークを訓練することができる。
再帰型
再帰型ニューラルネットワーク（recursive neural network）は、トポロジカル順序で可微分なグラフ様構造を横断することによって、同じ一連の重みを構造に再帰的に適用することによって作られる。このようなネットワークは典型的に自動微分の反転モードによって訓練することもできる。再帰型ニューラルネットワークは、論理項といった構造の分散表現を処理することできる。再帰型ニューラルネットワークの特殊な場合が、構造が直鎖に対応するRecurrent（回帰型）NNである。再帰型ニューラルネットワークは自然言語処理に応用されてきた。再帰型ニューラルテンソルネットワークは、木中の全てのノードに対してテンソルベースの合成関数を使用する。
ニューラルヒストリーコンプレッサ
ニューラルヒストリーコンプレッサ（neural history compressor）はRNNの教師なしスタックである[36]。入力レベルにおいて、前の入力から次の入力を予測することを学習する。この階層型構造において一部のRNNの予測不可能な入力のみが次のより高いレベルのRNNへの入力となる。したがって、極めてまれにしかその内部状態は再計算されない。ゆえに、個々のより高位のRNNは下位RNN中の情報の圧縮表現を学ぶ。これは、入力配列がより高レベルにおける表現から正確に再構成できるような方法で行われる。
このシステムは、記述長またはデータの確率の負の対数を効果的に最小化する。入ってくるデータ配列中の多量の学習可能な予測可能性を考えると、最高レベルのRNNは、重要な事象間に長い間隔がある深い配列でさえも容易に分類するために教師あり学習を用いることができる。
このRNN階層を2つのRNN、「意識的」チャンカー（高位）と「無意識的」オートマタイザー（下位）に抜き出すことが可能である。チャンカーがオートマタイザーによって予測不可能な入力の予測と圧縮を学習すると、次にオートマタイザーは次の学習フェーズにおいて追加ユニットを通して、よりゆっくりと変化するチャンカーの隠れ層を予測または模倣することになる。これによってオートマタイザーが、長い間隔を超えて適切な、めったに変化しない記憶を学習することが容易になる。次に、チャンカーが残った予測不可能な事象に注視できるように、これはオートマタイザーが以前は予測不可能だった入力の多くを予測できるものとするのを助ける。
生成モデルは、1992年に自動微分またはバックプロパゲーションの勾配消失問題を部分的に克服した。1993年、こういったシステムは時間方向に展開されたRNN中に1000を超える後続層を必要とする「非常に深い学習」課題を解決した。
二次RNN
二次（second order）RNNは、標準的な重み の代わりにより高次の重み を用い、状態は積となる。これによって、訓練、安定性、表現において有限状態機械への直接的マッピングが可能となる。長・短期記憶（LSTM）はこの一例であるが、こういった形式的マッピングまたは安定性の証明は持たない。
長・短期記憶
長・短期記憶（LSTM）は勾配消失問題を回避するディープラーニング（深層学習）システムである。LSTMは通常、「忘却」ゲートと呼ばれる回帰型ゲートによって拡張されている。LSTMは勾配の消失または爆発からの逆伝播誤差を防ぐ。代わりに、誤差は空間方向に展開された無制限の数のバーチャル層を通して逆向きに流れる。すなわち、LSTMは、数千または数百万離れた時間段階前に起こった事象の記憶を必要とする課題を学習できる。問題特化型のLSTM的トポロジーを発展させることができる。LSTMは重要な事象間に長い遅延が与えられても機能し、低周波数と高周波数成分を混合した信号を扱うことができる。
多くの応用がLSTM RNNのスタックを用いており、訓練セット中のラベル配列の確率を最大化するRNN重み行列を見付けるためにそれらをコネクショニスト時系列分類（CTC）によって訓練している。CTCはアラインメントと認識の両方を達成する。
LSTMは隠れマルコフモデル（HMM）や類似の概念に基づく以前のモデルとは異なり、文脈依存言語を認識することを学習することができる。
ゲート付き回帰型ユニット
ゲート付き回帰型ユニット（GRUs）は2014年に発表された回帰型ニューラルネットワークにおけるゲート機構である。完全な形式やいくつかの単純化された方式で使われている。多声音楽モデリングおよび音声信号モデリングにおけるそれらの性能は長・短期記憶の性能と似ていることが明らかにされた。これらは出力ゲートを持っていないため、LSTMよりもパラメータが少ない。
双方向性
双方向性（bi-directional）RNNsは要素の過去および未来の文脈に基づいて配列の個々の要素を予測あるいはラベル付けするために有限配列を用いる。これは、2つのRNNの出力を統合することによってなされる。一方のRNNは配列を左から右へ、もう一方は右から左へと処理する。統合された出力は教師が与えられた対象シグナルの予測である。この技法はLSTM RNNsを組み合わせた時に特に有用であることが証明されている。
連続時間
連続時間（continuous time）回帰型ニューラルネットワーク（CTRNN）は、入ってくるスパイクの一連の流れのニューロンへの影響をモデル化するために常微分方程式の系を用いる。